{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment CIFAR-10\n",
    "> Can we get better by training on our assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment_cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import dataclasses\n",
    "import traceback\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from blackhc.project import is_run_from_ipython\n",
    "from blackhc.project.experiment import embedded_experiments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import batchbald_redux.acquisition_functions as acquisition_functions\n",
    "from batchbald_redux.acquisition_functions import (\n",
    "    CandidateBatchComputer,\n",
    "    EvalCandidateBatchComputer,\n",
    ")\n",
    "from batchbald_redux.active_learning import ActiveLearningData, RandomFixedLengthSampler\n",
    "from batchbald_redux.black_box_model_training import evaluate, train, train_with_schedule\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    NamedDataset,\n",
    "    create_repeated_MNIST_dataset,\n",
    "    get_balanced_sample_indices,\n",
    "    get_base_dataset_index,\n",
    "    get_target, AdditiveGaussianNoise, AliasDataset, get_balanced_sample_indices_by_class,\n",
    ")\n",
    "from batchbald_redux.datasets import get_dataset\n",
    "from batchbald_redux.datasets import train_validation_split\n",
    "from batchbald_redux.di import DependencyInjection\n",
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "from batchbald_redux.model_optimizer_factory import ModelOptimizerFactory\n",
    "from batchbald_redux.resnet_models import Cifar10BayesianResnetFactory\n",
    "from batchbald_redux.train_eval_model import (\n",
    "    TrainEvalModel,\n",
    "    TrainSelfDistillationEvalModel,\n",
    "    TrainSelfDistillationEvalModelWithSchedule\n",
    ")\n",
    "from batchbald_redux.trained_model import TrainedMCDropoutModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "@dataclass\n",
    "class ExperimentData:\n",
    "    active_learning: ActiveLearningData\n",
    "    augmentation_node: AliasDataset\n",
    "    unaugmented_train_dataset: Dataset\n",
    "    augmented_train_dataset: Dataset\n",
    "    validation_dataset: Dataset\n",
    "    test_dataset: Dataset\n",
    "    evaluation_dataset: Dataset\n",
    "    initial_training_set_indices: [int]\n",
    "    evaluation_set_indices: [int]\n",
    "\n",
    "    def toggle_augmentations(self, augment: bool):\n",
    "        if augment:\n",
    "            self.augmentation_node.dataset = self.augmented_train_dataset\n",
    "        else:\n",
    "            self.augmentation_node.dataset = self.unaugmented_train_dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentDataConfig:\n",
    "    id_dataset_name: str\n",
    "    initial_training_set_size: int\n",
    "    validation_set_size: int\n",
    "    evaluation_set_size: int\n",
    "    id_repetitions: float\n",
    "    add_dataset_noise: bool\n",
    "    validation_split_random_state: int\n",
    "\n",
    "    device: str\n",
    "\n",
    "    def load(self) -> ExperimentData:\n",
    "        return load_experiment_data(\n",
    "            id_dataset_name=self.id_dataset_name,\n",
    "            initial_training_set_size=self.initial_training_set_size,\n",
    "            validation_set_size=self.validation_set_size,\n",
    "            evaluation_set_size=self.evaluation_set_size,\n",
    "            id_repetitions=self.id_repetitions,\n",
    "            add_dataset_noise=self.add_dataset_noise,\n",
    "            validation_split_random_state=self.validation_split_random_state,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    *,\n",
    "    id_dataset_name: str,\n",
    "    initial_training_set_size: int,\n",
    "    validation_set_size: int,\n",
    "    evaluation_set_size: int,\n",
    "    id_repetitions: float,\n",
    "    add_dataset_noise: bool,\n",
    "    validation_split_random_state: int,\n",
    "    device: str,\n",
    ") -> ExperimentData:\n",
    "    split_dataset = get_dataset(id_dataset_name, root=\"data\", train_augmentation=True, validation_set_size=validation_set_size,\n",
    "                                validation_split_random_state=validation_split_random_state, normalize_like_cifar10=True)\n",
    "    unaugmented_split_dataset = get_dataset(id_dataset_name, root=\"data\", train_augmentation=True, validation_set_size=validation_set_size,\n",
    "                                validation_split_random_state=validation_split_random_state, normalize_like_cifar10=True)\n",
    "\n",
    "    augmentation_node = AliasDataset(split_dataset.train, \"Augmentation Node\")\n",
    "    train_dataset=augmentation_node\n",
    "\n",
    "    # If we reduce the train set, we need to do so before picking the initial train set.\n",
    "    if id_repetitions < 1:\n",
    "        train_dataset = train_dataset * id_repetitions\n",
    "\n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    initial_samples_per_class = initial_training_set_size // num_classes\n",
    "    evaluation_set_samples_per_class = evaluation_set_size // num_classes\n",
    "    samples_per_class = initial_samples_per_class + evaluation_set_samples_per_class\n",
    "    balanced_samples_indices = get_balanced_sample_indices_by_class(\n",
    "        train_dataset,\n",
    "        num_classes=num_classes,\n",
    "        samples_per_class=samples_per_class,\n",
    "        seed=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    initial_training_set_indices = [\n",
    "        idx for by_class in balanced_samples_indices.values() for idx in by_class[:initial_samples_per_class]\n",
    "    ]\n",
    "    evaluation_set_indices = [\n",
    "        idx for by_class in balanced_samples_indices.values() for idx in by_class[initial_samples_per_class:]\n",
    "    ]\n",
    "\n",
    "    # If we over-sample the train set, we do so after picking the initial train set to avoid duplicates.\n",
    "    if id_repetitions > 1:\n",
    "        train_dataset = train_dataset * id_repetitions\n",
    "\n",
    "    if add_dataset_noise:\n",
    "        train_dataset = AdditiveGaussianNoise(train_dataset, 0.1)\n",
    "\n",
    "    active_learning_data = ActiveLearningData(train_dataset)\n",
    "\n",
    "    active_learning_data.acquire_base_indices(initial_training_set_indices)\n",
    "\n",
    "    evaluation_dataset = AliasDataset(\n",
    "        active_learning_data.extract_dataset_from_base_indices(evaluation_set_indices),\n",
    "        f\"Evaluation Set ({len(evaluation_set_indices)} samples)\",\n",
    "    )\n",
    "\n",
    "    return ExperimentData(\n",
    "        active_learning=active_learning_data,\n",
    "        augmentation_node=augmentation_node,\n",
    "        augmented_train_dataset=split_dataset.train,\n",
    "        unaugmented_train_dataset=unaugmented_split_dataset.train,\n",
    "        validation_dataset=split_dataset.validation,\n",
    "        test_dataset=split_dataset.test,\n",
    "        evaluation_dataset=evaluation_dataset,\n",
    "        initial_training_set_indices=initial_training_set_indices,\n",
    "        evaluation_set_indices=evaluation_set_indices,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    seed: int\n",
    "    acquisition_function: Union[\n",
    "        Type[CandidateBatchComputer], Type[EvalCandidateBatchComputer]\n",
    "    ]\n",
    "\n",
    "    id_dataset_name: str = \"CIFAR-10\"\n",
    "    initial_training_set_size: int = 5000\n",
    "    validation_set_size: int = 5000\n",
    "    evaluation_set_size: int = 0\n",
    "    id_repetitions: float = 1\n",
    "    add_dataset_noise: bool = False\n",
    "    validation_split_random_state: int = 0\n",
    "\n",
    "    acquisition_size: int = 2500\n",
    "    max_training_set: int = 40000\n",
    "    num_pool_samples: int = 20\n",
    "    num_validation_samples: int = 20\n",
    "    num_training_samples: int = 1\n",
    "    max_training_epochs: int = 120\n",
    "    training_batch_size: int = 128\n",
    "    device: str = \"cuda\"\n",
    "    min_samples_per_epoch: int = 5056\n",
    "    patience_schedule: [int] = (6, 4, 2)\n",
    "    factor_schedule: [int] = (0.1,)\n",
    "    train_eval_model: Type[TrainEvalModel] = TrainSelfDistillationEvalModelWithSchedule\n",
    "    model_optimizer_factory: Type[ModelOptimizerFactory] = Cifar10BayesianResnetFactory\n",
    "    acquisition_function_args: dict = None\n",
    "    temperature: float = 0.0\n",
    "    prefer_accuracy: bool = True\n",
    "\n",
    "    def load_experiment_data(self) -> ExperimentData:\n",
    "        di = DependencyInjection(vars(self))\n",
    "        edc: ExperimentDataConfig = di.create_dataclass_type(ExperimentDataConfig)\n",
    "        return edc.load()\n",
    "\n",
    "    # Simple Dependency Injection\n",
    "    def create_acquisition_function(self):\n",
    "        di = DependencyInjection(vars(self))\n",
    "        return di.create_dataclass_type(self.acquisition_function)\n",
    "\n",
    "    def create_train_eval_model(self, runtime_config) -> TrainEvalModel:\n",
    "        config = {**vars(self), **runtime_config}\n",
    "        di = DependencyInjection(config, [])\n",
    "        return di.create_dataclass_type(self.train_eval_model)\n",
    "\n",
    "    def run(self, store):\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # Active Learning setup\n",
    "        data = self.load_experiment_data()\n",
    "        store[\"dataset_info\"] = dict(training=repr(data.active_learning.base_dataset), test=repr(data.test_dataset))\n",
    "        store[\"initial_training_set_indices\"] = data.initial_training_set_indices\n",
    "        store[\"evaluation_set_indices\"] = data.evaluation_set_indices\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            data.active_learning.training_dataset,\n",
    "            batch_size=self.training_batch_size,\n",
    "            sampler=RandomFixedLengthSampler(data.active_learning.training_dataset, self.min_samples_per_epoch),\n",
    "            drop_last=True,\n",
    "        )\n",
    "        pool_loader = torch.utils.data.DataLoader(\n",
    "            data.active_learning.pool_dataset, batch_size=128, drop_last=False, shuffle=False\n",
    "        )\n",
    "\n",
    "        validation_loader = torch.utils.data.DataLoader(data.validation_dataset, batch_size=512, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(data.test_dataset, batch_size=512, drop_last=False)\n",
    "\n",
    "        store[\"active_learning_steps\"] = []\n",
    "        active_learning_steps = store[\"active_learning_steps\"]\n",
    "\n",
    "        acquisition_function = self.create_acquisition_function()\n",
    "\n",
    "        # Active Training Loop\n",
    "        while True:\n",
    "            training_set_size = len(data.active_learning.training_dataset)\n",
    "            print(f\"Training set size {training_set_size}:\")\n",
    "\n",
    "            # iteration_log = dict(training={}, pool_training={}, evaluation_metrics=None, acquisition=None)\n",
    "            active_learning_steps.append({})\n",
    "            iteration_log = active_learning_steps[-1]\n",
    "\n",
    "            iteration_log[\"training\"] = {}\n",
    "\n",
    "            model_optimizer = self.model_optimizer_factory().create_model_optimizer()\n",
    "\n",
    "            data.toggle_augmentations(True)\n",
    "\n",
    "            if training_set_size > 0:\n",
    "                train_with_schedule(\n",
    "                    model=model_optimizer.model,\n",
    "                    optimizer=model_optimizer.optimizer,\n",
    "                    training_samples=self.num_training_samples,\n",
    "                    validation_samples=self.num_validation_samples,\n",
    "                    train_loader=train_loader,\n",
    "                    validation_loader=validation_loader,\n",
    "                    patience_schedule = self.patience_schedule,\n",
    "                    factor_schedule = self.factor_schedule,\n",
    "                    max_epochs=self.max_training_epochs,\n",
    "                    device=self.device,\n",
    "                    training_log=iteration_log[\"training\"],\n",
    "                    prefer_accuracy=self.prefer_accuracy\n",
    "                )\n",
    "\n",
    "            data.toggle_augmentations(False)\n",
    "\n",
    "            evaluation_metrics = evaluate(\n",
    "                model=model_optimizer.model,\n",
    "                num_samples=self.num_validation_samples,\n",
    "                loader=test_loader,\n",
    "                device=self.device,\n",
    "            )\n",
    "            iteration_log[\"evaluation_metrics\"] = evaluation_metrics\n",
    "            print(f\"Perf after training {evaluation_metrics}\")\n",
    "\n",
    "            if training_set_size >= self.max_training_set:\n",
    "                print(\"Done.\")\n",
    "                break\n",
    "\n",
    "            trained_model = TrainedMCDropoutModel(num_samples=self.num_pool_samples, model=model_optimizer.model)\n",
    "\n",
    "            if isinstance(acquisition_function, CandidateBatchComputer):\n",
    "                candidate_batch = acquisition_function.compute_candidate_batch(trained_model, pool_loader, self.device)\n",
    "            elif isinstance(acquisition_function, EvalCandidateBatchComputer):\n",
    "                current_max_epochs = len(iteration_log[\"training\"][\"epochs\"])\n",
    "\n",
    "                if self.evaluation_set_size:\n",
    "                    eval_dataset = data.evaluation_dataset\n",
    "                else:\n",
    "                    eval_dataset = data.active_learning.pool_dataset\n",
    "\n",
    "                train_eval_model = self.create_train_eval_model(\n",
    "                    dict(\n",
    "                        max_epochs=current_max_epochs + 2,\n",
    "                        training_dataset=data.active_learning.training_dataset,\n",
    "                        eval_dataset=eval_dataset,\n",
    "                        validation_loader=validation_loader,\n",
    "                        trained_model=trained_model,\n",
    "                        data=data,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                iteration_log[\"eval_training\"] = {}\n",
    "                trained_eval_model = train_eval_model(training_log=iteration_log[\"eval_training\"], device=self.device)\n",
    "\n",
    "                candidate_batch = acquisition_function.compute_candidate_batch(\n",
    "                    trained_model, trained_eval_model, pool_loader, device=self.device\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown acquisition function {acquisition_function}!\")\n",
    "\n",
    "            candidate_global_indices = [\n",
    "                get_base_dataset_index(data.active_learning.pool_dataset, index).index\n",
    "                for index in candidate_batch.indices\n",
    "            ]\n",
    "            candidate_labels = [\n",
    "                get_target(data.active_learning.pool_dataset, index).item() for index in candidate_batch.indices\n",
    "            ]\n",
    "\n",
    "            iteration_log[\"acquisition\"] = dict(\n",
    "                indices=candidate_global_indices, labels=candidate_labels, scores=candidate_batch.scores\n",
    "            )\n",
    "\n",
    "            data.active_learning.acquire(candidate_batch.indices)\n",
    "\n",
    "            ls = \", \".join(f\"{label} ({score:.4})\" for label, score in zip(candidate_labels, candidate_batch.scores))\n",
    "            print(f\"Acquiring (label, score)s: {ls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tid_dataset_name=CIFAR-10,\n",
      "\tinitial_training_set_size=20000,\n",
      "\tvalidation_set_size=5000,\n",
      "\tevaluation_set_size=0,\n",
      "\tid_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8de39b24f9de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-010948f3291b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Active Learning setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_experiment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dataset_info\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"initial_training_set_indices\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_training_set_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-010948f3291b>\u001b[0m in \u001b[0;36mload_experiment_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDependencyInjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0medc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExperimentDataConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataclass_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperimentDataConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0medc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Simple Dependency Injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-546f37142c16>\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mExperimentData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         return load_experiment_data(\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mid_dataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_dataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0minitial_training_set_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_training_set_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-546f37142c16>\u001b[0m in \u001b[0;36mload_experiment_data\u001b[0;34m(id_dataset_name, initial_training_set_size, validation_set_size, evaluation_set_size, id_repetitions, add_dataset_noise, validation_split_random_state, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m     split_dataset = get_dataset(id_dataset_name, root=\"data\", train_augmentation=True, validation_set_size=validation_set_size,\n\u001b[1;32m     59\u001b[0m                                 validation_split_random_state=validation_split_random_state, normalize_like_cifar10=True)\n\u001b[0;32m---> 60\u001b[0;31m     unaugmented_split_dataset = get_dataset(id_dataset_name, root=\"data\", train_augmentation=True, validation_set_size=validation_set_size,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                 validation_split_random_state=validation_split_random_state, normalize_like_cifar10=True)\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/datasets.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(name, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, train_augmentation)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mtrain_augmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_augmentation\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain_augmentation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     split_dataset = dataset_factories[name](\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split_random_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_like_cifar10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     )\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/datasets.py\u001b[0m in \u001b[0;36mget_CIFAR10\u001b[0;34m(root, validation_set_size, validation_split_random_state, normalize_like_cifar10, train_augmentation)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mtrain_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mfull_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/CIFAR10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     full_validation_dataset = datasets.CIFAR10(\n\u001b[1;32m    136\u001b[0m         \u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/CIFAR10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m_check_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_integrity\u001b[0;34m(fpath, md5)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_md5\u001b[0;34m(fpath, md5, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcalculate_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcalculate_md5\u001b[0;34m(fpath, chunk_size)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    seed=1120,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20005,\n",
    "    acquisition_function=acquisition_functions.EvalBALD,\n",
    "    acquisition_size=5,\n",
    "    num_pool_samples=20,\n",
    "    initial_training_set_size=20000,\n",
    "    temperature=8,\n",
    "    min_samples_per_epoch=5000,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_training_set_indices': [38043,\n",
       "  40091,\n",
       "  17418,\n",
       "  2094,\n",
       "  39879,\n",
       "  3133,\n",
       "  5011,\n",
       "  40683,\n",
       "  54379,\n",
       "  24287,\n",
       "  9849,\n",
       "  59305,\n",
       "  39508,\n",
       "  39356,\n",
       "  8758,\n",
       "  52579,\n",
       "  13655,\n",
       "  7636,\n",
       "  21562,\n",
       "  41329],\n",
       " 'dataset_info': {'training': \"'FastMNIST (Train)'\",\n",
       "  'test': \"'FastMNIST (Test)'\"},\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.62109375,\n",
       "      'crossentropy': 2.6530187726020813},\n",
       "     {'accuracy': 0.6376953125, 'crossentropy': 2.762658029794693},\n",
       "     {'accuracy': 0.646484375, 'crossentropy': 3.056214064359665},\n",
       "     {'accuracy': 0.6416015625, 'crossentropy': 3.1257119178771973}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.631,\n",
       "    'crossentropy': 2.6251225173950195}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fc327793734c0eb2f361164ad86451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/384]   0%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/64]   2%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RestoringEarlyStopping: Restoring best parameters. (Score: -6.529030114412308)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/157]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.5367, 'crossentropy': 6.438035237884521}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions:   0%|          | 0/463616 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382711f4f7ad4ec7980c856e0f9d229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/1811]   0%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/64]   2%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RestoringEarlyStopping: Restoring best parameters. (Score: -5.1637596152722836)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions:   0%|          | 0/2317680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions:   0%|          | 0/2317680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/115884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/115884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/115884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/115884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquiring (label, score)s: 8 (0.8711), 8 (0.8687), 3 (0.876), 3 (0.8465), 3 (0.8811)\n",
      "Training set size 25:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c326c4165e48eab908014b4064f795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/384]   0%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/64]   2%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RestoringEarlyStopping: Restoring best parameters. (Score: -4.6851686127483845)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/157]   1%|           [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6256, 'crossentropy': 4.484497045135498}\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'initial_training_set_indices': [38043,\n",
       "  40091,\n",
       "  17418,\n",
       "  2094,\n",
       "  39879,\n",
       "  3133,\n",
       "  5011,\n",
       "  40683,\n",
       "  54379,\n",
       "  24287,\n",
       "  9849,\n",
       "  59305,\n",
       "  39508,\n",
       "  39356,\n",
       "  8758,\n",
       "  52579,\n",
       "  13655,\n",
       "  7636,\n",
       "  21562,\n",
       "  41329],\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.538818359375,\n",
       "      'crossentropy': 6.529030114412308}],\n",
       "    'best_epoch': 1},\n",
       "   'evalution_metrics': {'accuracy': 0.5367,\n",
       "    'crossentropy': 6.438035237884521},\n",
       "   'pool_training': {'epochs': [{'accuracy': 0.531005859375,\n",
       "      'crossentropy': 5.1637596152722836}],\n",
       "    'best_epoch': 1},\n",
       "   'acquisition': {'indices': [63338, 10856, 63452, 81864, 109287],\n",
       "    'labels': [8, 8, 3, 3, 3],\n",
       "    'scores': [0.8710822958846325,\n",
       "     0.8687216999221631,\n",
       "     0.8759664372823723,\n",
       "     0.8464646732511746,\n",
       "     0.8810812784952251]}},\n",
       "  {'training': {'epochs': [{'accuracy': 0.62255859375,\n",
       "      'crossentropy': 4.6851686127483845}],\n",
       "    'best_epoch': 1},\n",
       "   'evalution_metrics': {'accuracy': 0.6256,\n",
       "    'crossentropy': 4.484497045135498}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    max_training_epochs=1, max_training_set=25, acquisition_function=AcquisitionFunction.randombaldical\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "configs = [\n",
    "    Experiment(\n",
    "        seed=seed + 8945,\n",
    "        acquisition_function=acquisition_function,\n",
    "        acquisition_size=acquisition_size,\n",
    "        num_pool_samples=num_pool_samples,\n",
    "        initial_training_set_size=5000,\n",
    "        evaluation_set_size=0,\n",
    "        max_training_set=20000,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    for seed in range(5)\n",
    "    for acquisition_function in [\n",
    "        acquisition_functions.SoftmaxBALD,\n",
    "    ]\n",
    "    for acquisition_size in [100]\n",
    "    for num_pool_samples in [100]\n",
    "    for temperature in [1/128,1/256]\n",
    "] + [\n",
    "    Experiment(\n",
    "        seed=seed + 8945,\n",
    "        acquisition_function=acquisition_function,\n",
    "        acquisition_size=acquisition_size,\n",
    "        num_pool_samples=num_pool_samples,\n",
    "        initial_training_set_size=5000,\n",
    "        evaluation_set_size=0,\n",
    "        max_training_set=20000,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    for seed in range(15)\n",
    "    for acquisition_function in [\n",
    "        acquisition_functions.Random,\n",
    "        acquisition_functions.BALD,\n",
    "    ]\n",
    "    for acquisition_size in [100]\n",
    "    for num_pool_samples in [100]\n",
    "    for temperature in [0]\n",
    "]\n",
    "\n",
    "if not is_run_from_ipython() and __name__ == \"__main__\":\n",
    "    for job_id, store in embedded_experiments(__file__, len(configs)):\n",
    "        config = configs[job_id]\n",
    "        config.seed += job_id\n",
    "        print(config)\n",
    "        store[\"config\"] = dataclasses.asdict(config)\n",
    "        store[\"log\"] = {}\n",
    "\n",
    "        try:\n",
    "            config.run(store=store)\n",
    "        except Exception:\n",
    "            store[\"exception\"] = traceback.format_exc()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.TemperedBALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=5\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=10\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BALD,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100,\n",
      "        temperature=15\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7893,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7894,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7895,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7896,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7897,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7898,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7899,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7900,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7901,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7902,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7903,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7904,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7905,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7906,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7907,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7908,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7909,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7910,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7911,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    ),\n",
      "    Experiment(\n",
      "        seed=7912,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random,\n",
      "        max_training_set=20000,\n",
      "        num_pool_samples=100\n",
      "    )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import prettyprinter\n",
    "prettyprinter.install_extras({\"dataclasses\"})\n",
    "prettyprinter.pprint(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
