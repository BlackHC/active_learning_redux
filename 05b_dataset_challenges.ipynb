{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Challenges\n",
    "> “Whoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you.”\n",
    ">\n",
    "> ― Friedrich Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model real-world use cases better, we need:\n",
    "* redundant/duplicated data;\n",
    "* noisy labels (emulating noisy oracles);\n",
    "* class imbalance;\n",
    "* out-of-distribution data/outliers included in the unlabelled data;\n",
    "* noisy or ambiguous samples.\n",
    "\n",
    "RepeatedMNIST takes care of the first and last challenge in a very specific way. \n",
    "This chapter takes care of the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import bisect\n",
    "from typing import Optional, Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "from batchbald_redux.fast_mnist import FastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def _wrap_alias(dataset: data.Dataset):\n",
    "    if isinstance(dataset, NamedDataset):\n",
    "        return repr(dataset)\n",
    "    return f\"({dataset.alias})\"\n",
    "\n",
    "\n",
    "class _AliasDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset with an easier to understand alias.\n",
    "\n",
    "    And convenience operators.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset: data.Dataset\n",
    "    alias: str\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, alias: str):\n",
    "        self.dataset = dataset\n",
    "        self.alias = alias\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.alias\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return _AliasDataset(data.ConcatDataset([self, other]), f\"{_wrap_alias(self)} + {_wrap_alias(other)}\")\n",
    "\n",
    "    def __mul__(self, factor):\n",
    "        if int(factor) == factor:\n",
    "            return RepeatedDataset(self, num_repeats=factor)\n",
    "\n",
    "        return SubsetDataset(self, factor=factor, seed=0)\n",
    "\n",
    "    def __rmul__(self, factor):\n",
    "        if int(factor) == factor:\n",
    "            return RepeatedDataset(self, num_repeats=factor)\n",
    "\n",
    "        return SubsetDataset(self, factor=factor, seed=0)\n",
    "\n",
    "\n",
    "class NamedDataset(_AliasDataset):\n",
    "    def __init__(self, dataset: data.Dataset, name: str):\n",
    "        super().__init__(dataset, repr(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "\n",
    "MNIST = NamedDataset(FastMNIST(root=\"data/\", download=True, device=\"cpu\"), name=\"MNISTDataset\")\n",
    "\n",
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class OverridenTargetDataset(_AliasDataset):\n",
    "    reverse_indices: dict\n",
    "    new_targets: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, indices: list, new_targets: list):\n",
    "        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}\n",
    "        self.new_targets = new_targets\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} | override_targets{dict(indices=indices, new_targets=new_targets)}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if idx not in self.reverse_indices:\n",
    "            return data, label\n",
    "\n",
    "        ridx = self.reverse_indices[idx]\n",
    "        new_y = self.new_targets[ridx]\n",
    "        return data, new_y\n",
    "\n",
    "\n",
    "class CorruptedLabelsDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    implementation: OverridenTargetDataset\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset: data.Dataset, *, size_corrupted: Union[float, int], num_classes: int, seed: int, device=None\n",
    "    ):\n",
    "        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} | corrupt_labels{options}\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        N = len(dataset)\n",
    "\n",
    "        if size_corrupted > 1:\n",
    "            num_corrupted = size_corrupted\n",
    "        else:\n",
    "            num_corrupted = int(N * size_corrupted)\n",
    "\n",
    "        indices = generator.choice(N, size=num_corrupted, replace=False)\n",
    "        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)\n",
    "\n",
    "        self.implementation = OverridenTargetDataset(\n",
    "            dataset, indices=indices, new_targets=torch.as_tensor(new_targets, device=device)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.implementation[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.implementation)\n",
    "\n",
    "\n",
    "class RandomLabelsDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    new_labels: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, seed: int, device=None):\n",
    "        options = dict(num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} | randomize_labels{options}\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        N = len(dataset)\n",
    "\n",
    "        self.new_labels = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.new_labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZeroDataset' | corrupt_labels{'size_corrupted': 0.5, 'num_classes': 10, 'seed': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_dataset = NamedDataset(data.TensorDataset(torch.zeros(10), torch.zeros(10)), \"ZeroDataset\")\n",
    "\n",
    "corrupted_labels_dataset = CorruptedLabelsDataset(zero_dataset, size_corrupted=0.5, num_classes=10, seed=1)\n",
    "\n",
    "assert list(corrupted_labels_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "]\n",
    "\n",
    "corrupted_labels_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZeroDataset' | randomize_labels{'num_classes': 10, 'seed': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_dataset = RandomLabelsDataset(zero_dataset, num_classes=10, seed=2)\n",
    "\n",
    "assert list(corrupted_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(1)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(6)),\n",
    "]\n",
    "\n",
    "corrupted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator):\n",
    "    class_counts = list(class_counts)\n",
    "\n",
    "    subset_indices = []\n",
    "\n",
    "    remaining_samples = sum(class_counts)\n",
    "\n",
    "    indices = generator.permutation(len(dataset))\n",
    "    for index in indices:\n",
    "        _, y = dataset[index]\n",
    "\n",
    "        if class_counts[y] > 0:\n",
    "            subset_indices.append(index)\n",
    "            class_counts[y] -= 1\n",
    "            remaining_samples -= 1\n",
    "\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "\n",
    "    return subset_indices\n",
    "\n",
    "\n",
    "def get_balanced_sample_indices(dataset: data.Dataset, *, num_classes, samples_per_class, seed: int) -> List[int]:\n",
    "    class_counts = [samples_per_class] * num_classes\n",
    "    generator = np.random.default_rng(seed)\n",
    "\n",
    "    return get_class_indices(dataset, class_counts=class_counts, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class ImbalancedDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):\n",
    "        options = dict(class_counts=class_counts, seed=seed)\n",
    "        super().__init__(dataset, f\"ImbalancedDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class ImbalancedClassSplitDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset: data.Dataset, *, num_classes: int, majority_percentage: int, minority_percentage: int, seed: int\n",
    "    ):\n",
    "        assert (num_classes % 2) == 0\n",
    "\n",
    "        super().__init__(dataset, None)\n",
    "\n",
    "        num_samples_per_class = len(dataset) // num_classes\n",
    "        num_samples_majority = num_samples_per_class * majority_percentage // 100\n",
    "        num_samples_minority = num_samples_per_class * minority_percentage // 100\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        class_counts = [num_samples_majority] * (num_classes // 2) + [num_samples_minority] * (num_classes // 2)\n",
    "        class_counts = generator.permuted(class_counts)\n",
    "\n",
    "        self.options = dict(\n",
    "            num_classes=num_classes, majority_percentage=majority_percentage, seed=seed, class_counts=class_counts\n",
    "        )\n",
    "        self.alias = f\"ImbalancedDataset(dataset={self.dataset}, {self.options})\"\n",
    "\n",
    "        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 6, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dataset = NamedDataset(data.TensorDataset(torch.arange(9), torch.as_tensor(list(range(3)) * 3)), \"123\")\n",
    "\n",
    "imbalanced_indices = get_class_indices(three_dataset, class_counts=[3, 0, 0], generator=np.random.default_rng())\n",
    "\n",
    "three_dataset[imbalanced_indices][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 2, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset='123', {'class_counts': [1, 2, 3], 'seed': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbalanced_dataset = ImbalancedDataset(three_dataset, class_counts=[1, 2, 3], seed=2)\n",
    "print(imbalanced_dataset[:][1])\n",
    "imbalanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset='MNISTDataset', {'num_classes': 10, 'majority_percentage': 80, 'seed': 1, 'class_counts': array([1200, 4800, 1200, 4800, 4800, 4800, 1200, 1200, 1200, 4800])})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImbalancedClassSplitDataset(MNIST, num_classes=10, majority_percentage=80, minority_percentage=20, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing in OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "# Convert label dataset to one hot\n",
    "class OneHotDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    targets: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):\n",
    "        options = dict(num_classes=num_classes)\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} | one_hot_targets{options}\")\n",
    "        self.options = options\n",
    "\n",
    "        N = len(dataset)\n",
    "        targets = torch.zeros(len(dataset), num_classes, dtype=dtype, device=device)\n",
    "        # TODO: use get_targets() here, which will require a refactoring\n",
    "        for i, (_, label) in enumerate(dataset):\n",
    "            targets[i, label] = 1.0\n",
    "\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.targets[idx]\n",
    "\n",
    "\n",
    "class RepeatedDataset(_AliasDataset):\n",
    "    def __init__(self, dataset: data.Dataset, *, num_repeats: int):\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset}x{num_repeats}\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx > len(self):\n",
    "            return self.dataset[idx]\n",
    "\n",
    "        return self.dataset[idx % len(self.dataset)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.num_repeats\n",
    "\n",
    "\n",
    "class SubsetDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):\n",
    "        options = dict(size=size, factor=factor, seed=seed)\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)\n",
    "        if size is not None:\n",
    "            subset_size = size\n",
    "            if seed == 0:\n",
    "                alias = f\"{dataset}[:{size}]\"\n",
    "            else:\n",
    "                alias = f\"{dataset}[:{size};seed={seed}]\"\n",
    "        elif factor is not None:\n",
    "            subset_size = int(len(dataset) * factor)\n",
    "            if seed == 0:\n",
    "                alias = f\"{dataset}~x{factor}\"\n",
    "            else:\n",
    "                alias = f\"{dataset}~x{factor} (seed={seed})\"\n",
    "\n",
    "        self.indices = generator.choice(len(dataset), size=subset_size, replace=subset_size > len(dataset))\n",
    "\n",
    "        super().__init__(dataset, alias)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class ConstantTargetDataset(_AliasDataset):\n",
    "    target: object\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, target: object):\n",
    "        super().__init__(dataset, f\"{dataset} | constant_target{target}\")\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.target\n",
    "\n",
    "\n",
    "def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):\n",
    "    target = torch.ones(num_classes, dtype=dtype, device=device) / num_classes\n",
    "    result = ConstantTargetDataset(dataset, target)\n",
    "    result.options = dict(num_classes=num_classes)\n",
    "    result.alias = f\"{dataset} | uniform_targets{result.options}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To an OOD dataset, one can either use:\n",
    "```\n",
    "MNIST+OOD*0.5\n",
    "```\n",
    "and then use `get_base_dataset(dataset, index) == OOD` to check whether a picked sample is OOD (see below).\n",
    "\n",
    "Alternatively, we can use:\n",
    "```\n",
    "OneHotDataset(MNIST) + UniformTargetDataset(OOD * 0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset'~x0.1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset'x3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MNISTDataset'~x0.1 | one_hot_targets{'num_classes': 10}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_MNIST = OneHotDataset(MNIST * 0.1, num_classes=10)\n",
    "print(one_hot_MNIST[0][1])\n",
    "\n",
    "one_hot_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset' | uniform_targets{'num_classes': 10}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniformTargetDataset(MNIST, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that for large datasets, precomputing the noise to be added can use up a lot of memory (doubling the dataset size). Creating a new random generator for each sample is too slow, so it might be worth creating an entirely new dataset.\n",
    "\n",
    "However, we support exporting and importing datasets, so simply storing and loading the dataset is an option.\n",
    "\n",
    "> Tip: Do not use this on very large datasets (e.g. ImageNet)... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class AdditiveGaussianNoise(_AliasDataset):\n",
    "    noise: torch.Tensor\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, sigma: float):\n",
    "        sample = dataset[0][0]\n",
    "        self.noise = torch.randn(len(dataset), *sample.shape, device=sample.device)\n",
    "        self.options = dict(sigma=sigma)\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} + 𝓝(0;σ={sigma})\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.dataset[idx]\n",
    "        return sample + self.noise[idx], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ZeroDataset' + 𝓝(0;σ=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(0.7870), tensor(0.)),\n",
       " (tensor(1.0932), tensor(0.)),\n",
       " (tensor(-1.5405), tensor(0.)),\n",
       " (tensor(1.2409), tensor(0.)),\n",
       " (tensor(0.2093), tensor(0.)),\n",
       " (tensor(-0.1016), tensor(0.)),\n",
       " (tensor(-0.3672), tensor(0.)),\n",
       " (tensor(0.4202), tensor(0.)),\n",
       " (tensor(-1.1063), tensor(0.)),\n",
       " (tensor(-0.7468), tensor(0.))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_zero = AdditiveGaussianNoise(zero_dataset, sigma=1)\n",
    "\n",
    "print(noisy_zero)\n",
    "\n",
    "list(noisy_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Datasets\n",
    "\n",
    "Finally, to make it easier to use datasets across Python versions, we allow dataset exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def dataset_to_tensors(dataset):\n",
    "    samples = []\n",
    "    targets = []\n",
    "\n",
    "    for sample, target in dataset:\n",
    "        samples.append(sample.to(device=\"cpu\", non_blocking=True))\n",
    "        targets.append(target.to(device=\"cpu\", non_blocking=True))\n",
    "\n",
    "    samples = torch.stack(samples)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return samples, targets\n",
    "\n",
    "\n",
    "def get_dataset_state_dict(dataset):\n",
    "    dataset_alias = repr(dataset)\n",
    "\n",
    "    samples, targets = dataset_to_tensors(dataset)\n",
    "\n",
    "    state_dict = dict(alias=dataset_alias, samples=samples, targets=targets)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "class ImportedDataset(_AliasDataset):\n",
    "    def __init__(self, state_dict, device=None):\n",
    "        tensor_dataset = data.TensorDataset(state_dict[\"samples\"], state_dict[\"targets\"])\n",
    "        super().__init__(tensor_dataset, state_dict[\"alias\"])\n",
    "\n",
    "\n",
    "def save_dataset(dataset: data.Dataset, f, **kwargs):\n",
    "    torch.save(get_dataset_state_dict(dataset), f, **kwargs)\n",
    "\n",
    "\n",
    "def load_dataset(f, map_location=None, **kwargs):\n",
    "    state_dict = torch.load(f, map_location=map_location, **kwargs)\n",
    "    dataset = ImportedDataset(state_dict)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dataset = NamedDataset(data.TensorDataset(torch.arange(0, 10), torch.arange(90, 100)), \"LinearDataset\")\n",
    "\n",
    "samples, targets = dataset_to_tensors(linear_dataset)\n",
    "\n",
    "assert all(samples == torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "assert all(targets == torch.tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alias': \"'LinearDataset'\",\n",
       " 'samples': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'targets': tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_state_dict(linear_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(linear_dataset, \"linear.dataset\")\n",
    "\n",
    "loaded_linear_dataset = load_dataset(\"linear.dataset\")\n",
    "\n",
    "loaded_samples, loaded_targets = dataset_to_tensors(loaded_linear_dataset)\n",
    "\n",
    "assert all(loaded_samples == samples)\n",
    "assert all(loaded_targets == targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Methods\n",
    "\n",
    "We sometimes want to:\n",
    "\n",
    "* obtain the base dataset for a certain index (for OOD detection)\n",
    "* get only a target for a certain index; or\n",
    "* get all targets for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def get_base_dataset(dataset, index):\n",
    "    if isinstance(dataset, NamedDataset):\n",
    "        return dataset\n",
    "    elif isinstance(dataset, data.ConcatDataset):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return get_base_dataset(dataset.datasets[dataset_idx], sample_idx)\n",
    "    elif isinstance(dataset, ImbalancedDataset):\n",
    "        return get_base_dataset(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, ImbalancedClassSplitDataset):\n",
    "        return get_base_dataset(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, SubsetDataset):\n",
    "        return get_base_dataset(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        return get_base_dataset(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, RepeatedDataset):\n",
    "        return get_base_dataset(dataset.dataset, index % len(dataset.dataset))\n",
    "    elif isinstance(dataset, _AliasDataset):\n",
    "        return get_base_dataset(dataset.dataset, index)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_base_index(dataset, index):\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return get_base_index(dataset.datasets[dataset_idx], sample_idx)\n",
    "    elif isinstance(dataset, ImbalancedDataset):\n",
    "        return get_base_index(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, ImbalancedClassSplitDataset):\n",
    "        return get_base_index(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, SubsetDataset):\n",
    "        return get_base_index(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        return get_base_index(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, RepeatedDataset):\n",
    "        return get_base_index(dataset.dataset, index % len(dataset.dataset))\n",
    "    elif isinstance(dataset, _AliasDataset):\n",
    "        return get_base_index(dataset.dataset, index)\n",
    "    elif isinstance(dataset, data.TensorDataset):\n",
    "        return index\n",
    "    elif isinstance(dataset, torchvision.datasets.MNIST):\n",
    "        return index\n",
    "    elif isinstance(dataset, torchvision.datasets.CIFAR10):\n",
    "        return index\n",
    "\n",
    "    raise NotImplementedError(f\"Unrecognized dataset {dataset}!\")\n",
    "\n",
    "\n",
    "def get_target(dataset, index):\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return get_target(dataset.datasets[dataset_idx], sample_idx)\n",
    "    elif isinstance(dataset, CorruptedLabelsDataset):\n",
    "        return get_target(dataset.implementation, index)\n",
    "    elif isinstance(dataset, OverridenTargetDataset):\n",
    "        if index not in dataset.reverse_indices:\n",
    "            return get_target(dataset.dataset, index)\n",
    "\n",
    "        ridx = dataset.reverse_indices[index]\n",
    "        new_y = dataset.new_targets[ridx]\n",
    "        return new_y\n",
    "    elif isinstance(dataset, RandomLabelsDataset):\n",
    "        return dataset.new_labels[index]\n",
    "    elif isinstance(dataset, ImbalancedDataset):\n",
    "        return get_target(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, ImbalancedClassSplitDataset):\n",
    "        return get_target(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, SubsetDataset):\n",
    "        return get_target(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        return get_target(dataset.dataset, dataset.indices[index])\n",
    "    elif isinstance(dataset, RepeatedDataset):\n",
    "        return get_target(dataset.dataset, index % len(dataset.dataset))\n",
    "    elif isinstance(dataset, OneHotDataset):\n",
    "        return dataset.targets[index]\n",
    "    elif isinstance(dataset, ConstantTargetDataset):\n",
    "        return dataset.target\n",
    "    elif isinstance(dataset, _AliasDataset):\n",
    "        return get_target(dataset.dataset, index)\n",
    "    elif isinstance(dataset, data.TensorDataset):\n",
    "        return dataset.tensors[1][index]\n",
    "    elif isinstance(dataset, torchvision.datasets.MNIST):\n",
    "        return dataset.targets[index]\n",
    "    elif isinstance(dataset, torchvision.datasets.CIFAR10):\n",
    "        return dataset.targets[index]\n",
    "\n",
    "    raise NotImplementedError(f\"Unrecognized dataset {dataset}!\")\n",
    "\n",
    "\n",
    "def get_targets(dataset):\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        return torch.concat([get_targets(sub_dataset) for sub_dataset in dataset.datasets], device=\"cpu\")\n",
    "    elif isinstance(dataset, CorruptedLabelsDataset):\n",
    "        return get_targets(dataset.implementation)\n",
    "    elif isinstance(dataset, OverridenTargetDataset):\n",
    "        targets = torch.clone(get_targets(dataset.dataset))\n",
    "        targets[list(dataset.reverse_indices.keys())] = torch.as_tensor(dataset.new_targets)\n",
    "        return targets\n",
    "    elif isinstance(dataset, RandomLabelsDataset):\n",
    "        return dataset.new_labels\n",
    "    elif isinstance(dataset, ImbalancedDataset):\n",
    "        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]\n",
    "    elif isinstance(dataset, ImbalancedClassSplitDataset):\n",
    "        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]\n",
    "    elif isinstance(dataset, SubsetDataset):\n",
    "        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]\n",
    "    elif isinstance(dataset, Subset):\n",
    "        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]\n",
    "    elif isinstance(dataset, RepeatedDataset):\n",
    "        return get_targets(dataset.dataset).repeat(dataset.num_repeats)\n",
    "    elif isinstance(dataset, OneHotDataset):\n",
    "        return dataset.targets\n",
    "    elif isinstance(dataset, ConstantTargetDataset):\n",
    "        return dataset.target.expand(len(dataset), *dataset.target.shape)\n",
    "    elif isinstance(dataset, _AliasDataset):\n",
    "        return get_targets(dataset.dataset)\n",
    "    elif isinstance(dataset, data.TensorDataset):\n",
    "        return torch.as_tensor(dataset.tensors[1])\n",
    "    elif isinstance(dataset, torchvision.datasets.MNIST):\n",
    "        return torch.as_tensor(dataset.targets)\n",
    "    elif isinstance(dataset, torchvision.datasets.CIFAR10):\n",
    "        return torch.as_tensor(dataset.targets)\n",
    "\n",
    "    raise NotImplementedError(f\"Unrecognized dataset {dataset} with type {type(dataset)}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def create_repeated_MNIST_dataset(*, device=None, num_repetitions: int = 3, add_noise: bool = True):\n",
    "    # num_classes = 10, input_size = 28\n",
    "\n",
    "    train_dataset = NamedDataset(FastMNIST(\"data\", train=True, download=True, device=device), \"FastMNIST (Train)\")\n",
    "\n",
    "    rmnist_train_dataset = train_dataset\n",
    "    if num_repetitions > 1:\n",
    "        rmnist_train_dataset = train_dataset * num_repetitions\n",
    "\n",
    "    if add_noise:\n",
    "        rmnist_train_dataset = AdditiveGaussianNoise(rmnist_train_dataset, 0.1)\n",
    "\n",
    "    test_dataset = NamedDataset(FastMNIST(\"data\", train=False, device=device), \"FastMNIST (Test)\")\n",
    "\n",
    "    return rmnist_train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def create_MNIST_dataset(device=None):\n",
    "    return create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FastMNIST (Train)'x2 + 𝓝(0;σ=0.1), 'FastMNIST (Test)')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmnist_example = create_repeated_MNIST_dataset(device=\"cpu\", num_repetitions=2, add_noise=True)\n",
    "rmnist_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_targets(rmnist_example[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FastMNIST (Train)'x2 + 𝓝(0;σ=0.1)) + 'FastMNIST (Test)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmnist_example[0] + rmnist_example[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
