{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Challenges\n",
    "> â€œWhoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you.â€\n",
    ">\n",
    "> â€• Friedrich Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model real-world use cases better, we need:\n",
    "* redundant/duplicated data;\n",
    "* noisy labels (emulating noisy oracles);\n",
    "* class imbalance;\n",
    "* out-of-distribution data/outliers included in the unlabelled data;\n",
    "* noisy or ambiguous samples.\n",
    "\n",
    "RepeatedMNIST takes care of the first and last challenge in a very specific way. \n",
    "This chapter takes care of the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import bisect\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Set, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets\n",
    "\n",
    "from batchbald_redux.fast_mnist import FastFashionMNIST, FastMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monkey patch data.Dataset to have better doc strings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new, more extension friendly Dataset base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetIndex:\n",
    "    dataset: data.Dataset\n",
    "    index: int\n",
    "\n",
    "\n",
    "def _wrap_alias(dataset: data.Dataset):\n",
    "    if not isinstance(dataset, NamedDataset):\n",
    "        return repr(dataset)\n",
    "    return f\"({dataset.alias})\"\n",
    "\n",
    "\n",
    "class AliasDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset with an easier to understand alias. And convenience operators and methods.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset: data.Dataset\n",
    "    alias: str\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, alias: str):\n",
    "        self.dataset = dataset\n",
    "        self.alias = alias\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.alias\n",
    "\n",
    "    def __add__(self, other):\n",
    "        assert get_num_classes(self.dataset) == get_num_classes(other)\n",
    "        return AliasDataset(data.ConcatDataset([self, other]), f\"{_wrap_alias(self)} + {_wrap_alias(other)}\")\n",
    "\n",
    "    def __mul__(self, factor):\n",
    "        if int(factor) == factor:\n",
    "            return RepeatedDataset(self, num_repeats=int(factor))\n",
    "        if factor < 1:\n",
    "            return RandomSubsetDataset(self, factor=factor, seed=0)\n",
    "        return RepeatedDataset(self, num_repeats=int(factor)) + RandomSubsetDataset(self, factor=factor % 1, seed=0)\n",
    "\n",
    "    def __rmul__(self, factor):\n",
    "        return self * factor\n",
    "\n",
    "    def get_base_dataset_index(self, index):\n",
    "        return get_base_dataset_index(self.dataset, index)\n",
    "\n",
    "    def get_index(self, dataset_index: DatasetIndex) -> Optional[int]:\n",
    "        return get_index(self, dataset_index)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return get_num_classes(self.dataset)\n",
    "\n",
    "    def get_target(self, index, *, device=None):\n",
    "        return get_target(self.dataset, index)\n",
    "\n",
    "    def get_targets(self, device=None):\n",
    "        return get_targets(self.dataset)\n",
    "\n",
    "    def subset(self, indices: list):\n",
    "        return SubsetAliasDataset(self, indices)\n",
    "\n",
    "    def imbalance(self, *, class_counts: list, seed: int):\n",
    "        return ImbalancedDataset(self, class_counts=class_counts, seed=seed)\n",
    "\n",
    "    def override_targets(self, *, targets: Union[list, torch.Tensor], indices: list = None, num_classes: int = None):\n",
    "        if not indices:\n",
    "            return ReplaceTargetsDataset(self, targets=targets, num_classes=num_classes)\n",
    "        return OverrideTargetsDataset(self, indices=indices, targets=targets, num_classes=num_classes)\n",
    "\n",
    "    def corrupt_labels(self, *, size_corrupted: Union[float, int], seed: int, num_classes: int = None, device=None):\n",
    "        return CorruptedLabelsDataset(\n",
    "            self, size_corrupted=size_corrupted, seed=seed, num_classes=num_classes, device=device\n",
    "        )\n",
    "\n",
    "    def randomize_labels(self, *, seed: int, num_classes: int = None, device=None):\n",
    "        return RandomLabelsDataset(self, seed=seed, num_classes=num_classes, device=device)\n",
    "\n",
    "    # def imbalance_dataset_split(\n",
    "    #         self, *, num_majority_classes: int, majority_percentage: int, minority_percentage: int, seed: int\n",
    "    # ):\n",
    "    #     return ImbalancedClassSplitDataset(\n",
    "    #         self,\n",
    "    #         num_majority_classes=num_majority_classes,\n",
    "    #         majority_percentage=majority_percentage,\n",
    "    #         minority_percentage=minority_percentage,\n",
    "    #         seed=seed,\n",
    "    #     )\n",
    "    def imbalance_subsample(self, *, minority_classes: Set[int], minority_percentage: float, seed: int):\n",
    "        return ImbalancedClassSplitDataset(\n",
    "            self, minority_classes=minority_classes, minority_percentage=minority_percentage, seed=seed\n",
    "        )\n",
    "\n",
    "    def one_hot(self, *, dtype=None, device=None):\n",
    "        return OneHotDataset(self, dtype=dtype, device=device)\n",
    "\n",
    "    def constant_target(self, target: torch.Tensor, *, num_classes=None):\n",
    "        return ConstantTargetDataset(dataset=self, target=target, num_classes=num_classes)\n",
    "\n",
    "    def uniform_target(self, *, num_classes=None, dtype=None, device=None):\n",
    "        return UniformTargetDataset(self, num_classes=num_classes, dtype=dtype, device=device)\n",
    "\n",
    "    def split(self, indices=List[int]):\n",
    "        return split(self, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Methods\n",
    "\n",
    "We sometimes want to:\n",
    "\n",
    "* obtain the base dataset for a certain index (for OOD detection)\n",
    "* get only a target for a certain index; or\n",
    "* get all targets for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def get_base_dataset_index(dataset: data.Dataset, index: int) -> DatasetIndex:\n",
    "    if index < 0:\n",
    "        if -index > len(dataset):\n",
    "            raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "        index = len(dataset) + index\n",
    "\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        dataset_idx = bisect.bisect_right(dataset.cumulative_sizes, index)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = index\n",
    "        else:\n",
    "            sample_idx = index - dataset.cumulative_sizes[dataset_idx - 1]\n",
    "        return get_base_dataset_index(dataset.datasets[dataset_idx], int(sample_idx))\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        return get_base_dataset_index(dataset.dataset, int(dataset.indices[index]))\n",
    "    elif isinstance(dataset, data.TensorDataset):\n",
    "        return DatasetIndex(dataset, int(index))\n",
    "    elif isinstance(\n",
    "        dataset,\n",
    "        (\n",
    "            torchvision.datasets.MNIST,\n",
    "            torchvision.datasets.CIFAR10,\n",
    "            torchvision.datasets.SVHN,\n",
    "            torchvision.datasets.ImageFolder,\n",
    "        ),\n",
    "    ):\n",
    "        return DatasetIndex(dataset, int(index))\n",
    "    elif isinstance(dataset, AliasDataset):\n",
    "        return dataset.get_base_dataset_index(index)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unrecognized dataset {dataset}!\")\n",
    "\n",
    "\n",
    "def get_index(dataset: data.Dataset, dataset_index: DatasetIndex) -> Optional[int]:\n",
    "    if dataset == dataset_index.dataset:\n",
    "        return dataset_index.index\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        base_index = get_index(dataset.dataset, dataset_index)\n",
    "        if base_index is not None:\n",
    "            try:\n",
    "                index = dataset.indices.index(base_index)\n",
    "                return index\n",
    "            except ValueError:\n",
    "                pass\n",
    "    elif isinstance(dataset, data.ConcatDataset):\n",
    "        for index, sub_dataset in enumerate(dataset.datasets):\n",
    "            base_index = get_index(sub_dataset, dataset_index)\n",
    "            if base_index is not None:\n",
    "                if index == 0:\n",
    "                    return base_index\n",
    "                else:\n",
    "                    return dataset.cumulative_sizes[index - 1] + base_index\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_num_classes(dataset: data.Dataset) -> int:\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        return get_num_classes(dataset.datasets[0])\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        return get_num_classes(dataset.dataset)\n",
    "    elif isinstance(\n",
    "        dataset, (torchvision.datasets.MNIST, torchvision.datasets.CIFAR10, torchvision.datasets.ImageFolder)\n",
    "    ):\n",
    "        return len(dataset.classes)\n",
    "    elif isinstance(dataset, torchvision.datasets.SVHN):\n",
    "        return 10\n",
    "    elif isinstance(dataset, AliasDataset):\n",
    "        return dataset.get_num_classes()\n",
    "    elif hasattr(dataset, \"num_classes\"):\n",
    "        return dataset.num_classes\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unrecognized dataset {dataset}!\")\n",
    "\n",
    "\n",
    "def get_target(dataset, index: int, *, device=None):\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        if index < 0:\n",
    "            if -index > len(dataset):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            index = len(dataset) + index\n",
    "        dataset_idx = bisect.bisect_right(dataset.cumulative_sizes, index)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = index\n",
    "        else:\n",
    "            sample_idx = index - dataset.cumulative_sizes[dataset_idx - 1]\n",
    "        target = get_target(dataset.datasets[dataset_idx], sample_idx, device=device)\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        target = get_target(dataset.dataset, dataset.indices[index], device=device)\n",
    "    elif isinstance(dataset, data.TensorDataset):\n",
    "        target = dataset.tensors[1][index]\n",
    "    elif isinstance(dataset, (torchvision.datasets.MNIST, torchvision.datasets.CIFAR10, torchvision.datasets.ImageFolder)):\n",
    "        target = dataset.targets[index]\n",
    "    elif isinstance(dataset, torchvision.datasets.SVHN):\n",
    "        target = dataset.labels[index]\n",
    "    elif isinstance(dataset, AliasDataset):\n",
    "        target = dataset.get_target(index, device=device)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unrecognized dataset {dataset}!\")\n",
    "    return torch.as_tensor(target, device=device)\n",
    "\n",
    "\n",
    "def get_targets(dataset, *, device=None) -> torch.Tensor:\n",
    "    if isinstance(dataset, data.ConcatDataset):\n",
    "        # Move all tensors to the same device for torch.cat. When in doubt, use the cpu.\n",
    "        return torch.cat([get_targets(sub_dataset, device=device or \"cpu\") for sub_dataset in dataset.datasets])\n",
    "    elif isinstance(dataset, data.Subset):\n",
    "        return get_targets(dataset.dataset, device=device)[torch.as_tensor(dataset.indices)]\n",
    "    elif isinstance(dataset, data.TensorDataset):\n",
    "        return torch.as_tensor(dataset.tensors[1], device=device)\n",
    "    elif isinstance(dataset, (torchvision.datasets.MNIST, torchvision.datasets.CIFAR10, torchvision.datasets.ImageFolder)):\n",
    "        return torch.as_tensor(dataset.targets, device=device)\n",
    "    elif isinstance(dataset, torchvision.datasets.SVHN):\n",
    "        return torch.as_tensor(dataset.labels, device=device)\n",
    "    elif isinstance(dataset, AliasDataset):\n",
    "        return dataset.get_targets(device=device)\n",
    "\n",
    "    raise NotImplementedError(f\"Unrecognized dataset {dataset} with type {type(dataset)}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class SubsetAliasDataset(AliasDataset):\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, indices: list, alias: str = None):\n",
    "        alias = alias or f\"{dataset}[{indices}]\"\n",
    "\n",
    "        super().__init__(dataset, alias)\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def get_base_dataset_index(self, index) -> DatasetIndex:\n",
    "        return get_base_dataset_index(self.dataset, self.indices[index])\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return get_num_classes(self.dataset)\n",
    "\n",
    "    def get_target(self, index, *, device=None) -> torch.Tensor:\n",
    "        return get_target(self.dataset, self.indices[index], device=device)\n",
    "\n",
    "    def get_targets(self, device=None) -> torch.Tensor:\n",
    "        return get_targets(self.dataset, device=device)[torch.as_tensor(self.indices)]\n",
    "\n",
    "\n",
    "class ReplaceTargetsDataset(AliasDataset):\n",
    "    targets: torch.Tensor\n",
    "    num_classes: int\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, targets: torch.Tensor, *, num_classes: int = None, alias: str = None):\n",
    "        assert len(dataset) == len(targets)\n",
    "\n",
    "        num_classes = num_classes or get_num_classes(dataset)\n",
    "        alias = alias or f\"{dataset} | replace_targets{dict(targets=targets, num_classes=num_classes)}\"\n",
    "\n",
    "        super().__init__(dataset, alias)\n",
    "        self.num_classes = num_classes\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, _ = self.dataset[index]\n",
    "        return data, self.targets[index]\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_target(self, index, *, device=None):\n",
    "        return torch.as_tensor(self.targets[index], device=device)\n",
    "\n",
    "    def get_targets(self, device=None):\n",
    "        return torch.as_tensor(self.targets, device=device)\n",
    "\n",
    "\n",
    "class NamedDataset(AliasDataset):\n",
    "    def __init__(self, dataset: data.Dataset, name: str):\n",
    "        super().__init__(dataset, repr(name))\n",
    "\n",
    "    def get_base_dataset_index(self, index) -> DatasetIndex:\n",
    "        # NamedDatasets are a leaf for all purposes.\n",
    "        return DatasetIndex(self, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "\n",
    "MNIST = NamedDataset(FastMNIST(root=\"data/\", download=True, device=\"cpu\"), name=\"MNISTDataset\")\n",
    "\n",
    "MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "def split(dataset: data.Dataset, positive_indices: List[int]):\n",
    "    positive_subset = SubsetAliasDataset(dataset, positive_indices)\n",
    "\n",
    "    mask = np.full((len(dataset),), True)\n",
    "    mask[positive_indices]=False\n",
    "    negative_indices = np.nonzero(mask)[0]\n",
    "    negative_subset = SubsetAliasDataset(dataset, negative_indices, alias=f\"{dataset}[~{positive_indices}]\")\n",
    "    return positive_subset, negative_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MNISTDataset'[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       " 'MNISTDataset'[~[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(MNIST, list(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class OverrideTargetsDataset(AliasDataset):\n",
    "    reverse_indices: dict\n",
    "    new_targets: list\n",
    "    num_classes: int\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset: data.Dataset, *, indices: list, targets: Union[list, torch.Tensor], num_classes: int = None\n",
    "    ):\n",
    "        assert len(indices) == len(targets)\n",
    "\n",
    "        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}\n",
    "        self.targets = targets\n",
    "        self.num_classes = num_classes or get_num_classes(self.dataset)\n",
    "\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            f\"{dataset} | override_targets{dict(indices=indices, targets=targets, num_classes=num_classes)}\",\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.dataset[index]\n",
    "\n",
    "        if index not in self.reverse_indices:\n",
    "            return data, target\n",
    "\n",
    "        reverse_index = self.reverse_indices[index]\n",
    "        new_target = self.targets[reverse_index]\n",
    "        return data, new_target\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_target(self, index, *, device=None):\n",
    "        if index not in self.reverse_indices:\n",
    "            return get_target(self.dataset, index, device=device)\n",
    "\n",
    "        ridx = self.reverse_indices[index]\n",
    "        new_target = self.targets[ridx]\n",
    "        return torch.as_tensor(new_target, device=device)\n",
    "\n",
    "    def get_targets(self, device=None):\n",
    "        targets = torch.clone(get_targets(self.dataset))\n",
    "        targets[list(self.reverse_indices.keys())] = torch.as_tensor(self.targets)\n",
    "        return torch.as_tensor(targets, device=device)\n",
    "\n",
    "\n",
    "class CorruptedLabelsDataset(AliasDataset):\n",
    "    options: dict\n",
    "    implementation: OverrideTargetsDataset\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: data.Dataset,\n",
    "        *,\n",
    "        size_corrupted: Union[float, int],\n",
    "        seed: int,\n",
    "        num_classes: int = None,\n",
    "        device=None,\n",
    "    ):\n",
    "        num_classes = num_classes or get_num_classes(dataset)\n",
    "        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} | corrupt_labels{options}\")\n",
    "        self.options = options\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        N = len(dataset)\n",
    "\n",
    "        if size_corrupted > 1:\n",
    "            num_corrupted = size_corrupted\n",
    "        else:\n",
    "            num_corrupted = int(N * size_corrupted)\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        indices = generator.choice(N, size=num_corrupted, replace=False)\n",
    "        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)\n",
    "\n",
    "        self.implementation = OverrideTargetsDataset(\n",
    "            dataset, indices=indices, targets=torch.as_tensor(new_targets, device=device), num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.implementation[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.implementation)\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_target(self, index, *, device=None):\n",
    "        return self.implementation.get_target(index, device=device)\n",
    "\n",
    "    def get_targets(self, device=None):\n",
    "        return self.implementation.get_targets(device=device)\n",
    "\n",
    "\n",
    "class RandomLabelsDataset(ReplaceTargetsDataset):\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, seed: int, num_classes: int = None, device=None):\n",
    "        num_classes = num_classes or get_num_classes(dataset)\n",
    "        options = dict(num_classes=num_classes, seed=seed)\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        N = len(dataset)\n",
    "        targets = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)\n",
    "\n",
    "        super().__init__(dataset, targets, num_classes=num_classes, alias=f\"{dataset} | randomize_labels{options}\")\n",
    "        self.options = options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZeroDataset' | corrupt_labels{'size_corrupted': 0.5, 'num_classes': 10, 'seed': 1}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_dataset = NamedDataset(data.TensorDataset(torch.zeros(10), torch.zeros(10)), \"ZeroDataset\")\n",
    "\n",
    "corrupted_labels_dataset = CorruptedLabelsDataset(zero_dataset, size_corrupted=0.5, num_classes=10, seed=1)\n",
    "\n",
    "assert list(corrupted_labels_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "]\n",
    "\n",
    "corrupted_labels_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZeroDataset' | randomize_labels{'num_classes': 10, 'seed': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_dataset = RandomLabelsDataset(zero_dataset, num_classes=10, seed=2)\n",
    "\n",
    "assert list(corrupted_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(1)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(6)),\n",
    "]\n",
    "\n",
    "corrupted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def get_class_indices_by_class(\n",
    "    dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator\n",
    ") -> Dict[int, List[int]]:\n",
    "    class_counts = list(class_counts)\n",
    "\n",
    "    subset_indices = {label: [] for label in range(len(class_counts))}\n",
    "\n",
    "    remaining_samples = sum(class_counts)\n",
    "\n",
    "    indices = generator.permutation(len(dataset))\n",
    "    targets = get_targets(dataset)\n",
    "    for index in indices:\n",
    "        target = targets[index].item()\n",
    "\n",
    "        if class_counts[target] > 0:\n",
    "            subset_indices[target].append(index)\n",
    "            class_counts[target] -= 1\n",
    "            remaining_samples -= 1\n",
    "\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "\n",
    "    return subset_indices\n",
    "\n",
    "\n",
    "def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator) -> List[int]:\n",
    "    indices_by_class = get_class_indices_by_class(dataset=dataset, class_counts=class_counts, generator=generator)\n",
    "    return [index for by_class in indices_by_class.values() for index in by_class]\n",
    "\n",
    "\n",
    "def get_balanced_sample_indices(dataset: data.Dataset, *, num_classes, samples_per_class, seed: int) -> List[int]:\n",
    "    class_counts = [samples_per_class] * num_classes\n",
    "    generator = np.random.default_rng(seed)\n",
    "\n",
    "    return get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "\n",
    "def get_balanced_sample_indices_by_class(\n",
    "    dataset: data.Dataset, *, num_classes, samples_per_class, seed: int\n",
    ") -> Dict[int, int]:\n",
    "    class_counts = [samples_per_class] * num_classes\n",
    "    generator = np.random.default_rng(seed)\n",
    "\n",
    "    return get_class_indices_by_class(dataset, class_counts=class_counts, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class ImbalancedDataset(SubsetAliasDataset):\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):\n",
    "        options = dict(class_counts=class_counts, seed=seed)\n",
    "        generator = np.random.default_rng(seed)\n",
    "        indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "        super().__init__(dataset, indices, f\"ImbalancedDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "\n",
    "class ImbalancedClassSplitDataset(SubsetAliasDataset):\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, minority_classes: Set[int], minority_percentage: float, seed: int):\n",
    "        num_samples = len(dataset)\n",
    "        num_classes = get_num_classes(dataset)\n",
    "        assert len(minority_classes) < num_classes\n",
    "\n",
    "        samples_per_class = num_samples // num_classes\n",
    "        class_counts = [\n",
    "            int(samples_per_class * minority_percentage / 100) if i in minority_classes else samples_per_class\n",
    "            for i in range(num_classes)\n",
    "        ]\n",
    "\n",
    "        # num_samples_per_class = len(dataset) // num_classes\n",
    "        # num_samples_majority = num_samples_per_class * majority_percentage // 100\n",
    "        # num_samples_minority = num_samples_per_class * minority_percentage // 100\n",
    "        #\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        # class_counts = [num_samples_majority] * num_majority_classes + [num_samples_minority] * (num_classes - num_majority_classes)\n",
    "        # class_counts = list(generator.permuted(class_counts))\n",
    "\n",
    "        indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "        options = dict(\n",
    "            # num_majority_classes=num_majority_classes,\n",
    "            # majority_percentage=majority_percentage,\n",
    "            minority_classes=minority_classes,\n",
    "            minority_percentage=minority_percentage,\n",
    "            seed=seed,\n",
    "            class_counts=class_counts,\n",
    "        )\n",
    "        super().__init__(dataset, indices, f\"ImbalancedClassSplitDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 6, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dataset = NamedDataset(data.TensorDataset(torch.arange(9), torch.as_tensor(list(range(3)) * 3)), \"123\")\n",
    "\n",
    "imbalanced_indices = get_class_indices(three_dataset, class_counts=[3, 0, 0], generator=np.random.default_rng())\n",
    "\n",
    "three_dataset[imbalanced_indices][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset='123', {'class_counts': [1, 2, 3], 'seed': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbalanced_dataset = ImbalancedDataset(three_dataset, class_counts=[1, 2, 3], seed=2)\n",
    "print(imbalanced_dataset[:][1])\n",
    "imbalanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImbalancedClassSplitDataset(dataset='MNISTDataset', {'minority_classes': [1, 2], 'minority_percentage': 0, 'seed': 1, 'class_counts': [6000, 0, 0, 6000, 6000, 6000, 6000, 6000, 6000, 6000]})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImbalancedClassSplitDataset(MNIST, minority_classes=[1, 2], minority_percentage=0, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImbalancedClassSplitDataset(dataset='MNISTDataset', {'minority_classes': [1, 2], 'minority_percentage': 50, 'seed': 1, 'class_counts': [6000, 3000, 3000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImbalancedClassSplitDataset(MNIST, minority_classes=[1, 2], minority_percentage=50, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing in OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "# Convert label dataset to one hot\n",
    "class OneHotDataset(ReplaceTargetsDataset):\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes=None, dtype=None, device=None):\n",
    "        num_classes = num_classes or get_num_classes(dataset)\n",
    "        options = dict(num_classes=num_classes)\n",
    "\n",
    "        N = len(dataset)\n",
    "        targets = torch.zeros(N, num_classes, dtype=dtype, device=device)\n",
    "        for i, label in enumerate(get_targets(dataset)):\n",
    "            targets[i, label.item()] = 1.0\n",
    "\n",
    "        super().__init__(dataset, targets, num_classes=num_classes, alias=f\"{dataset} | one_hot_targets{options}\")\n",
    "        self.options = options\n",
    "\n",
    "\n",
    "class RepeatedDataset(AliasDataset):\n",
    "    def __init__(self, dataset: data.Dataset, *, num_repeats: int, interleaved:bool=None):\n",
    "        self.num_repeats = num_repeats\n",
    "        self.interleaved = interleaved if interleaved is not None else True\n",
    "\n",
    "        alias = f\"{_wrap_alias(dataset)}x{num_repeats}\"\n",
    "        if not self.interleaved:\n",
    "            alias += \" (non-interleaved)\"\n",
    "\n",
    "        super().__init__(dataset, alias)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx > len(self) or idx < -len(self):\n",
    "            # Fail out using the standard error message if we are out-of-bounds.\n",
    "            return self.dataset[idx]\n",
    "\n",
    "        if idx < 0:\n",
    "            idx = len(self) + idx\n",
    "\n",
    "        if self.interleaved:\n",
    "            return self.dataset[idx // self.num_repeats]\n",
    "        else:\n",
    "            return self.dataset[idx % len(self.dataset)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.num_repeats\n",
    "\n",
    "    def get_base_dataset_index(self, index):\n",
    "        return get_base_dataset_index(self.dataset, index % len(self.dataset))\n",
    "\n",
    "    def get_target(self, index, *, device=None):\n",
    "        return get_target(self.dataset, index % len(self.dataset), device=device)\n",
    "\n",
    "    def get_targets(self, device=None):\n",
    "        return get_targets(self.dataset, device=device).repeat(self.num_repeats)\n",
    "\n",
    "\n",
    "class RandomSubsetDataset(SubsetAliasDataset):\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):\n",
    "        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)\n",
    "        if size is not None:\n",
    "            subset_size = size\n",
    "            if seed == 0:\n",
    "                alias = f\"{_wrap_alias(dataset)}[:{size}]\"\n",
    "            else:\n",
    "                alias = f\"{_wrap_alias(dataset)}[:{size};seed={seed}]\"\n",
    "        elif factor is not None:\n",
    "            subset_size = int(len(dataset) * factor)\n",
    "            if seed == 0:\n",
    "                alias = f\"{_wrap_alias(dataset)}~x{factor}\"\n",
    "            else:\n",
    "                alias = f\"{_wrap_alias(dataset)}~x{factor} (seed={seed})\"\n",
    "\n",
    "        N = len(dataset)\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        assert subset_size < N\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        indices = generator.choice(N, size=subset_size, replace=False)\n",
    "\n",
    "        # noinspection PyUnboundLocalVariable\n",
    "        super().__init__(dataset, indices, alias)\n",
    "        self.options = dict(size=size, factor=factor, seed=seed)\n",
    "\n",
    "\n",
    "class ConstantTargetDataset(AliasDataset):\n",
    "    target: torch.Tensor\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, target: torch.Tensor, num_classes=None):\n",
    "        num_classes = num_classes or get_num_classes(dataset)\n",
    "        super().__init__(dataset, f\"{dataset} | constant_target{dict(target=target, num_classes=num_classes)}\")\n",
    "        self.target = target\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.target\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_target(self, index, *, device=None):\n",
    "        return torch.as_tensor(self.target, device=device)\n",
    "\n",
    "    def get_targets(self, device=None):\n",
    "        return torch.as_tensor(self.target, device=device).expand(len(self), *self.target.shape)\n",
    "\n",
    "\n",
    "def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int = None, dtype=None, device=None):\n",
    "    num_classes = num_classes or get_num_classes(dataset)\n",
    "    target = torch.ones(num_classes, dtype=dtype, device=device) / num_classes\n",
    "    result = ConstantTargetDataset(dataset, target, num_classes=num_classes)\n",
    "    result.options = dict(num_classes=num_classes)\n",
    "    result.alias = f\"{dataset} | uniform_targets{result.options}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To an OOD dataset, one can either use:\n",
    "```\n",
    "MNIST+OOD*0.5\n",
    "```\n",
    "and then use `get_base_dataset(dataset, index) == OOD` to check whether a picked sample is OOD (see below).\n",
    "\n",
    "Alternatively, we can use:\n",
    "```\n",
    "OneHotDataset(MNIST) + UniformTargetDataset(OOD * 0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MNISTDataset')~x0.1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MNIST * 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MNISTDataset')x3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MNISTDataset')x3 + ('MNISTDataset')~x0.5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST * 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_3 = MNIST * 2\n",
    "assert torch.equal(MNIST_3[0][0], MNIST_3[1][0])\n",
    "assert not torch.equal(MNIST_3[1][0], MNIST_3[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('MNISTDataset')~x0.1 | one_hot_targets{'num_classes': 10}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_MNIST = OneHotDataset(MNIST * 0.1)\n",
    "print(one_hot_MNIST[0][1])\n",
    "\n",
    "one_hot_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset' | uniform_targets{'num_classes': 10}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniformTargetDataset(MNIST, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that for large datasets, precomputing the noise to be added can use up a lot of memory (doubling the dataset size). Creating a new random generator for each sample is too slow, so it might be worth creating an entirely new dataset.\n",
    "\n",
    "However, we support exporting and importing datasets, so simply storing and loading the dataset is an option.\n",
    "\n",
    "> Tip: Do not use this on very large datasets (e.g. ImageNet)... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class AdditiveGaussianNoise(AliasDataset):\n",
    "    noise: torch.Tensor\n",
    "    options: dict\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, sigma: float, *, max_num_noise_samples=None):\n",
    "        sample = dataset[0][0]\n",
    "\n",
    "        # Goal: easily add redundancy without OOM.\n",
    "        if max_num_noise_samples is None:\n",
    "            # A \"big\" prime, so it is unlikely we will repeat the noise\n",
    "            max_num_noise_samples = 16273\n",
    "\n",
    "        num_noise_samples = min(len(dataset), max_num_noise_samples)\n",
    "\n",
    "        self.noise = sigma * torch.randn(num_noise_samples, *sample.shape, device=sample.device)\n",
    "        self.options = dict(sigma=sigma, max_num_noise_samples=max_num_noise_samples)\n",
    "\n",
    "        super().__init__(dataset, f\"{dataset} + ð“(0;Ïƒ={sigma})\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.dataset[idx]\n",
    "        return sample + self.noise[idx % len(self.noise)], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ZeroDataset' + ð“(0;Ïƒ=0.1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(-0.2434), tensor(0.)),\n",
       " (tensor(0.0644), tensor(0.)),\n",
       " (tensor(0.0256), tensor(0.)),\n",
       " (tensor(-0.2434), tensor(0.)),\n",
       " (tensor(0.0644), tensor(0.)),\n",
       " (tensor(0.0256), tensor(0.)),\n",
       " (tensor(-0.2434), tensor(0.)),\n",
       " (tensor(0.0644), tensor(0.)),\n",
       " (tensor(0.0256), tensor(0.)),\n",
       " (tensor(-0.2434), tensor(0.))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_zero = AdditiveGaussianNoise(zero_dataset, sigma=0.1, max_num_noise_samples=3)\n",
    "\n",
    "print(noisy_zero)\n",
    "\n",
    "list(noisy_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Datasets\n",
    "\n",
    "Finally, to make it easier to use datasets across Python versions, we allow dataset exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def dataset_to_tensors(dataset):\n",
    "    samples = []\n",
    "    targets = []\n",
    "\n",
    "    for sample, target in dataset:\n",
    "        samples.append(sample.to(device=\"cpu\", non_blocking=True))\n",
    "        targets.append(target.to(device=\"cpu\", non_blocking=True))\n",
    "\n",
    "    samples = torch.stack(samples)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return samples, targets\n",
    "\n",
    "\n",
    "def get_dataset_state_dict(dataset):\n",
    "    dataset_alias = repr(dataset)\n",
    "\n",
    "    samples, targets = dataset_to_tensors(dataset)\n",
    "\n",
    "    num_classes = get_num_classes(dataset)\n",
    "\n",
    "    state_dict = dict(alias=dataset_alias, samples=samples, targets=targets, num_classes=num_classes)\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "class ImportedDataset(AliasDataset):\n",
    "    def __init__(self, state_dict):\n",
    "        tensor_dataset = data.TensorDataset(state_dict[\"samples\"], state_dict[\"targets\"])\n",
    "        super().__init__(tensor_dataset, state_dict[\"alias\"])\n",
    "        self.num_classes = state_dict[\"num_classes\"]\n",
    "\n",
    "    def get_num_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "\n",
    "def save_dataset(dataset: data.Dataset, f, **kwargs):\n",
    "    torch.save(get_dataset_state_dict(dataset), f, **kwargs)\n",
    "\n",
    "\n",
    "def load_dataset(f, map_location=None, **kwargs):\n",
    "    state_dict = torch.load(f, map_location=map_location, **kwargs)\n",
    "    dataset = ImportedDataset(state_dict)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = data.TensorDataset(torch.arange(0, 10), torch.arange(90, 100))\n",
    "tensor_dataset.num_classes = 100\n",
    "linear_dataset = NamedDataset(tensor_dataset, \"LinearDataset\")\n",
    "\n",
    "samples, targets = dataset_to_tensors(linear_dataset)\n",
    "\n",
    "assert all(samples == torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "assert all(targets == torch.tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alias': \"'LinearDataset'\",\n",
       " 'samples': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'targets': tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " 'num_classes': 100}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_state_dict(linear_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(linear_dataset, \"linear.dataset\")\n",
    "\n",
    "loaded_linear_dataset = load_dataset(\"linear.dataset\")\n",
    "\n",
    "loaded_samples, loaded_targets = dataset_to_tensors(loaded_linear_dataset)\n",
    "\n",
    "assert all(loaded_samples == samples)\n",
    "assert all(loaded_targets == targets)\n",
    "assert get_num_classes(loaded_linear_dataset) == 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def create_named_mnist(\n",
    "    mnist_type: Union[Type[FastMNIST], Type[FastFashionMNIST]],\n",
    "    root: str,\n",
    "    *,\n",
    "    train: bool = True,\n",
    "    download: bool = False,\n",
    "    device=None,\n",
    "):\n",
    "    dataset = mnist_type(root=root, train=train, download=download, device=device)\n",
    "    name = f\"{mnist_type.__name__} {'Train' if train else 'Test'} ({len(dataset)} samples)\"\n",
    "    return NamedDataset(dataset, name)\n",
    "\n",
    "\n",
    "def create_repeated_MNIST_dataset(*, device=None, num_repetitions: int = 3, add_noise: bool = True):\n",
    "    # num_classes = 10, input_size = 28\n",
    "\n",
    "    train_dataset = NamedDataset(FastMNIST(\"data\", train=True, download=True, device=device), \"FastMNIST (Train)\")\n",
    "\n",
    "    rmnist_train_dataset = train_dataset\n",
    "    if num_repetitions > 1:\n",
    "        rmnist_train_dataset = train_dataset * num_repetitions\n",
    "\n",
    "    if add_noise:\n",
    "        rmnist_train_dataset = AdditiveGaussianNoise(rmnist_train_dataset, 0.1)\n",
    "\n",
    "    test_dataset = NamedDataset(FastMNIST(\"data\", train=False, device=device), \"FastMNIST (Test)\")\n",
    "\n",
    "    return rmnist_train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def create_MNIST_dataset(device=None):\n",
    "    return create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('FastMNIST (Train)')x2 + ð“(0;Ïƒ=0.1), 'FastMNIST (Test)')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmnist_example = create_repeated_MNIST_dataset(device=\"cpu\", num_repetitions=2, add_noise=True)\n",
    "rmnist_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_targets(rmnist_example[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FastMNIST (Train)')x2 + ð“(0;Ïƒ=0.1) + ('FastMNIST (Test)')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmnist_example[0] + rmnist_example[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNISTDataset' | one_hot_targets{'num_classes': 10}x2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST.one_hot() * 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
