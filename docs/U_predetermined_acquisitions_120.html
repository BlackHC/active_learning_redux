---

title: Evaluate Predetermined Top-K Acquisitions


keywords: fastai
sidebar: home_sidebar

summary: "Resistance is futile."
description: "Resistance is futile."
nb_path: "U_predetermined_acquisitions_120.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: U_predetermined_acquisitions_120.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Appended /home/blackhc/PycharmProjects/bald-ical/src to paths
Switched to directory /home/blackhc/PycharmProjects/bald-ical
%load_ext autoreload
%autoreload 2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Import modules and functions were are going to use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">traceback</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.utils.data</span>
<span class="kn">from</span> <span class="nn">blackhc.project</span> <span class="kn">import</span> <span class="n">is_run_from_ipython</span>
<span class="kn">from</span> <span class="nn">blackhc.project.experiment</span> <span class="kn">import</span> <span class="n">embedded_experiments</span>

<span class="kn">import</span> <span class="nn">batchbald_redux.acquisition_functions.bald</span>
<span class="kn">from</span> <span class="nn">batchbald_redux</span> <span class="kn">import</span> <span class="n">acquisition_functions</span><span class="p">,</span> <span class="n">baseline_acquisition_functions</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.acquisition_functions</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CandidateBatchComputer</span><span class="p">,</span>
    <span class="n">EvalDatasetBatchComputer</span><span class="p">,</span>
    <span class="n">EvalModelBatchComputer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.joint_entropy</span> <span class="kn">import</span> <span class="n">compute_entropy</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.acquisition_functions.bald</span> <span class="kn">import</span> <span class="n">get_bald_scores</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.black_box_model_training</span> <span class="kn">import</span> <span class="n">evaluate</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.dataset_challenges</span> <span class="kn">import</span> <span class="n">get_base_dataset_index</span><span class="p">,</span> <span class="n">get_target</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.di</span> <span class="kn">import</span> <span class="n">DependencyInjection</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.experiment_data</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ExperimentData</span><span class="p">,</span>
    <span class="n">ExperimentDataConfig</span><span class="p">,</span>
    <span class="n">OoDDatasetConfig</span><span class="p">,</span>
    <span class="n">StandardExperimentDataConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.models</span> <span class="kn">import</span> <span class="n">MnistModelTrainer</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.resnet_models</span> <span class="kn">import</span> <span class="n">Cifar10ModelTrainer</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.train_eval_model</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TrainEvalModel</span><span class="p">,</span>
    <span class="n">TrainSelfDistillationEvalModel</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.trained_model</span> <span class="kn">import</span> <span class="n">BayesianEnsembleModelTrainer</span><span class="p">,</span> <span class="n">ModelTrainer</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">additional_initial_acquisitions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">26919</span><span class="p">,</span>
    <span class="mi">43627</span><span class="p">,</span>
    <span class="mi">1666</span><span class="p">,</span>
    <span class="mi">354</span><span class="p">,</span>
    <span class="mi">23669</span><span class="p">,</span>
    <span class="mi">48412</span><span class="p">,</span>
    <span class="mi">48486</span><span class="p">,</span>
    <span class="mi">18284</span><span class="p">,</span>
    <span class="mi">51745</span><span class="p">,</span>
    <span class="mi">8120</span><span class="p">,</span>
    <span class="mi">41099</span><span class="p">,</span>
    <span class="mi">11397</span><span class="p">,</span>
    <span class="mi">17942</span><span class="p">,</span>
    <span class="mi">38275</span><span class="p">,</span>
    <span class="mi">9674</span><span class="p">,</span>
    <span class="mi">7069</span><span class="p">,</span>
    <span class="mi">2810</span><span class="p">,</span>
    <span class="mi">35239</span><span class="p">,</span>
    <span class="mi">1279</span><span class="p">,</span>
    <span class="mi">11383</span><span class="p">,</span>
    <span class="mi">2271</span><span class="p">,</span>
    <span class="mi">921</span><span class="p">,</span>
    <span class="mi">15619</span><span class="p">,</span>
    <span class="mi">32386</span><span class="p">,</span>
    <span class="mi">17830</span><span class="p">,</span>
    <span class="mi">1385</span><span class="p">,</span>
    <span class="mi">20850</span><span class="p">,</span>
    <span class="mi">16780</span><span class="p">,</span>
    <span class="mi">15765</span><span class="p">,</span>
    <span class="mi">6786</span><span class="p">,</span>
    <span class="mi">18938</span><span class="p">,</span>
    <span class="mi">46468</span><span class="p">,</span>
    <span class="mi">54880</span><span class="p">,</span>
    <span class="mi">14885</span><span class="p">,</span>
    <span class="mi">15543</span><span class="p">,</span>
    <span class="mi">13091</span><span class="p">,</span>
    <span class="mi">39530</span><span class="p">,</span>
    <span class="mi">9241</span><span class="p">,</span>
    <span class="mi">21243</span><span class="p">,</span>
    <span class="mi">48253</span><span class="p">,</span>
    <span class="mi">42363</span><span class="p">,</span>
    <span class="mi">31951</span><span class="p">,</span>
    <span class="mi">6689</span><span class="p">,</span>
    <span class="mi">20219</span><span class="p">,</span>
    <span class="mi">17178</span><span class="p">,</span>
    <span class="mi">26621</span><span class="p">,</span>
    <span class="mi">27534</span><span class="p">,</span>
    <span class="mi">3889</span><span class="p">,</span>
    <span class="mi">48169</span><span class="p">,</span>
    <span class="mi">38735</span><span class="p">,</span>
    <span class="mi">31664</span><span class="p">,</span>
    <span class="mi">20215</span><span class="p">,</span>
    <span class="mi">554</span><span class="p">,</span>
    <span class="mi">23576</span><span class="p">,</span>
    <span class="mi">46590</span><span class="p">,</span>
    <span class="mi">16769</span><span class="p">,</span>
    <span class="mi">11169</span><span class="p">,</span>
    <span class="mi">27305</span><span class="p">,</span>
    <span class="mi">10657</span><span class="p">,</span>
    <span class="mi">13150</span><span class="p">,</span>
    <span class="mi">935</span><span class="p">,</span>
    <span class="mi">50655</span><span class="p">,</span>
    <span class="mi">55379</span><span class="p">,</span>
    <span class="mi">36245</span><span class="p">,</span>
    <span class="mi">42634</span><span class="p">,</span>
    <span class="mi">34842</span><span class="p">,</span>
    <span class="mi">1785</span><span class="p">,</span>
    <span class="mi">52851</span><span class="p">,</span>
    <span class="mi">47218</span><span class="p">,</span>
    <span class="mi">29309</span><span class="p">,</span>
    <span class="mi">50342</span><span class="p">,</span>
    <span class="mi">20286</span><span class="p">,</span>
    <span class="mi">33050</span><span class="p">,</span>
    <span class="mi">35791</span><span class="p">,</span>
    <span class="mi">50338</span><span class="p">,</span>
    <span class="mi">42327</span><span class="p">,</span>
    <span class="mi">48649</span><span class="p">,</span>
    <span class="mi">49161</span><span class="p">,</span>
    <span class="mi">18021</span><span class="p">,</span>
    <span class="mi">37139</span><span class="p">,</span>
    <span class="mi">15876</span><span class="p">,</span>
    <span class="mi">21433</span><span class="p">,</span>
    <span class="mi">47625</span><span class="p">,</span>
    <span class="mi">29242</span><span class="p">,</span>
    <span class="mi">7308</span><span class="p">,</span>
    <span class="mi">29771</span><span class="p">,</span>
    <span class="mi">30687</span><span class="p">,</span>
    <span class="mi">9840</span><span class="p">,</span>
    <span class="mi">5501</span><span class="p">,</span>
    <span class="mi">26821</span><span class="p">,</span>
    <span class="mi">10240</span><span class="p">,</span>
    <span class="mi">54942</span><span class="p">,</span>
    <span class="mi">15126</span><span class="p">,</span>
    <span class="mi">2032</span><span class="p">,</span>
    <span class="mi">26545</span><span class="p">,</span>
    <span class="mi">31021</span><span class="p">,</span>
    <span class="mi">50273</span><span class="p">,</span>
    <span class="mi">45507</span><span class="p">,</span>
    <span class="mi">41945</span><span class="p">,</span>
    <span class="mi">45728</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">predetermind_acquisition_base_indices</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">26767</span><span class="p">,</span>
    <span class="mi">3091</span><span class="p">,</span>
    <span class="mi">10668</span><span class="p">,</span>
    <span class="mi">52645</span><span class="p">,</span>
    <span class="mi">4188</span><span class="p">,</span>
    <span class="mi">17698</span><span class="p">,</span>
    <span class="mi">21777</span><span class="p">,</span>
    <span class="mi">47170</span><span class="p">,</span>
    <span class="mi">4848</span><span class="p">,</span>
    <span class="mi">55040</span><span class="p">,</span>
    <span class="mi">2955</span><span class="p">,</span>
    <span class="mi">20452</span><span class="p">,</span>
    <span class="mi">47102</span><span class="p">,</span>
    <span class="mi">35440</span><span class="p">,</span>
    <span class="mi">25022</span><span class="p">,</span>
    <span class="mi">48046</span><span class="p">,</span>
    <span class="mi">1885</span><span class="p">,</span>
    <span class="mi">195</span><span class="p">,</span>
    <span class="mi">54748</span><span class="p">,</span>
    <span class="mi">2742</span><span class="p">,</span>
    <span class="mi">602</span><span class="p">,</span>
    <span class="mi">40966</span><span class="p">,</span>
    <span class="mi">17780</span><span class="p">,</span>
    <span class="mi">33040</span><span class="p">,</span>
    <span class="mi">21695</span><span class="p">,</span>
    <span class="mi">691</span><span class="p">,</span>
    <span class="mi">6833</span><span class="p">,</span>
    <span class="mi">20734</span><span class="p">,</span>
    <span class="mi">7524</span><span class="p">,</span>
    <span class="mi">33360</span><span class="p">,</span>
    <span class="mi">30947</span><span class="p">,</span>
    <span class="mi">24622</span><span class="p">,</span>
    <span class="mi">1098</span><span class="p">,</span>
    <span class="mi">45954</span><span class="p">,</span>
    <span class="mi">53816</span><span class="p">,</span>
    <span class="mi">40749</span><span class="p">,</span>
    <span class="mi">35042</span><span class="p">,</span>
    <span class="mi">48998</span><span class="p">,</span>
    <span class="mi">22294</span><span class="p">,</span>
    <span class="mi">6757</span><span class="p">,</span>
    <span class="mi">51453</span><span class="p">,</span>
    <span class="mi">44707</span><span class="p">,</span>
    <span class="mi">7645</span><span class="p">,</span>
    <span class="mi">19343</span><span class="p">,</span>
    <span class="mi">25459</span><span class="p">,</span>
    <span class="mi">53810</span><span class="p">,</span>
    <span class="mi">8914</span><span class="p">,</span>
    <span class="mi">43911</span><span class="p">,</span>
    <span class="mi">38467</span><span class="p">,</span>
    <span class="mi">15528</span><span class="p">,</span>
    <span class="mi">11561</span><span class="p">,</span>
    <span class="mi">4901</span><span class="p">,</span>
    <span class="mi">25686</span><span class="p">,</span>
    <span class="mi">6691</span><span class="p">,</span>
    <span class="mi">2965</span><span class="p">,</span>
    <span class="mi">3567</span><span class="p">,</span>
    <span class="mi">28770</span><span class="p">,</span>
    <span class="mi">54980</span><span class="p">,</span>
    <span class="mi">2091</span><span class="p">,</span>
    <span class="mi">32231</span><span class="p">,</span>
    <span class="mi">52877</span><span class="p">,</span>
    <span class="mi">26457</span><span class="p">,</span>
    <span class="mi">53015</span><span class="p">,</span>
    <span class="mi">390</span><span class="p">,</span>
    <span class="mi">39737</span><span class="p">,</span>
    <span class="mi">32776</span><span class="p">,</span>
    <span class="mi">3057</span><span class="p">,</span>
    <span class="mi">16646</span><span class="p">,</span>
    <span class="mi">7904</span><span class="p">,</span>
    <span class="mi">20061</span><span class="p">,</span>
    <span class="mi">54900</span><span class="p">,</span>
    <span class="mi">11657</span><span class="p">,</span>
    <span class="mi">12219</span><span class="p">,</span>
    <span class="mi">44095</span><span class="p">,</span>
    <span class="mi">27270</span><span class="p">,</span>
    <span class="mi">35404</span><span class="p">,</span>
    <span class="mi">51743</span><span class="p">,</span>
    <span class="mi">10990</span><span class="p">,</span>
    <span class="mi">44581</span><span class="p">,</span>
    <span class="mi">1018</span><span class="p">,</span>
    <span class="mi">34758</span><span class="p">,</span>
    <span class="mi">31671</span><span class="p">,</span>
    <span class="mi">7397</span><span class="p">,</span>
    <span class="mi">39136</span><span class="p">,</span>
    <span class="mi">4379</span><span class="p">,</span>
    <span class="mi">48458</span><span class="p">,</span>
    <span class="mi">27978</span><span class="p">,</span>
    <span class="mi">18849</span><span class="p">,</span>
    <span class="mi">42861</span><span class="p">,</span>
    <span class="mi">51520</span><span class="p">,</span>
    <span class="mi">28510</span><span class="p">,</span>
    <span class="mi">41936</span><span class="p">,</span>
    <span class="mi">48347</span><span class="p">,</span>
    <span class="mi">52998</span><span class="p">,</span>
    <span class="mi">7063</span><span class="p">,</span>
    <span class="mi">16289</span><span class="p">,</span>
    <span class="mi">36170</span><span class="p">,</span>
    <span class="mi">42023</span><span class="p">,</span>
    <span class="mi">55730</span><span class="p">,</span>
    <span class="mi">6592</span><span class="p">,</span>
    <span class="mi">49127</span><span class="p">,</span>
    <span class="mi">2086</span><span class="p">,</span>
    <span class="mi">50683</span><span class="p">,</span>
    <span class="mi">24763</span><span class="p">,</span>
    <span class="mi">41349</span><span class="p">,</span>
    <span class="mi">30736</span><span class="p">,</span>
    <span class="mi">37284</span><span class="p">,</span>
    <span class="mi">16125</span><span class="p">,</span>
    <span class="mi">37877</span><span class="p">,</span>
    <span class="mi">2605</span><span class="p">,</span>
    <span class="mi">13340</span><span class="p">,</span>
    <span class="mi">19671</span><span class="p">,</span>
    <span class="mi">36722</span><span class="p">,</span>
    <span class="mi">43153</span><span class="p">,</span>
    <span class="mi">49403</span><span class="p">,</span>
    <span class="mi">20401</span><span class="p">,</span>
    <span class="mi">21660</span><span class="p">,</span>
    <span class="mi">22860</span><span class="p">,</span>
    <span class="mi">34520</span><span class="p">,</span>
    <span class="mi">50196</span><span class="p">,</span>
    <span class="mi">26829</span><span class="p">,</span>
    <span class="mi">29844</span><span class="p">,</span>
    <span class="mi">37518</span><span class="p">,</span>
    <span class="mi">21925</span><span class="p">,</span>
    <span class="mi">42679</span><span class="p">,</span>
    <span class="mi">35731</span><span class="p">,</span>
    <span class="mi">5394</span><span class="p">,</span>
    <span class="mi">30989</span><span class="p">,</span>
    <span class="mi">43810</span><span class="p">,</span>
    <span class="mi">48016</span><span class="p">,</span>
    <span class="mi">25245</span><span class="p">,</span>
    <span class="mi">26659</span><span class="p">,</span>
    <span class="mi">44900</span><span class="p">,</span>
    <span class="mi">21821</span><span class="p">,</span>
    <span class="mi">18035</span><span class="p">,</span>
    <span class="mi">53636</span><span class="p">,</span>
    <span class="mi">35916</span><span class="p">,</span>
    <span class="mi">42335</span><span class="p">,</span>
    <span class="mi">38260</span><span class="p">,</span>
    <span class="mi">5323</span><span class="p">,</span>
    <span class="mi">23279</span><span class="p">,</span>
    <span class="mi">48463</span><span class="p">,</span>
    <span class="mi">47556</span><span class="p">,</span>
    <span class="mi">40304</span><span class="p">,</span>
    <span class="mi">31793</span><span class="p">,</span>
    <span class="mi">22162</span><span class="p">,</span>
    <span class="mi">6487</span><span class="p">,</span>
    <span class="mi">15077</span><span class="p">,</span>
    <span class="mi">50</span><span class="p">,</span>
    <span class="mi">28001</span><span class="p">,</span>
    <span class="mi">20066</span><span class="p">,</span>
    <span class="mi">18550</span><span class="p">,</span>
    <span class="mi">53152</span><span class="p">,</span>
    <span class="mi">5656</span><span class="p">,</span>
    <span class="mi">4311</span><span class="p">,</span>
    <span class="mi">21942</span><span class="p">,</span>
    <span class="mi">34014</span><span class="p">,</span>
    <span class="mi">6800</span><span class="p">,</span>
    <span class="mi">1255</span><span class="p">,</span>
    <span class="mi">17524</span><span class="p">,</span>
    <span class="mi">54817</span><span class="p">,</span>
    <span class="mi">43244</span><span class="p">,</span>
    <span class="mi">105</span><span class="p">,</span>
    <span class="mi">19567</span><span class="p">,</span>
    <span class="mi">34998</span><span class="p">,</span>
    <span class="mi">45053</span><span class="p">,</span>
    <span class="mi">37468</span><span class="p">,</span>
    <span class="mi">33686</span><span class="p">,</span>
    <span class="mi">24406</span><span class="p">,</span>
    <span class="mi">41560</span><span class="p">,</span>
    <span class="mi">37711</span><span class="p">,</span>
    <span class="mi">13444</span><span class="p">,</span>
    <span class="mi">51128</span><span class="p">,</span>
    <span class="mi">45892</span><span class="p">,</span>
    <span class="mi">41896</span><span class="p">,</span>
    <span class="mi">15933</span><span class="p">,</span>
    <span class="mi">43579</span><span class="p">,</span>
    <span class="mi">37268</span><span class="p">,</span>
    <span class="mi">20537</span><span class="p">,</span>
    <span class="mi">19775</span><span class="p">,</span>
    <span class="mi">27477</span><span class="p">,</span>
    <span class="mi">51285</span><span class="p">,</span>
    <span class="mi">34783</span><span class="p">,</span>
    <span class="mi">33337</span><span class="p">,</span>
    <span class="mi">50425</span><span class="p">,</span>
    <span class="mi">39567</span><span class="p">,</span>
    <span class="mi">44169</span><span class="p">,</span>
    <span class="mi">6502</span><span class="p">,</span>
    <span class="mi">20178</span><span class="p">,</span>
    <span class="mi">54905</span><span class="p">,</span>
    <span class="mi">22487</span><span class="p">,</span>
    <span class="mi">47358</span><span class="p">,</span>
    <span class="mi">24596</span><span class="p">,</span>
    <span class="mi">1251</span><span class="p">,</span>
    <span class="mi">30297</span><span class="p">,</span>
    <span class="mi">39042</span><span class="p">,</span>
    <span class="mi">38405</span><span class="p">,</span>
    <span class="mi">20730</span><span class="p">,</span>
    <span class="mi">13097</span><span class="p">,</span>
    <span class="mi">39117</span><span class="p">,</span>
    <span class="mi">5075</span><span class="p">,</span>
    <span class="mi">52625</span><span class="p">,</span>
    <span class="mi">27138</span><span class="p">,</span>
    <span class="mi">11285</span><span class="p">,</span>
    <span class="mi">42406</span><span class="p">,</span>
    <span class="mi">54700</span><span class="p">,</span>
    <span class="mi">50777</span><span class="p">,</span>
    <span class="mi">17841</span><span class="p">,</span>
    <span class="mi">30157</span><span class="p">,</span>
    <span class="mi">53900</span><span class="p">,</span>
    <span class="mi">4571</span><span class="p">,</span>
    <span class="mi">23534</span><span class="p">,</span>
    <span class="mi">13887</span><span class="p">,</span>
    <span class="mi">5907</span><span class="p">,</span>
    <span class="mi">1808</span><span class="p">,</span>
    <span class="mi">12777</span><span class="p">,</span>
    <span class="mi">35830</span><span class="p">,</span>
    <span class="mi">48662</span><span class="p">,</span>
    <span class="mi">8816</span><span class="p">,</span>
    <span class="mi">9287</span><span class="p">,</span>
    <span class="mi">14403</span><span class="p">,</span>
    <span class="mi">26458</span><span class="p">,</span>
    <span class="mi">14149</span><span class="p">,</span>
    <span class="mi">14916</span><span class="p">,</span>
    <span class="mi">52783</span><span class="p">,</span>
    <span class="mi">7213</span><span class="p">,</span>
    <span class="mi">6759</span><span class="p">,</span>
    <span class="mi">39059</span><span class="p">,</span>
    <span class="mi">20086</span><span class="p">,</span>
    <span class="mi">14781</span><span class="p">,</span>
    <span class="mi">2760</span><span class="p">,</span>
    <span class="mi">37460</span><span class="p">,</span>
    <span class="mi">48848</span><span class="p">,</span>
    <span class="mi">8605</span><span class="p">,</span>
    <span class="mi">10291</span><span class="p">,</span>
    <span class="mi">35034</span><span class="p">,</span>
    <span class="mi">30994</span><span class="p">,</span>
    <span class="mi">47933</span><span class="p">,</span>
    <span class="mi">16090</span><span class="p">,</span>
    <span class="mi">39436</span><span class="p">,</span>
    <span class="mi">35192</span><span class="p">,</span>
    <span class="mi">46194</span><span class="p">,</span>
    <span class="mi">28586</span><span class="p">,</span>
    <span class="mi">10641</span><span class="p">,</span>
    <span class="mi">34097</span><span class="p">,</span>
    <span class="mi">11253</span><span class="p">,</span>
    <span class="mi">3754</span><span class="p">,</span>
    <span class="mi">44070</span><span class="p">,</span>
    <span class="mi">2082</span><span class="p">,</span>
    <span class="mi">20441</span><span class="p">,</span>
    <span class="mi">53264</span><span class="p">,</span>
    <span class="mi">5722</span><span class="p">,</span>
    <span class="mi">44452</span><span class="p">,</span>
    <span class="mi">40529</span><span class="p">,</span>
    <span class="mi">45886</span><span class="p">,</span>
    <span class="mi">15417</span><span class="p">,</span>
    <span class="mi">46270</span><span class="p">,</span>
    <span class="mi">31418</span><span class="p">,</span>
    <span class="mi">25854</span><span class="p">,</span>
    <span class="mi">3942</span><span class="p">,</span>
    <span class="mi">12225</span><span class="p">,</span>
    <span class="mi">22024</span><span class="p">,</span>
    <span class="mi">5246</span><span class="p">,</span>
    <span class="mi">15846</span><span class="p">,</span>
    <span class="mi">54435</span><span class="p">,</span>
    <span class="mi">6551</span><span class="p">,</span>
    <span class="mi">9559</span><span class="p">,</span>
    <span class="mi">3301</span><span class="p">,</span>
    <span class="mi">17364</span><span class="p">,</span>
    <span class="mi">29613</span><span class="p">,</span>
    <span class="mi">43959</span><span class="p">,</span>
    <span class="mi">51047</span><span class="p">,</span>
    <span class="mi">39633</span><span class="p">,</span>
    <span class="mi">27723</span><span class="p">,</span>
    <span class="mi">28189</span><span class="p">,</span>
    <span class="mi">1271</span><span class="p">,</span>
    <span class="mi">534</span><span class="p">,</span>
    <span class="mi">38503</span><span class="p">,</span>
    <span class="mi">47661</span><span class="p">,</span>
    <span class="mi">28499</span><span class="p">,</span>
    <span class="mi">9737</span><span class="p">,</span>
    <span class="mi">45190</span><span class="p">,</span>
    <span class="mi">11495</span><span class="p">,</span>
    <span class="mi">23346</span><span class="p">,</span>
    <span class="mi">21045</span><span class="p">,</span>
    <span class="mi">25509</span><span class="p">,</span>
    <span class="mi">49692</span><span class="p">,</span>
    <span class="mi">36635</span><span class="p">,</span>
    <span class="mi">2652</span><span class="p">,</span>
    <span class="mi">40844</span><span class="p">,</span>
    <span class="mi">7489</span><span class="p">,</span>
    <span class="mi">37917</span><span class="p">,</span>
    <span class="mi">36464</span><span class="p">,</span>
    <span class="mi">4995</span><span class="p">,</span>
    <span class="mi">54231</span><span class="p">,</span>
    <span class="mi">48017</span><span class="p">,</span>
    <span class="mi">6803</span><span class="p">,</span>
    <span class="mi">24767</span><span class="p">,</span>
    <span class="mi">10898</span><span class="p">,</span>
    <span class="mi">13739</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ActiveLearner" class="doc_header"><code>class</code> <code>ActiveLearner</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/predetermined_acquisitions_120.py#L456" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ActiveLearner</code>(<strong><code>acquisition_size</code></strong>:<code>int</code>, <strong><code>num_validation_samples</code></strong>:<code>int</code>, <strong><code>num_pool_samples</code></strong>:<code>int</code>, <strong><code>train_eval_model</code></strong>:<code>TrainEvalModel</code>, <strong><code>model_trainer</code></strong>:<code>ModelTrainer</code>, <strong><code>data</code></strong>:<a href="/batchbald_redux/experiment_cifar10_xmi_labels_clean.html#ExperimentData"><code>ExperimentData</code></a>, <strong><code>disable_training_augmentations</code></strong>:<code>bool</code>, <strong><code>device</code></strong>:<code>Optional</code>)</p>
</blockquote>
<p>ActiveLearner(acquisition_size: int, num_validation_samples: int, num_pool_samples: int, train_eval_model: batchbald_redux.train_eval_model.TrainEvalModel, model_trainer: batchbald_redux.trained_model.ModelTrainer, data: batchbald_redux.experiment_data.ExperimentData, disable_training_augmentations: bool, device: Optional)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="UnifiedExperiment" class="doc_header"><code>class</code> <code>UnifiedExperiment</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/predetermined_acquisitions_120.py#L539" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>UnifiedExperiment</code>(<strong><code>seed</code></strong>:<code>int</code>, <strong><code>experiment_data_config</code></strong>:<a href="/batchbald_redux/experiment_cifar10_xmi_labels_clean.html#ExperimentDataConfig"><code>ExperimentDataConfig</code></a>, <strong><code>acquisition_size</code></strong>:<code>int</code>=<em><code>5</code></em>, <strong><code>max_training_epochs</code></strong>:<code>int</code>=<em><code>300</code></em>, <strong><code>num_pool_samples</code></strong>:<code>int</code>=<em><code>100</code></em>, <strong><code>num_validation_samples</code></strong>:<code>int</code>=<em><code>20</code></em>, <strong><code>num_training_samples</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>device</code></strong>:<code>str</code>=<em><code>'cuda'</code></em>, <strong><code>acquisition_function</code></strong>:<code>Union</code>[<code>Type</code>[<code>CandidateBatchComputer</code>], <code>Type</code>[<code>EvalModelBatchComputer</code>]]=<em><code>BALD</code></em>, <strong><code>train_eval_model</code></strong>:<code>Type</code>[<code>TrainEvalModel</code>]=<em><code>TrainSelfDistillationEvalModel</code></em>, <strong><code>model_trainer_factory</code></strong>:<code>Type</code>[<code>ModelTrainer</code>]=<em><code>Cifar10ModelTrainer</code></em>, <strong><code>ensemble_size</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>temperature</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>epig_bootstrap_type</code></strong>:<code>BootstrapType</code>=<em><code>&lt;BootstrapType.NO_BOOTSTRAP: 0&gt;</code></em>, <strong><code>epig_bootstrap_factor</code></strong>:<code>float</code>=<em><code>1.0</code></em>, <strong><code>epig_dtype</code></strong>:<code>dtype</code>=<em><code>torch.float64</code></em>, <strong><code>disable_training_augmentations</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>cache_explicit_eval_model</code></strong>:<code>bool</code>=<em><code>False</code></em>)</p>
</blockquote>
<p>UnifiedExperiment(seed: int, experiment_data_config: batchbald_redux.experiment_data.ExperimentDataConfig, acquisition_size: int = 5, max_training_epochs: int = 300, num_pool_samples: int = 100, num_validation_samples: int = 20, num_training_samples: int = 1, device: str = 'cuda', acquisition_function: Union[Type[batchbald_redux.acquisition_functions.candidate_batch_computers.CandidateBatchComputer], Type[batchbald_redux.acquisition_functions.candidate_batch_computers.EvalModelBatchComputer]] = &lt;class 'batchbald_redux.acquisition_functions.bald.BALD'&gt;, train_eval_model: Type[batchbald_redux.train_eval_model.TrainEvalModel] = &lt;class 'batchbald_redux.train_eval_model.TrainSelfDistillationEvalModel'&gt;, model_trainer_factory: Type[batchbald_redux.trained_model.ModelTrainer] = &lt;class 'batchbald_redux.resnet_models.Cifar10ModelTrainer'&gt;, ensemble_size: int = 1, temperature: float = 0.0, epig_bootstrap_type: batchbald_redux.acquisition_functions.epig.BootstrapType = &lt;BootstrapType.NO_BOOTSTRAP: 0&gt;, epig_bootstrap_factor: float = 1.0, epig_dtype: torch.dtype = torch.float64, disable_training_augmentations: bool = False, cache_explicit_eval_model: bool = False)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ActiveLearner</span><span class="p">:</span>
    <span class="n">acquisition_size</span><span class="p">:</span> <span class="nb">int</span>

    <span class="n">num_validation_samples</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">num_pool_samples</span><span class="p">:</span> <span class="nb">int</span>

    <span class="n">train_eval_model</span><span class="p">:</span> <span class="n">TrainEvalModel</span>
    <span class="n">model_trainer</span><span class="p">:</span> <span class="n">ModelTrainer</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">ExperimentData</span>

    <span class="n">disable_training_augmentations</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log</span><span class="p">):</span>
        <span class="n">log</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span>

        <span class="c1"># Active Learning setup</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>

        <span class="n">train_augmentations</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_augmentations</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">disable_training_augmentations</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="n">model_trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_trainer</span>
        <span class="n">train_eval_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_eval_model</span>

        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">model_trainer</span><span class="o">.</span><span class="n">get_train_dataloader</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">training_dataset</span><span class="p">)</span>
        <span class="n">pool_loader</span> <span class="o">=</span> <span class="n">model_trainer</span><span class="o">.</span><span class="n">get_evaluation_dataloader</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">pool_dataset</span><span class="p">)</span>
        <span class="n">validation_loader</span> <span class="o">=</span> <span class="n">model_trainer</span><span class="o">.</span><span class="n">get_evaluation_dataloader</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">validation_dataset</span><span class="p">)</span>
        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">model_trainer</span><span class="o">.</span><span class="n">get_evaluation_dataloader</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">test_dataset</span><span class="p">)</span>

        <span class="n">log</span><span class="p">[</span><span class="s2">&quot;active_learning_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">active_learning_steps</span> <span class="o">=</span> <span class="n">log</span><span class="p">[</span><span class="s2">&quot;active_learning_steps&quot;</span><span class="p">]</span>

        <span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">acquire_base_indices</span><span class="p">(</span><span class="n">additional_initial_acquisitions</span><span class="p">)</span>

        <span class="c1"># Active Training Loop</span>
        <span class="k">for</span> <span class="n">base_index</span> <span class="ow">in</span> <span class="n">predetermind_acquisition_base_indices</span><span class="p">:</span>
            <span class="n">training_set_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">training_dataset</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training set size </span><span class="si">{</span><span class="n">training_set_size</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>

            <span class="c1"># iteration_log = dict(training={}, pool_training={}, evaluation_metrics=None, acquisition=None)</span>
            <span class="n">active_learning_steps</span><span class="o">.</span><span class="n">append</span><span class="p">({})</span>
            <span class="n">iteration_log</span> <span class="o">=</span> <span class="n">active_learning_steps</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">iteration_log</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="c1"># TODO: this is a hack! :(</span>
            <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ood_dataset</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">validation_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">data</span><span class="o">.</span><span class="n">ood_exposure</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">(</span><span class="n">log_target</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;batchmean&quot;</span><span class="p">)</span>
                <span class="n">validation_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">validation_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

            <span class="n">trained_model</span> <span class="o">=</span> <span class="n">model_trainer</span><span class="o">.</span><span class="n">get_trained</span><span class="p">(</span>
                <span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span>
                <span class="n">train_augmentations</span><span class="o">=</span><span class="n">train_augmentations</span><span class="p">,</span>
                <span class="n">validation_loader</span><span class="o">=</span><span class="n">validation_loader</span><span class="p">,</span>
                <span class="n">log</span><span class="o">=</span><span class="n">iteration_log</span><span class="p">[</span><span class="s2">&quot;training&quot;</span><span class="p">],</span>
                <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
                <span class="n">validation_loss</span><span class="o">=</span><span class="n">validation_loss</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">evaluation_metrics</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">trained_model</span><span class="p">,</span>
                <span class="n">num_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_validation_samples</span><span class="p">,</span>
                <span class="n">loader</span><span class="o">=</span><span class="n">test_loader</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">storage_device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">iteration_log</span><span class="p">[</span><span class="s2">&quot;evaluation_metrics&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">evaluation_metrics</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Perf after training </span><span class="si">{</span><span class="n">evaluation_metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="n">iteration_log</span><span class="p">[</span><span class="s2">&quot;acquisition&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="p">[</span><span class="n">base_index</span><span class="p">])</span>
            <span class="n">acquired_label</span> <span class="o">=</span> <span class="n">get_target</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">base_dataset</span><span class="p">,</span> <span class="n">base_index</span><span class="p">)</span>

            <span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">acquire_base_indices</span><span class="p">([</span><span class="n">base_index</span><span class="p">])</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Acquiring base index </span><span class="si">{</span><span class="n">base_index</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">acquired_label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">UnifiedExperiment</span><span class="p">:</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span>

    <span class="n">experiment_data_config</span><span class="p">:</span> <span class="n">ExperimentDataConfig</span>

    <span class="n">acquisition_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span>

    <span class="n">max_training_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">300</span>

    <span class="n">num_pool_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">num_validation_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="n">num_training_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
    <span class="n">acquisition_function</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">CandidateBatchComputer</span><span class="p">],</span> <span class="n">Type</span><span class="p">[</span><span class="n">EvalModelBatchComputer</span><span class="p">]]</span> <span class="o">=</span> <span class="n">batchbald_redux</span>\
        <span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">bald</span><span class="o">.</span><span class="n">BALD</span>
    <span class="n">train_eval_model</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">TrainEvalModel</span><span class="p">]</span> <span class="o">=</span> <span class="n">TrainSelfDistillationEvalModel</span>
    <span class="n">model_trainer_factory</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">ModelTrainer</span><span class="p">]</span> <span class="o">=</span> <span class="n">Cifar10ModelTrainer</span>
    <span class="n">ensemble_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">epig_bootstrap_type</span><span class="p">:</span> <span class="n">acquisition_functions</span><span class="o">.</span><span class="n">BootstrapType</span> <span class="o">=</span> <span class="n">acquisition_functions</span><span class="o">.</span><span class="n">BootstrapType</span><span class="o">.</span><span class="n">NO_BOOTSTRAP</span>
    <span class="n">epig_bootstrap_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">epig_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">double</span>
    <span class="n">disable_training_augmentations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">cache_explicit_eval_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">load_experiment_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ExperimentData</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">experiment_data_config</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">experiment_data_config</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Simple Dependency Injection</span>
    <span class="k">def</span> <span class="nf">create_train_eval_model</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TrainEvalModel</span><span class="p">:</span>
        <span class="n">di</span> <span class="o">=</span> <span class="n">DependencyInjection</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">di</span><span class="o">.</span><span class="n">create_dataclass_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_eval_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">create_model_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelTrainer</span><span class="p">:</span>
        <span class="n">di</span> <span class="o">=</span> <span class="n">DependencyInjection</span><span class="p">(</span><span class="nb">vars</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">di</span><span class="o">.</span><span class="n">create_dataclass_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_trainer_factory</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">store</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Active Learning setup</span>
        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">load_experiment_data</span><span class="p">()</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;dataset_info&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="nb">repr</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">active_learning</span><span class="o">.</span><span class="n">base_dataset</span><span class="p">),</span> <span class="n">test</span><span class="o">=</span><span class="nb">repr</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">test_dataset</span><span class="p">))</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;initial_training_set_indices&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">initial_training_set_indices</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;evaluation_set_indices&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">evaluation_set_indices</span>

        <span class="n">model_trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_model_trainer</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ensemble_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">model_trainer</span> <span class="o">=</span> <span class="n">BayesianEnsembleModelTrainer</span><span class="p">(</span><span class="n">model_trainer</span><span class="o">=</span><span class="n">model_trainer</span><span class="p">,</span> <span class="n">ensemble_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ensemble_size</span><span class="p">)</span>
        <span class="n">train_eval_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_train_eval_model</span><span class="p">()</span>

        <span class="n">active_learner</span> <span class="o">=</span> <span class="n">ActiveLearner</span><span class="p">(</span>
            <span class="n">acquisition_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">acquisition_size</span><span class="p">,</span>
            <span class="n">num_validation_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_validation_samples</span><span class="p">,</span>
            <span class="n">num_pool_samples</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_pool_samples</span><span class="p">,</span>
            <span class="n">disable_training_augmentations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">disable_training_augmentations</span><span class="p">,</span>
            <span class="n">train_eval_model</span><span class="o">=</span><span class="n">train_eval_model</span><span class="p">,</span>
            <span class="n">model_trainer</span><span class="o">=</span><span class="n">model_trainer</span><span class="p">,</span>
            <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">active_learner</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MNIST-only">MNIST only<a class="anchor-link" href="#MNIST-only"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># MNIST experiment (ood_exposure=False)</span>

<span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">UnifiedExperiment</span><span class="p">(</span>
        <span class="n">experiment_data_config</span><span class="o">=</span><span class="n">StandardExperimentDataConfig</span><span class="p">(</span>
            <span class="n">id_dataset_name</span><span class="o">=</span><span class="s2">&quot;MNIST&quot;</span><span class="p">,</span>
            <span class="n">id_repetitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">initial_training_set_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">validation_set_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
            <span class="n">validation_split_random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">evaluation_set_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">add_dataset_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">ood_dataset_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">trial</span><span class="p">,</span>
        <span class="n">max_training_epochs</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
        <span class="n">model_trainer_factory</span><span class="o">=</span><span class="n">MnistModelTrainer</span><span class="p">,</span>
        <span class="n">num_pool_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">ensemble_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="p">]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">is_run_from_ipython</span><span class="p">()</span> <span class="ow">and</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">job_id</span><span class="p">,</span> <span class="n">store</span> <span class="ow">in</span> <span class="n">embedded_experiments</span><span class="p">(</span><span class="vm">__file__</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">configs</span><span class="p">)):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="n">job_id</span><span class="p">]</span>
        <span class="n">config</span><span class="o">.</span><span class="n">seed</span> <span class="o">+=</span> <span class="n">job_id</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">asdict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">store</span><span class="p">[</span><span class="s2">&quot;exception&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_exc</span><span class="p">()</span>
            <span class="k">raise</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">experiment</span><span class="o">.</span><span class="n">max_training_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">experiment</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">store</span><span class="o">=</span><span class="n">results</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>StandardExperimentDataConfig(id_dataset_name=&#39;MNIST&#39;, id_repetitions=1, initial_training_set_size=20, validation_set_size=4096, validation_split_random_state=0, evaluation_set_size=0, add_dataset_noise=False, ood_dataset_config=None)
Creating: MnistModelTrainer(
	device=cuda,
	num_training_samples=1,
	num_validation_samples=20,
	max_training_epochs=1
)
Creating: TrainSelfDistillationEvalModel(
	num_pool_samples=100
)
Training set size 120:
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return floored.astype(np.int)
/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return floored.astype(np.int)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch metrics: {&#39;accuracy&#39;: 0.216796875, &#39;crossentropy&#39;: 2.2411887645721436}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.216796875)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.1416015625, &#39;crossentropy&#39;: 2.245363473892212}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.1416015625)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.2294, &#39;crossentropy&#39;: tensor(2.2386)}
Acquiring base index 39492 7
Training set size 121:
Epoch metrics: {&#39;accuracy&#39;: 0.162353515625, &#39;crossentropy&#39;: 2.230421304702759}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.162353515625)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.185791015625, &#39;crossentropy&#39;: 2.2466177940368652}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.185791015625)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.2193, &#39;crossentropy&#39;: tensor(2.2274)}
Acquiring base index 22860 3
Training set size 122:
Epoch metrics: {&#39;accuracy&#39;: 0.166748046875, &#39;crossentropy&#39;: 2.2508232593536377}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.166748046875)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.141357421875, &#39;crossentropy&#39;: 2.239248752593994}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.141357421875)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.178, &#39;crossentropy&#39;: tensor(2.2445)}
Acquiring base index 8309 1
Training set size 123:
Epoch metrics: {&#39;accuracy&#39;: 0.124755859375, &#39;crossentropy&#39;: 2.2086641788482666}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.124755859375)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.17529296875, &#39;crossentropy&#39;: 2.226715564727783}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.17529296875)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.1815, &#39;crossentropy&#39;: tensor(2.2109)}
Acquiring base index 1584 8
Training set size 124:
Epoch metrics: {&#39;accuracy&#39;: 0.138671875, &#39;crossentropy&#39;: 2.249375820159912}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.138671875)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.10205078125, &#39;crossentropy&#39;: 2.257321834564209}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.10205078125)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.1113, &#39;crossentropy&#39;: tensor(2.2510)}
Acquiring base index 30947 5
Training set size 125:
Epoch metrics: {&#39;accuracy&#39;: 0.099853515625, &#39;crossentropy&#39;: 2.2375547885894775}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.099853515625)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.202392578125, &#39;crossentropy&#39;: 2.2418019771575928}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.202392578125)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.1032, &#39;crossentropy&#39;: tensor(2.2402)}
Acquiring base index 27798 7
Training set size 126:
Epoch metrics: {&#39;accuracy&#39;: 0.11279296875, &#39;crossentropy&#39;: 2.2528951168060303}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.11279296875)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.216552734375, &#39;crossentropy&#39;: 2.2426328659057617}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.216552734375)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.1234, &#39;crossentropy&#39;: tensor(2.2524)}
Acquiring base index 49452 8
Training set size 127:
Epoch metrics: {&#39;accuracy&#39;: 0.211181640625, &#39;crossentropy&#39;: 2.2270491123199463}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.211181640625)
RestoringEarlyStopping: Restoring optimizer.
Epoch metrics: {&#39;accuracy&#39;: 0.157958984375, &#39;crossentropy&#39;: 2.2429473400115967}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.157958984375)
RestoringEarlyStopping: Restoring optimizer.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-13-150d5a6183ad&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> results <span class="ansi-blue-fg">=</span> <span class="ansi-blue-fg">{</span><span class="ansi-blue-fg">}</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> experiment<span class="ansi-blue-fg">.</span>max_training_epochs <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">1</span>
<span class="ansi-green-fg">----&gt; 3</span><span class="ansi-red-fg"> </span>experiment<span class="ansi-blue-fg">.</span>run<span class="ansi-blue-fg">(</span>store<span class="ansi-blue-fg">=</span>results<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-11-e784b37b2212&gt;</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-fg">(self, store)</span>
<span class="ansi-green-intense-fg ansi-bold">    150</span>         )
<span class="ansi-green-intense-fg ansi-bold">    151</span> 
<span class="ansi-green-fg">--&gt; 152</span><span class="ansi-red-fg">         </span>active_learner<span class="ansi-blue-fg">(</span>store<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">&lt;ipython-input-11-e784b37b2212&gt;</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, log)</span>
<span class="ansi-green-intense-fg ansi-bold">     67</span>             )
<span class="ansi-green-intense-fg ansi-bold">     68</span> 
<span class="ansi-green-fg">---&gt; 69</span><span class="ansi-red-fg">             evaluation_metrics = evaluate(
</span><span class="ansi-green-intense-fg ansi-bold">     70</span>                 model<span class="ansi-blue-fg">=</span>trained_model<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     71</span>                 num_samples<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>num_validation_samples<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/black_box_model_training.py</span> in <span class="ansi-cyan-fg">evaluate</span><span class="ansi-blue-fg">(model, loader, num_samples, device, storage_device, loss)</span>
<span class="ansi-green-intense-fg ansi-bold">    517</span> 
<span class="ansi-green-intense-fg ansi-bold">    518</span> <span class="ansi-green-fg">def</span> evaluate<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span> model<span class="ansi-blue-fg">:</span> TrainedModel<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">,</span> loss<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">None</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 519</span><span class="ansi-red-fg">     log_probs_N_K_C, labels_N = model.get_log_probs_N_K_C_labels_N(
</span><span class="ansi-green-intense-fg ansi-bold">    520</span>         loader<span class="ansi-blue-fg">=</span>loader<span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>device<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">=</span>storage_device
<span class="ansi-green-intense-fg ansi-bold">    521</span>     )

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/trained_model.py</span> in <span class="ansi-cyan-fg">get_log_probs_N_K_C_labels_N</span><span class="ansi-blue-fg">(self, loader, num_samples, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    110</span> 
<span class="ansi-green-intense-fg ansi-bold">    111</span>         <span class="ansi-green-fg">for</span> model <span class="ansi-green-fg">in</span> self<span class="ansi-blue-fg">.</span>models<span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 112</span><span class="ansi-red-fg">             log_probs_N_K_C, labels_B = model.get_log_probs_N_K_C_labels_N(
</span><span class="ansi-green-intense-fg ansi-bold">    113</span>                 loader<span class="ansi-blue-fg">=</span>loader<span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">=</span>member_num_samples<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>device<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">=</span>storage_device
<span class="ansi-green-intense-fg ansi-bold">    114</span>             )

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/trained_model.py</span> in <span class="ansi-cyan-fg">get_log_probs_N_K_C_labels_N</span><span class="ansi-blue-fg">(self, loader, num_samples, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">     50</span>         self<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">:</span> DataLoader<span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">:</span> int<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">:</span> object<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">:</span> object
<span class="ansi-green-intense-fg ansi-bold">     51</span>     ):
<span class="ansi-green-fg">---&gt; 52</span><span class="ansi-red-fg">         log_probs_N_K_C, labels_B = self.model.get_predictions_labels(
</span><span class="ansi-green-intense-fg ansi-bold">     53</span>             num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">=</span>loader<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>device<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">=</span>storage_device
<span class="ansi-green-intense-fg ansi-bold">     54</span>         )

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">get_predictions_labels</span><span class="ansi-blue-fg">(self, num_samples, loader, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    147</span>         self<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">:</span> int<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">:</span> data<span class="ansi-blue-fg">.</span>DataLoader<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">,</span> storage_device
<span class="ansi-green-intense-fg ansi-bold">    148</span>     ):
<span class="ansi-green-fg">--&gt; 149</span><span class="ansi-red-fg">         return bmodule_get_predictions_labels(
</span><span class="ansi-green-intense-fg ansi-bold">    150</span>             self<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    151</span>             num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/autograd/grad_mode.py</span> in <span class="ansi-cyan-fg">decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">     26</span>         <span class="ansi-green-fg">def</span> decorate_context<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>             <span class="ansi-green-fg">with</span> self<span class="ansi-blue-fg">.</span>__class__<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 28</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> func<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     29</span>         <span class="ansi-green-fg">return</span> cast<span class="ansi-blue-fg">(</span>F<span class="ansi-blue-fg">,</span> decorate_context<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     30</span> 

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">bmodule_get_predictions_labels</span><span class="ansi-blue-fg">(self, num_samples, loader, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    391</span> 
<span class="ansi-green-intense-fg ansi-bold">    392</span>     <span class="ansi-blue-fg">@</span>toma<span class="ansi-blue-fg">.</span>execute<span class="ansi-blue-fg">.</span>range<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">128</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 393</span><span class="ansi-red-fg">     </span><span class="ansi-green-fg">def</span> get_prediction_batch<span class="ansi-blue-fg">(</span>start<span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    394</span>         <span class="ansi-green-fg">nonlocal</span> predictions
<span class="ansi-green-intense-fg ansi-bold">    395</span>         <span class="ansi-green-fg">nonlocal</span> labels

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py</span> in <span class="ansi-cyan-fg">execute_range</span><span class="ansi-blue-fg">(func)</span>
<span class="ansi-green-intense-fg ansi-bold">    184</span> 
<span class="ansi-green-intense-fg ansi-bold">    185</span>             <span class="ansi-green-fg">def</span> execute_range<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 186</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> explicit<span class="ansi-blue-fg">.</span>range<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">,</span> start<span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">,</span> initial_step<span class="ansi-blue-fg">,</span> toma_cache_type<span class="ansi-blue-fg">=</span>cache_type<span class="ansi-blue-fg">,</span> toma_context<span class="ansi-blue-fg">=</span>context<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    187</span> 
<span class="ansi-green-intense-fg ansi-bold">    188</span>             <span class="ansi-green-fg">return</span> execute_range

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py</span> in <span class="ansi-cyan-fg">range</span><span class="ansi-blue-fg">(func, start, end, initial_step, toma_context, toma_cache_type, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    267</span>         <span class="ansi-green-fg">while</span> current <span class="ansi-blue-fg">&lt;</span> end<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    268</span>             <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 269</span><span class="ansi-red-fg">                 </span>func<span class="ansi-blue-fg">(</span>current<span class="ansi-blue-fg">,</span> min<span class="ansi-blue-fg">(</span>current <span class="ansi-blue-fg">+</span> batchsize<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    270</span>                 current <span class="ansi-blue-fg">+=</span> batchsize<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    271</span>                 gc_cuda<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">get_prediction_batch</span><span class="ansi-blue-fg">(start, end)</span>
<span class="ansi-green-intense-fg ansi-bold">    423</span>             predictions<span class="ansi-blue-fg">[</span>data_start<span class="ansi-blue-fg">:</span>data_end<span class="ansi-blue-fg">,</span> start<span class="ansi-blue-fg">:</span>end<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>copy_<span class="ansi-blue-fg">(</span>batch_predictions<span class="ansi-blue-fg">,</span> non_blocking<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    424</span>             <span class="ansi-green-fg">if</span> start <span class="ansi-blue-fg">==</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 425</span><span class="ansi-red-fg">                 </span>labels<span class="ansi-blue-fg">[</span>data_start<span class="ansi-blue-fg">:</span>data_end<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>copy_<span class="ansi-blue-fg">(</span>batch_labels<span class="ansi-blue-fg">,</span> non_blocking<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    426</span> 
<span class="ansi-green-intense-fg ansi-bold">    427</span>             data_start <span class="ansi-blue-fg">=</span> data_end

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

