---

title: Experiment MNIST: (real EPIG) vs EvalBALD vs BALD


keywords: fastai
sidebar: home_sidebar

summary: "Can we get better by training on our assumptions?"
description: "Can we get better by training on our assumptions?"
nb_path: "09b_experiment_mnist_epig_vs_eval_vs_bald.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 09b_experiment_mnist_epig_vs_eval_vs_bald.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Appended /home/blackhc/PycharmProjects/bald-ical/src to paths
Switched to directory /home/blackhc/PycharmProjects/bald-ical
%load_ext autoreload
%autoreload 2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Import modules and functions were are going to use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="kn">from</span> <span class="nn">blackhc.project</span> <span class="kn">import</span> <span class="n">is_run_from_ipython</span>
<span class="kn">from</span> <span class="nn">blackhc.project.experiment</span> <span class="kn">import</span> <span class="n">embedded_experiments</span>

<span class="kn">import</span> <span class="nn">batchbald_redux.acquisition_functions.bald</span>
<span class="kn">import</span> <span class="nn">batchbald_redux.acquisition_functions.epig</span>
<span class="kn">from</span> <span class="nn">batchbald_redux</span> <span class="kn">import</span> <span class="n">acquisition_functions</span>
<span class="kn">from</span> <span class="nn">batchbald_redux</span> <span class="kn">import</span> <span class="n">baseline_acquisition_functions</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.unified_experiment</span> <span class="kn">import</span> <span class="n">UnifiedExperiment</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.experiment_data</span> <span class="kn">import</span> <span class="n">StandardExperimentDataConfig</span>

<span class="kn">from</span> <span class="nn">batchbald_redux.models</span> <span class="kn">import</span> <span class="n">MnistModelTrainer</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">UnifiedExperiment</span><span class="p">(</span>
        <span class="n">experiment_data_config</span><span class="o">=</span><span class="n">StandardExperimentDataConfig</span><span class="p">(</span>
            <span class="n">id_dataset_name</span><span class="o">=</span><span class="s2">&quot;MNIST&quot;</span><span class="p">,</span>
            <span class="n">id_repetitions</span><span class="o">=</span><span class="n">id_repetitions</span><span class="p">,</span>
            <span class="n">initial_training_set_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">validation_set_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
            <span class="n">validation_split_random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">evaluation_set_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">add_dataset_noise</span><span class="o">=</span><span class="n">id_repetitions</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">ood_dataset_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span> <span class="o">+</span> <span class="mi">8945</span><span class="p">,</span>
        <span class="n">acquisition_function</span><span class="o">=</span><span class="n">acquisition_function</span><span class="p">,</span>
        <span class="n">acquisition_size</span><span class="o">=</span><span class="n">acquisition_size</span><span class="p">,</span>
        <span class="n">num_pool_samples</span><span class="o">=</span><span class="n">num_pool_samples</span><span class="p">,</span>
        <span class="n">max_training_set</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
        <span class="n">model_trainer_factory</span><span class="o">=</span><span class="n">MnistModelTrainer</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">acquisition_function</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">baseline_acquisition_functions</span><span class="o">.</span><span class="n">BADGE</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">acquisition_size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">num_pool_samples</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">id_repetitions</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
    <span class="n">UnifiedExperiment</span><span class="p">(</span>
        <span class="n">experiment_data_config</span><span class="o">=</span><span class="n">StandardExperimentDataConfig</span><span class="p">(</span>
            <span class="n">id_dataset_name</span><span class="o">=</span><span class="s2">&quot;MNIST&quot;</span><span class="p">,</span>
            <span class="n">id_repetitions</span><span class="o">=</span><span class="n">id_repetitions</span><span class="p">,</span>
            <span class="n">initial_training_set_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">validation_set_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>
            <span class="n">validation_split_random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">evaluation_set_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">add_dataset_noise</span><span class="o">=</span><span class="n">id_repetitions</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">ood_dataset_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span> <span class="o">+</span> <span class="mi">8945</span><span class="p">,</span>
        <span class="n">acquisition_function</span><span class="o">=</span><span class="n">acquisition_function</span><span class="p">,</span>
        <span class="n">acquisition_size</span><span class="o">=</span><span class="n">acquisition_size</span><span class="p">,</span>
        <span class="n">num_pool_samples</span><span class="o">=</span><span class="n">num_pool_samples</span><span class="p">,</span>
        <span class="n">max_training_set</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
        <span class="n">model_trainer_factory</span><span class="o">=</span><span class="n">MnistModelTrainer</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">acquisition_function</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">baseline_acquisition_functions</span><span class="o">.</span><span class="n">BADGE</span><span class="p">,</span>
        <span class="n">batchbald_redux</span><span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">epig</span><span class="o">.</span><span class="n">EPIG</span><span class="p">,</span>
        <span class="n">batchbald_redux</span><span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">epig</span><span class="o">.</span><span class="n">EvalBALD</span><span class="p">,</span>
        <span class="n">batchbald_redux</span><span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">bald</span><span class="o">.</span><span class="n">BALD</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">acquisition_size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">num_pool_samples</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">id_repetitions</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">is_run_from_ipython</span><span class="p">()</span> <span class="ow">and</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">job_id</span><span class="p">,</span> <span class="n">store</span> <span class="ow">in</span> <span class="n">embedded_experiments</span><span class="p">(</span><span class="vm">__file__</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">configs</span><span class="p">)):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="n">job_id</span><span class="p">]</span>
        <span class="n">config</span><span class="o">.</span><span class="n">seed</span> <span class="o">+=</span> <span class="n">job_id</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">asdict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">store</span><span class="p">[</span><span class="s2">&quot;exception&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_exc</span><span class="p">()</span>
            <span class="k">raise</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">configs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>20</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">prettyprinter</span>
<span class="n">prettyprinter</span><span class="o">.</span><span class="n">install_extras</span><span class="p">({</span><span class="s2">&quot;dataclasses&quot;</span><span class="p">})</span>
<span class="n">prettyprinter</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">configs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8948,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8949,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EPIG,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EPIG,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EPIG,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8948,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EPIG,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8949,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EPIG,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8948,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8949,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8948,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8949,
        experiment_data_config=batchbald_redux.experiment_data.StandardExperimentDataConfig(
            id_dataset_name=&#39;MNIST&#39;,
            id_repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4096,
            validation_split_random_state=0,
            evaluation_set_size=0,
            add_dataset_noise=False,
            ood_dataset_config=None
        ),
        acquisition_size=1,
        max_training_set=120,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    )
]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">store</span><span class="o">=</span><span class="p">{}</span>

<span class="n">configs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max_training_epochs</span><span class="o">=</span><span class="mi">1</span>
<span class="n">configs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">num_pool_samples</span><span class="o">=</span><span class="mi">5</span>
<span class="n">configs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">evaluation_set_size</span><span class="o">=</span><span class="mi">1024</span>
<span class="n">configs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Creating: BADGE(
	acquisition_size=1
)
Creating: MnistModelTrainer(
	device=cuda,
	num_training_samples=1,
	num_validation_samples=20,
	max_training_epochs=1
)
Creating: TrainSelfDistillationEvalModel(
	num_pool_samples=5
)
Training set size 20:
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return floored.astype(np.int)
/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return floored.astype(np.int)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch metrics: {&#39;accuracy&#39;: 0.5322265625, &#39;crossentropy&#39;: 1.880748711526394}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.5322265625)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5478, &#39;crossentropy&#39;: tensor(1.7148)}
#Samps	Total Distance
CandidateBatch(scores=[0.0], indices=[40429])
[(&#39;id&#39;, 40446)]
Acquiring (label, score)s: 0 (0.0)
Training set size 21:
Epoch metrics: {&#39;accuracy&#39;: 0.5615234375, &#39;crossentropy&#39;: 1.8467455208301544}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.5615234375)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.6066, &#39;crossentropy&#39;: tensor(1.5495)}
#Samps	Total Distance
CandidateBatch(scores=[0.0], indices=[116])
[(&#39;id&#39;, 116)]
Acquiring (label, score)s: 0 (0.0)
Training set size 22:
Epoch metrics: {&#39;accuracy&#39;: 0.53173828125, &#39;crossentropy&#39;: 1.842999268323183}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.53173828125)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5618, &#39;crossentropy&#39;: tensor(1.6569)}
#Samps	Total Distance
CandidateBatch(scores=[0.0], indices=[19358])
[(&#39;id&#39;, 19365)]
Acquiring (label, score)s: 0 (0.0)
Training set size 23:
Epoch metrics: {&#39;accuracy&#39;: 0.505859375, &#39;crossentropy&#39;: 1.8935364410281181}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.505859375)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.4991, &#39;crossentropy&#39;: tensor(1.6983)}
#Samps	Total Distance
CandidateBatch(scores=[0.0], indices=[35957])
[(&#39;id&#39;, 35976)]
Acquiring (label, score)s: 4 (0.0)
Training set size 24:
Epoch metrics: {&#39;accuracy&#39;: 0.558349609375, &#39;crossentropy&#39;: 1.814862221479416}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.558349609375)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5505, &#39;crossentropy&#39;: tensor(1.6885)}
#Samps	Total Distance
CandidateBatch(scores=[0.0], indices=[4586])
[(&#39;id&#39;, 4588)]
Acquiring (label, score)s: 0 (0.0)
Training set size 25:
Epoch metrics: {&#39;accuracy&#39;: 0.487060546875, &#39;crossentropy&#39;: 1.8638857491314411}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.487060546875)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5002, &#39;crossentropy&#39;: tensor(1.6770)}
#Samps	Total Distance
CandidateBatch(scores=[0.0], indices=[10113])
[(&#39;id&#39;, 10117)]
Acquiring (label, score)s: 0 (0.0)
Training set size 26:
Epoch metrics: {&#39;accuracy&#39;: 0.41162109375, &#39;crossentropy&#39;: 1.972345657646656}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.41162109375)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.3807, &#39;crossentropy&#39;: tensor(1.8047)}
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-13-53ebc3871ad4&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> configs<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>num_pool_samples<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> configs<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>evaluation_set_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1024</span>
<span class="ansi-green-fg">----&gt; 6</span><span class="ansi-red-fg"> </span>configs<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>run<span class="ansi-blue-fg">(</span>store<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/unified_experiment.py</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-fg">(self, store)</span>
<span class="ansi-green-intense-fg ansi-bold">    246</span>                                        model_trainer=model_trainer, data=data, device=self.device)
<span class="ansi-green-intense-fg ansi-bold">    247</span> 
<span class="ansi-green-fg">--&gt; 248</span><span class="ansi-red-fg">         </span>active_learner<span class="ansi-blue-fg">(</span>store<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    249</span> 
<span class="ansi-green-intense-fg ansi-bold">    250</span> 

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/unified_experiment.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, log)</span>
<span class="ansi-green-intense-fg ansi-bold">    112</span> 
<span class="ansi-green-intense-fg ansi-bold">    113</span>             <span class="ansi-green-fg">if</span> isinstance<span class="ansi-blue-fg">(</span>acquisition_function<span class="ansi-blue-fg">,</span> CandidateBatchComputer<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 114</span><span class="ansi-red-fg">                 </span>candidate_batch <span class="ansi-blue-fg">=</span> acquisition_function<span class="ansi-blue-fg">.</span>compute_candidate_batch<span class="ansi-blue-fg">(</span>trained_model<span class="ansi-blue-fg">,</span> pool_loader<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>device<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    115</span>             <span class="ansi-green-fg">elif</span> isinstance<span class="ansi-blue-fg">(</span>acquisition_function<span class="ansi-blue-fg">,</span> EvalDatasetBatchComputer<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    116</span>                 <span class="ansi-green-fg">if</span> len<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">.</span>evaluation_dataset<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">&gt;</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">:</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/baseline_acquisition_functions.py</span> in <span class="ansi-cyan-fg">compute_candidate_batch</span><span class="ansi-blue-fg">(self, model, pool_loader, device)</span>
<span class="ansi-green-intense-fg ansi-bold">     62</span>         self<span class="ansi-blue-fg">,</span> model<span class="ansi-blue-fg">:</span> TrainedModel<span class="ansi-blue-fg">,</span> pool_loader<span class="ansi-blue-fg">:</span> torch<span class="ansi-blue-fg">.</span>utils<span class="ansi-blue-fg">.</span>data<span class="ansi-blue-fg">.</span>DataLoader<span class="ansi-blue-fg">,</span> device
<span class="ansi-green-intense-fg ansi-bold">     63</span>     ) -&gt; CandidateBatch:
<span class="ansi-green-fg">---&gt; 64</span><span class="ansi-red-fg">         </span>grad_embeddings <span class="ansi-blue-fg">=</span> model<span class="ansi-blue-fg">.</span>get_grad_embeddings<span class="ansi-blue-fg">(</span>pool_loader<span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> loss<span class="ansi-blue-fg">=</span>torch<span class="ansi-blue-fg">.</span>nn<span class="ansi-blue-fg">.</span>functional<span class="ansi-blue-fg">.</span>nll_loss<span class="ansi-blue-fg">,</span> model_labels<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">,</span> grad_embedding_type<span class="ansi-blue-fg">=</span>GradEmbeddingType<span class="ansi-blue-fg">.</span>LINEAR<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>device<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">=</span><span class="ansi-blue-fg">&#34;cpu&#34;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     65</span>         chosen_indices <span class="ansi-blue-fg">=</span> init_centers<span class="ansi-blue-fg">(</span>grad_embeddings<span class="ansi-blue-fg">.</span>squeeze<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>numpy<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>acquisition_size<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     66</span> 

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/trained_model.py</span> in <span class="ansi-cyan-fg">get_grad_embeddings</span><span class="ansi-blue-fg">(self, loader, num_samples, loss, grad_embedding_type, model_labels, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">     70</span>         storage_device<span class="ansi-blue-fg">:</span> object<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     71</span>     ):
<span class="ansi-green-fg">---&gt; 72</span><span class="ansi-red-fg">         grad_embeddings_N_K_E = self.model.get_grad_embeddings(
</span><span class="ansi-green-intense-fg ansi-bold">     73</span>             num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">     74</span>             loader<span class="ansi-blue-fg">=</span>loader<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">get_grad_embeddings</span><span class="ansi-blue-fg">(self, num_samples, loader, loss, grad_embedding_type, model_labels, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    182</span>         storage_device<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    183</span>     ):
<span class="ansi-green-fg">--&gt; 184</span><span class="ansi-red-fg">         return bmodule_get_grad_embeddings(
</span><span class="ansi-green-intense-fg ansi-bold">    185</span>             self<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    186</span>             num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">bmodule_get_grad_embeddings</span><span class="ansi-blue-fg">(self, num_samples, loader, loss, grad_embedding_type, model_labels, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    275</span> 
<span class="ansi-green-intense-fg ansi-bold">    276</span>     <span class="ansi-blue-fg">@</span>toma<span class="ansi-blue-fg">.</span>execute<span class="ansi-blue-fg">.</span>range<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> real_num_samples<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">128</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 277</span><span class="ansi-red-fg">     </span><span class="ansi-green-fg">def</span> get_prediction_batch<span class="ansi-blue-fg">(</span>start<span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    278</span>         <span class="ansi-green-fg">nonlocal</span> grad_embeddings
<span class="ansi-green-intense-fg ansi-bold">    279</span> 

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py</span> in <span class="ansi-cyan-fg">execute_range</span><span class="ansi-blue-fg">(func)</span>
<span class="ansi-green-intense-fg ansi-bold">    184</span> 
<span class="ansi-green-intense-fg ansi-bold">    185</span>             <span class="ansi-green-fg">def</span> execute_range<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 186</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> explicit<span class="ansi-blue-fg">.</span>range<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">,</span> start<span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">,</span> initial_step<span class="ansi-blue-fg">,</span> toma_cache_type<span class="ansi-blue-fg">=</span>cache_type<span class="ansi-blue-fg">,</span> toma_context<span class="ansi-blue-fg">=</span>context<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    187</span> 
<span class="ansi-green-intense-fg ansi-bold">    188</span>             <span class="ansi-green-fg">return</span> execute_range

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py</span> in <span class="ansi-cyan-fg">range</span><span class="ansi-blue-fg">(func, start, end, initial_step, toma_context, toma_cache_type, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    267</span>         <span class="ansi-green-fg">while</span> current <span class="ansi-blue-fg">&lt;</span> end<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    268</span>             <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 269</span><span class="ansi-red-fg">                 </span>func<span class="ansi-blue-fg">(</span>current<span class="ansi-blue-fg">,</span> min<span class="ansi-blue-fg">(</span>current <span class="ansi-blue-fg">+</span> batchsize<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    270</span>                 current <span class="ansi-blue-fg">+=</span> batchsize<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    271</span>                 gc_cuda<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">get_prediction_batch</span><span class="ansi-blue-fg">(start, end)</span>
<span class="ansi-green-intense-fg ansi-bold">    315</span> 
<span class="ansi-green-intense-fg ansi-bold">    316</span>             batch_grad_embedding <span class="ansi-blue-fg">=</span> grad_embedding_type<span class="ansi-blue-fg">.</span>get_grad_embedding<span class="ansi-blue-fg">(</span>batch_embeddings<span class="ansi-blue-fg">,</span> batch_loss_grad<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 317</span><span class="ansi-red-fg">             </span>grad_embeddings<span class="ansi-blue-fg">[</span>data_start<span class="ansi-blue-fg">:</span>data_end<span class="ansi-blue-fg">,</span> start<span class="ansi-blue-fg">:</span>end<span class="ansi-blue-fg">]</span><span class="ansi-blue-fg">.</span>copy_<span class="ansi-blue-fg">(</span>batch_grad_embedding<span class="ansi-blue-fg">,</span> non_blocking<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    318</span> 
<span class="ansi-green-intense-fg ansi-bold">    319</span>             data_start <span class="ansi-blue-fg">=</span> data_end

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

