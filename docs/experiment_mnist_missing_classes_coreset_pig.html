---

title: Experiment MNIST: PIGs


keywords: fastai
sidebar: home_sidebar

summary: "Can we get better by training on our assumptions?"
description: "Can we get better by training on our assumptions?"
nb_path: "09b_experiment_mnist_missing_classes_coreset_pig.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 09b_experiment_mnist_missing_classes_coreset_pig.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Appended /home/blackhc/PycharmProjects/bald-ical/src to paths
Switched to directory /home/blackhc/PycharmProjects/bald-ical
%load_ext autoreload
%autoreload 2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Import modules and functions were are going to use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">import</span> <span class="nn">traceback</span>

<span class="kn">from</span> <span class="nn">blackhc.project</span> <span class="kn">import</span> <span class="n">is_run_from_ipython</span>
<span class="kn">from</span> <span class="nn">blackhc.project.experiment</span> <span class="kn">import</span> <span class="n">embedded_experiments</span>

<span class="kn">import</span> <span class="nn">batchbald_redux.acquisition_functions.bald</span>
<span class="kn">import</span> <span class="nn">batchbald_redux.acquisition_functions.coreset</span>
<span class="kn">from</span> <span class="nn">batchbald_redux</span> <span class="kn">import</span> <span class="n">acquisition_functions</span><span class="p">,</span> <span class="n">baseline_acquisition_functions</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.experiment_data</span> <span class="kn">import</span> <span class="n">ImbalancedTestDistributionExperimentDataConfig</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.models</span> <span class="kn">import</span> <span class="n">MnistModelTrainer</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.unified_experiment</span> <span class="kn">import</span> <span class="n">UnifiedExperiment</span>
<span class="kn">from</span> <span class="nn">batchbald_redux.train_eval_model</span> <span class="kn">import</span> <span class="n">TrainExplicitEvalModel</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">configs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">UnifiedExperiment</span><span class="p">(</span>
        <span class="n">experiment_data_config</span><span class="o">=</span><span class="n">ImbalancedTestDistributionExperimentDataConfig</span><span class="p">(</span>
            <span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;MNIST&quot;</span><span class="p">,</span>
            <span class="n">repetitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">initial_training_set_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">validation_set_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
            <span class="n">validation_split_random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">evaluation_set_size</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
            <span class="n">add_dataset_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">minority_classes</span><span class="o">=</span><span class="p">{</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">},</span>
            <span class="n">minority_class_percentage</span><span class="o">=</span><span class="mf">0.</span>
        <span class="p">),</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span> <span class="o">+</span> <span class="mi">8945</span><span class="p">,</span>
        <span class="n">acquisition_function</span><span class="o">=</span><span class="n">acquisition_function</span><span class="p">,</span>
        <span class="n">acquisition_size</span><span class="o">=</span><span class="n">acquisition_size</span><span class="p">,</span>
        <span class="n">num_pool_samples</span><span class="o">=</span><span class="n">num_pool_samples</span><span class="p">,</span>
        <span class="n">max_training_set</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
        <span class="n">model_trainer_factory</span><span class="o">=</span><span class="n">MnistModelTrainer</span><span class="p">,</span>
        <span class="n">train_eval_model</span><span class="o">=</span><span class="n">TrainExplicitEvalModel</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">acquisition_size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">num_pool_samples</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">id_repetitions</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">acquisition_function</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="n">batchbald_redux</span><span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">bald</span><span class="o">.</span><span class="n">BALD</span><span class="p">,</span>
        <span class="n">batchbald_redux</span><span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">coreset</span><span class="o">.</span><span class="n">CoreSetPIG</span><span class="p">,</span>
        <span class="n">batchbald_redux</span><span class="o">.</span><span class="n">acquisition_functions</span><span class="o">.</span><span class="n">coreset</span><span class="o">.</span><span class="n">CoreSetPIGBALD</span><span class="p">,</span>
    <span class="p">]</span>
 <span class="p">]</span>
<span class="c1">#    + [</span>
<span class="c1">#     UnifiedExperiment(</span>
<span class="c1">#         experiment_data_config=StandardExperimentDataConfig(</span>
<span class="c1">#             id_dataset_name=&quot;MNIST&quot;,</span>
<span class="c1">#             id_repetitions=id_repetitions,</span>
<span class="c1">#             initial_training_set_size=20,</span>
<span class="c1">#             validation_set_size=4096,</span>
<span class="c1">#             validation_split_random_state=0,</span>
<span class="c1">#             evaluation_set_size=0,</span>
<span class="c1">#             add_dataset_noise=id_repetitions &gt; 1,</span>
<span class="c1">#             ood_dataset_config=None,</span>
<span class="c1">#         ),</span>
<span class="c1">#         seed=seed + 8945,</span>
<span class="c1">#         acquisition_function=acquisition_function,</span>
<span class="c1">#         acquisition_size=acquisition_size,</span>
<span class="c1">#         num_pool_samples=num_pool_samples,</span>
<span class="c1">#         max_training_set=120,</span>
<span class="c1">#         model_trainer_factory=MnistModelTrainer,</span>
<span class="c1">#     )</span>
<span class="c1">#     for acquisition_function in [</span>
<span class="c1">#         baseline_acquisition_functions.BADGE,</span>
<span class="c1">#         acquisition_functions.EPIG,</span>
<span class="c1">#         acquisition_functions.EvalBALD,</span>
<span class="c1">#         acquisition_functions.BALD,</span>
<span class="c1">#     ]</span>
<span class="c1">#     for seed in range(5)</span>
<span class="c1">#     for acquisition_size in [1]</span>
<span class="c1">#     for num_pool_samples in [100]</span>
<span class="c1">#     for id_repetitions in [1]</span>
<span class="c1"># ]</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">is_run_from_ipython</span><span class="p">()</span> <span class="ow">and</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">job_id</span><span class="p">,</span> <span class="n">store</span> <span class="ow">in</span> <span class="n">embedded_experiments</span><span class="p">(</span><span class="vm">__file__</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">configs</span><span class="p">)):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="n">job_id</span><span class="p">]</span>
        <span class="n">config</span><span class="o">.</span><span class="n">seed</span> <span class="o">+=</span> <span class="n">job_id</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">asdict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">store</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">store</span><span class="o">=</span><span class="n">store</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">store</span><span class="p">[</span><span class="s2">&quot;exception&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">traceback</span><span class="o">.</span><span class="n">format_exc</span><span class="p">()</span>
            <span class="k">raise</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">configs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>18</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">prettyprinter</span>

<span class="n">prettyprinter</span><span class="o">.</span><span class="n">install_extras</span><span class="p">({</span><span class="s2">&quot;dataclasses&quot;</span><span class="p">})</span>
<span class="n">prettyprinter</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">configs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIG,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIGBALD,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIG,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8945,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIGBALD,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIG,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIGBALD,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIG,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8946,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIGBALD,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIG,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=1,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIGBALD,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIG,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    ),
    batchbald_redux.unified_experiment.UnifiedExperiment(
        seed=8947,
        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(
            dataset_name=&#39;MNIST&#39;,
            repetitions=1,
            initial_training_set_size=20,
            validation_set_size=4000,
            validation_split_random_state=0,
            evaluation_set_size=4000,
            add_dataset_noise=False,
            minority_classes={2, 3, 4, 5, 7},
            minority_class_percentage=0.0
        ),
        acquisition_size=10,
        max_training_set=120,
        # class
        acquisition_function=batchbald_redux.acquisition_functions.CoreSetPIGBALD,
        # class
        train_eval_model=batchbald_redux.train_eval_model.TrainExplicitEvalModel,
        model_trainer_factory=batchbald_redux.models.MnistModelTrainer  # class
    )
]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">store</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">configs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

<span class="n">config</span><span class="o">.</span><span class="n">max_training_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">config</span><span class="o">.</span><span class="n">num_pool_samples</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">config</span><span class="o">.</span><span class="n">evaluation_set_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">config</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">store</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return floored.astype(np.int)
/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  return floored.astype(np.int)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Evaluation Set Class Counts: [800, 800, 0, 0, 0, 0, 800, 0, 800, 800]
Creating: CoreSetPIGBALD(
	acquisition_size=1,
	num_pool_samples=5
)
Creating: MnistModelTrainer(
	device=cuda,
	num_training_samples=1,
	num_validation_samples=20,
	max_training_epochs=1
)
Creating: TrainExplicitEvalModel(

)
Training set size 20:
Epoch metrics: {&#39;accuracy&#39;: 0.4921598381385938, &#39;crossentropy&#39;: 1.9003716223036091}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.4921598381385938)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5254478827361564, &#39;crossentropy&#39;: tensor(1.7038)}
Epoch metrics: {&#39;accuracy&#39;: 0.9595346484572584, &#39;crossentropy&#39;: 0.2310884614514171}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9595346484572584)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.5273528874213971], indices=[39170])
[(&#39;id&#39;, 42213)]
Acquiring (label, score)s: 9 (0.5274)
Training set size 21:
Epoch metrics: {&#39;accuracy&#39;: 0.5366717248356095, &#39;crossentropy&#39;: 1.8462990864877695}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.5366717248356095)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.6091205211726385, &#39;crossentropy&#39;: tensor(1.6142)}
Epoch metrics: {&#39;accuracy&#39;: 0.9524532119372787, &#39;crossentropy&#39;: 0.24733110851540852}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9524532119372787)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.21572065899850443], indices=[18046])
[(&#39;id&#39;, 19470)]
Acquiring (label, score)s: 0 (0.2157)
Training set size 22:
Epoch metrics: {&#39;accuracy&#39;: 0.6793120890237734, &#39;crossentropy&#39;: 1.7569914671526934}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.6793120890237734)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.7210912052117264, &#39;crossentropy&#39;: tensor(1.6012)}
Epoch metrics: {&#39;accuracy&#39;: 0.9519473950429944, &#39;crossentropy&#39;: 0.24403711187911867}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9519473950429944)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.32757135265707665], indices=[21623])
[(&#39;id&#39;, 23293)]
Acquiring (label, score)s: 6 (0.3276)
Training set size 23:
Epoch metrics: {&#39;accuracy&#39;: 0.5973697521497218, &#39;crossentropy&#39;: 1.9510006993843199}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.5973697521497218)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.7206840390879479, &#39;crossentropy&#39;: tensor(1.7656)}
Epoch metrics: {&#39;accuracy&#39;: 0.9585230146686899, &#39;crossentropy&#39;: 0.22182592485791575}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9585230146686899)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.4754005835038122], indices=[23724])
[(&#39;id&#39;, 25579)]
Acquiring (label, score)s: 8 (0.4754)
Training set size 24:
Epoch metrics: {&#39;accuracy&#39;: 0.48254931714719274, &#39;crossentropy&#39;: 1.9340383326940966}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.48254931714719274)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5368485342019544, &#39;crossentropy&#39;: tensor(1.7565)}
Epoch metrics: {&#39;accuracy&#39;: 0.9428426909458776, &#39;crossentropy&#39;: 0.29598414481862}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9428426909458776)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.3395063962411848], indices=[37921])
[(&#39;id&#39;, 40867)]
Acquiring (label, score)s: 8 (0.3395)
Training set size 25:
Epoch metrics: {&#39;accuracy&#39;: 0.4936772888214466, &#39;crossentropy&#39;: 1.7923489747532222}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.4936772888214466)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.5539495114006515, &#39;crossentropy&#39;: tensor(1.6026)}
Epoch metrics: {&#39;accuracy&#39;: 0.9534648457258472, &#39;crossentropy&#39;: 0.23926327296532218}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9534648457258472)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.24913571986378402], indices=[38645])
[(&#39;id&#39;, 41647)]
Acquiring (label, score)s: 1 (0.2491)
Training set size 26:
Epoch metrics: {&#39;accuracy&#39;: 0.6342943854324734, &#39;crossentropy&#39;: 1.6728567638117433}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.6342943854324734)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.63314332247557, &#39;crossentropy&#39;: tensor(1.5951)}
Epoch metrics: {&#39;accuracy&#39;: 0.9484066767830045, &#39;crossentropy&#39;: 0.2605491497025806}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9484066767830045)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.3263394577319514], indices=[17899])
[(&#39;id&#39;, 19301)]
Acquiring (label, score)s: 0 (0.3263)
Training set size 27:
Epoch metrics: {&#39;accuracy&#39;: 0.7708649468892261, &#39;crossentropy&#39;: 1.4996099903780098}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.7708649468892261)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.8377442996742671, &#39;crossentropy&#39;: tensor(1.3971)}
Epoch metrics: {&#39;accuracy&#39;: 0.9458775923115832, &#39;crossentropy&#39;: 0.28655153579282594}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9458775923115832)
RestoringEarlyStopping: Restoring optimizer.
CandidateBatch(scores=[0.2341271318513647], indices=[51134])
[(&#39;id&#39;, 55097)]
Acquiring (label, score)s: 8 (0.2341)
Training set size 28:
Epoch metrics: {&#39;accuracy&#39;: 0.469903894790086, &#39;crossentropy&#39;: 1.6288945401745971}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.469903894790086)
RestoringEarlyStopping: Restoring optimizer.
Perf after training {&#39;accuracy&#39;: 0.44177524429967424, &#39;crossentropy&#39;: tensor(1.5574)}
Epoch metrics: {&#39;accuracy&#39;: 0.9549822964087, &#39;crossentropy&#39;: 0.2426720654491107}
RestoringEarlyStopping: Restoring best parameters. (Score: 0.9549822964087)
RestoringEarlyStopping: Restoring optimizer.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-25-6e3f5b49750c&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      6</span> config<span class="ansi-blue-fg">.</span>num_pool_samples <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">5</span>
<span class="ansi-green-intense-fg ansi-bold">      7</span> config<span class="ansi-blue-fg">.</span>evaluation_set_size <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">1024</span>
<span class="ansi-green-fg">----&gt; 8</span><span class="ansi-red-fg"> </span>config<span class="ansi-blue-fg">.</span>run<span class="ansi-blue-fg">(</span>store<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/unified_experiment.py</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-fg">(self, store)</span>
<span class="ansi-green-intense-fg ansi-bold">    252</span>         )
<span class="ansi-green-intense-fg ansi-bold">    253</span> 
<span class="ansi-green-fg">--&gt; 254</span><span class="ansi-red-fg">         </span>active_learner<span class="ansi-blue-fg">(</span>store<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    255</span> 
<span class="ansi-green-intense-fg ansi-bold">    256</span> <span class="ansi-red-fg"># Cell</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/unified_experiment.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-fg">(self, log)</span>
<span class="ansi-green-intense-fg ansi-bold">    153</span>                 )
<span class="ansi-green-intense-fg ansi-bold">    154</span> 
<span class="ansi-green-fg">--&gt; 155</span><span class="ansi-red-fg">                 candidate_batch = acquisition_function.compute_candidate_batch(
</span><span class="ansi-green-intense-fg ansi-bold">    156</span>                     trained_model<span class="ansi-blue-fg">,</span> trained_eval_model<span class="ansi-blue-fg">,</span> pool_loader<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>self<span class="ansi-blue-fg">.</span>device
<span class="ansi-green-intense-fg ansi-bold">    157</span>                 )

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/acquisition_functions.py</span> in <span class="ansi-cyan-fg">compute_candidate_batch</span><span class="ansi-blue-fg">(self, model, eval_model, pool_loader, device)</span>
<span class="ansi-green-intense-fg ansi-bold">    441</span>             pool_loader<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>num_pool_samples<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#34;cpu&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">    442</span>         )
<span class="ansi-green-fg">--&gt; 443</span><span class="ansi-red-fg">         eval_log_probs_N_K_C, eval_labels_N = eval_model.get_log_probs_N_K_C_labels_N(
</span><span class="ansi-green-intense-fg ansi-bold">    444</span>             pool_loader<span class="ansi-blue-fg">,</span> self<span class="ansi-blue-fg">.</span>num_pool_samples<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">&#34;cpu&#34;</span>
<span class="ansi-green-intense-fg ansi-bold">    445</span>         )

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/trained_model.py</span> in <span class="ansi-cyan-fg">get_log_probs_N_K_C_labels_N</span><span class="ansi-blue-fg">(self, loader, num_samples, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">     50</span>         self<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">:</span> DataLoader<span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">:</span> int<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">:</span> object<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">:</span> object
<span class="ansi-green-intense-fg ansi-bold">     51</span>     ):
<span class="ansi-green-fg">---&gt; 52</span><span class="ansi-red-fg">         log_probs_N_K_C, labels_B = self.model.get_predictions_labels(
</span><span class="ansi-green-intense-fg ansi-bold">     53</span>             num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">=</span>loader<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">=</span>device<span class="ansi-blue-fg">,</span> storage_device<span class="ansi-blue-fg">=</span>storage_device
<span class="ansi-green-intense-fg ansi-bold">     54</span>         )

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">get_predictions_labels</span><span class="ansi-blue-fg">(self, num_samples, loader, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    147</span>         self<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span><span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">:</span> int<span class="ansi-blue-fg">,</span> loader<span class="ansi-blue-fg">:</span> data<span class="ansi-blue-fg">.</span>DataLoader<span class="ansi-blue-fg">,</span> device<span class="ansi-blue-fg">,</span> storage_device
<span class="ansi-green-intense-fg ansi-bold">    148</span>     ):
<span class="ansi-green-fg">--&gt; 149</span><span class="ansi-red-fg">         return bmodule_get_predictions_labels(
</span><span class="ansi-green-intense-fg ansi-bold">    150</span>             self<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">    151</span>             num_samples<span class="ansi-blue-fg">=</span>num_samples<span class="ansi-blue-fg">,</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/autograd/grad_mode.py</span> in <span class="ansi-cyan-fg">decorate_context</span><span class="ansi-blue-fg">(*args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">     24</span>         <span class="ansi-green-fg">def</span> decorate_context<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     25</span>             <span class="ansi-green-fg">with</span> self<span class="ansi-blue-fg">.</span>__class__<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 26</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> func<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     27</span>         <span class="ansi-green-fg">return</span> cast<span class="ansi-blue-fg">(</span>F<span class="ansi-blue-fg">,</span> decorate_context<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     28</span> 

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">bmodule_get_predictions_labels</span><span class="ansi-blue-fg">(self, num_samples, loader, device, storage_device)</span>
<span class="ansi-green-intense-fg ansi-bold">    391</span> 
<span class="ansi-green-intense-fg ansi-bold">    392</span>     <span class="ansi-blue-fg">@</span>toma<span class="ansi-blue-fg">.</span>execute<span class="ansi-blue-fg">.</span>range<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> num_samples<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">128</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 393</span><span class="ansi-red-fg">     </span><span class="ansi-green-fg">def</span> get_prediction_batch<span class="ansi-blue-fg">(</span>start<span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    394</span>         <span class="ansi-green-fg">nonlocal</span> predictions
<span class="ansi-green-intense-fg ansi-bold">    395</span>         <span class="ansi-green-fg">nonlocal</span> labels

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py</span> in <span class="ansi-cyan-fg">execute_range</span><span class="ansi-blue-fg">(func)</span>
<span class="ansi-green-intense-fg ansi-bold">    184</span> 
<span class="ansi-green-intense-fg ansi-bold">    185</span>             <span class="ansi-green-fg">def</span> execute_range<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 186</span><span class="ansi-red-fg">                 </span><span class="ansi-green-fg">return</span> explicit<span class="ansi-blue-fg">.</span>range<span class="ansi-blue-fg">(</span>func<span class="ansi-blue-fg">,</span> start<span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">,</span> initial_step<span class="ansi-blue-fg">,</span> toma_cache_type<span class="ansi-blue-fg">=</span>cache_type<span class="ansi-blue-fg">,</span> toma_context<span class="ansi-blue-fg">=</span>context<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    187</span> 
<span class="ansi-green-intense-fg ansi-bold">    188</span>             <span class="ansi-green-fg">return</span> execute_range

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py</span> in <span class="ansi-cyan-fg">range</span><span class="ansi-blue-fg">(func, start, end, initial_step, toma_context, toma_cache_type, *args, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    267</span>         <span class="ansi-green-fg">while</span> current <span class="ansi-blue-fg">&lt;</span> end<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    268</span>             <span class="ansi-green-fg">try</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">--&gt; 269</span><span class="ansi-red-fg">                 </span>func<span class="ansi-blue-fg">(</span>current<span class="ansi-blue-fg">,</span> min<span class="ansi-blue-fg">(</span>current <span class="ansi-blue-fg">+</span> batchsize<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> end<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">*</span>args<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    270</span>                 current <span class="ansi-blue-fg">+=</span> batchsize<span class="ansi-blue-fg">.</span>get<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    271</span>                 gc_cuda<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/PycharmProjects/bald-ical/batchbald_redux/consistent_mc_dropout.py</span> in <span class="ansi-cyan-fg">get_prediction_batch</span><span class="ansi-blue-fg">(start, end)</span>
<span class="ansi-green-intense-fg ansi-bold">    405</span> 
<span class="ansi-green-intense-fg ansi-bold">    406</span>         data_start <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">0</span>
<span class="ansi-green-fg">--&gt; 407</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">for</span> batch_x<span class="ansi-blue-fg">,</span> batch_labels <span class="ansi-green-fg">in</span> loader<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    408</span>             batch_x <span class="ansi-blue-fg">=</span> batch_x<span class="ansi-blue-fg">.</span>to<span class="ansi-blue-fg">(</span>device<span class="ansi-blue-fg">=</span>device<span class="ansi-blue-fg">,</span> non_blocking<span class="ansi-blue-fg">=</span><span class="ansi-green-fg">True</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    409</span>             batch_predictions <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">(</span>batch_x<span class="ansi-blue-fg">,</span> num_sub_samples<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/dataloader.py</span> in <span class="ansi-cyan-fg">__next__</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    433</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_sampler_iter <span class="ansi-green-fg">is</span> <span class="ansi-green-fg">None</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    434</span>             self<span class="ansi-blue-fg">.</span>_reset<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 435</span><span class="ansi-red-fg">         </span>data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_next_data<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    436</span>         self<span class="ansi-blue-fg">.</span>_num_yielded <span class="ansi-blue-fg">+=</span> <span class="ansi-cyan-fg">1</span>
<span class="ansi-green-intense-fg ansi-bold">    437</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_dataset_kind <span class="ansi-blue-fg">==</span> _DatasetKind<span class="ansi-blue-fg">.</span>Iterable <span class="ansi-green-fg">and</span><span class="ansi-red-fg"> </span><span class="ansi-red-fg">\</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/dataloader.py</span> in <span class="ansi-cyan-fg">_next_data</span><span class="ansi-blue-fg">(self)</span>
<span class="ansi-green-intense-fg ansi-bold">    473</span>     <span class="ansi-green-fg">def</span> _next_data<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    474</span>         index <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_next_index<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># may raise StopIteration</span>
<span class="ansi-green-fg">--&gt; 475</span><span class="ansi-red-fg">         </span>data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>_dataset_fetcher<span class="ansi-blue-fg">.</span>fetch<span class="ansi-blue-fg">(</span>index<span class="ansi-blue-fg">)</span>  <span class="ansi-red-fg"># may raise StopIteration</span>
<span class="ansi-green-intense-fg ansi-bold">    476</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>_pin_memory<span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    477</span>             data <span class="ansi-blue-fg">=</span> _utils<span class="ansi-blue-fg">.</span>pin_memory<span class="ansi-blue-fg">.</span>pin_memory<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py</span> in <span class="ansi-cyan-fg">fetch</span><span class="ansi-blue-fg">(self, possibly_batched_index)</span>
<span class="ansi-green-intense-fg ansi-bold">     45</span>         <span class="ansi-green-fg">else</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     46</span>             data <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>dataset<span class="ansi-blue-fg">[</span>possibly_batched_index<span class="ansi-blue-fg">]</span>
<span class="ansi-green-fg">---&gt; 47</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>collate_fn<span class="ansi-blue-fg">(</span>data<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">default_collate</span><span class="ansi-blue-fg">(batch)</span>
<span class="ansi-green-intense-fg ansi-bold">     81</span>             <span class="ansi-green-fg">raise</span> RuntimeError<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;each element in list of batch should be of equal size&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     82</span>         transposed <span class="ansi-blue-fg">=</span> zip<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>batch<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 83</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> <span class="ansi-blue-fg">[</span>default_collate<span class="ansi-blue-fg">(</span>samples<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> samples <span class="ansi-green-fg">in</span> transposed<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">     84</span> 
<span class="ansi-green-intense-fg ansi-bold">     85</span>     <span class="ansi-green-fg">raise</span> TypeError<span class="ansi-blue-fg">(</span>default_collate_err_msg_format<span class="ansi-blue-fg">.</span>format<span class="ansi-blue-fg">(</span>elem_type<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">&lt;listcomp&gt;</span><span class="ansi-blue-fg">(.0)</span>
<span class="ansi-green-intense-fg ansi-bold">     81</span>             <span class="ansi-green-fg">raise</span> RuntimeError<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#39;each element in list of batch should be of equal size&#39;</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     82</span>         transposed <span class="ansi-blue-fg">=</span> zip<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">*</span>batch<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 83</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> <span class="ansi-blue-fg">[</span>default_collate<span class="ansi-blue-fg">(</span>samples<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">for</span> samples <span class="ansi-green-fg">in</span> transposed<span class="ansi-blue-fg">]</span>
<span class="ansi-green-intense-fg ansi-bold">     84</span> 
<span class="ansi-green-intense-fg ansi-bold">     85</span>     <span class="ansi-green-fg">raise</span> TypeError<span class="ansi-blue-fg">(</span>default_collate_err_msg_format<span class="ansi-blue-fg">.</span>format<span class="ansi-blue-fg">(</span>elem_type<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py</span> in <span class="ansi-cyan-fg">default_collate</span><span class="ansi-blue-fg">(batch)</span>
<span class="ansi-green-intense-fg ansi-bold">     53</span>             storage <span class="ansi-blue-fg">=</span> elem<span class="ansi-blue-fg">.</span>storage<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">.</span>_new_shared<span class="ansi-blue-fg">(</span>numel<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     54</span>             out <span class="ansi-blue-fg">=</span> elem<span class="ansi-blue-fg">.</span>new<span class="ansi-blue-fg">(</span>storage<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 55</span><span class="ansi-red-fg">         </span><span class="ansi-green-fg">return</span> torch<span class="ansi-blue-fg">.</span>stack<span class="ansi-blue-fg">(</span>batch<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">0</span><span class="ansi-blue-fg">,</span> out<span class="ansi-blue-fg">=</span>out<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     56</span>     <span class="ansi-green-fg">elif</span> elem_type<span class="ansi-blue-fg">.</span>__module__ <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;numpy&#39;</span> <span class="ansi-green-fg">and</span> elem_type<span class="ansi-blue-fg">.</span>__name__ <span class="ansi-blue-fg">!=</span> <span class="ansi-blue-fg">&#39;str_&#39;</span><span class="ansi-red-fg"> </span><span class="ansi-red-fg">\</span>
<span class="ansi-green-intense-fg ansi-bold">     57</span>             <span class="ansi-green-fg">and</span> elem_type<span class="ansi-blue-fg">.</span>__name__ <span class="ansi-blue-fg">!=</span> <span class="ansi-blue-fg">&#39;string_&#39;</span><span class="ansi-blue-fg">:</span>

<span class="ansi-red-fg">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

