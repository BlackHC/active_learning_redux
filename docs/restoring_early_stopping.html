---

title: Restoring Early Stopping


keywords: fastai
sidebar: home_sidebar

summary: "“The past is never where you think you left it.”"
description: "“The past is never where you think you left it.”"
nb_path: "05c_restoring_early_stopping.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 05c_restoring_early_stopping.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="RestoringEarlyStopping" class="doc_header"><code>class</code> <code>RestoringEarlyStopping</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L19" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>RestoringEarlyStopping</code>(<strong><code>patience</code></strong>, <strong><code>score_function</code></strong>, <strong><code>training_engine</code></strong>:<code>Engine</code>, <strong><code>validation_engine</code></strong>:<code>Engine</code>, <strong><code>module</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>optimizer</code></strong>:<code>Optimizer</code>=<em><code>None</code></em>, <strong><code>out_of_patience_callback</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>RestoringEarlyStopping handler can be used to stop the training if no improvement after a given number of events</p>
<p>Args:
    patience (int):
        Number of events to wait if no improvement and then stop the training
    score_function (Callable):
        It should be a function taking a single argument, an <code>ignite.engine.Engine</code> object,
        and return a score <code>float</code>. An improvement is considered if the score is higher.
    trainer (Engine):
        trainer engine to stop the run if no improvement</p>
<p>Examples:</p>
<p>.. code-block:: python</p>

<pre><code>from ignite.engine import Engine, Events
from ignite.handlers import EarlyStopping

def score_function(engine):
    val_loss = engine.state.metrics['nll']
    return -val_loss

handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)
# Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset)
evaluator.add_event_handler(Events.COMPLETED, handler)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">inf</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">ignite.engine</span> <span class="kn">import</span> <span class="n">Engine</span><span class="p">,</span> <span class="n">Events</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">EPOCH_DEPRECATION_WARNING</span>


<span class="k">class</span> <span class="nc">RestoringEarlyStopping</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RestoringEarlyStopping handler can be used to stop the training if no improvement after a given number of events</span>

<span class="sd">    Args:</span>
<span class="sd">        patience (int):</span>
<span class="sd">            Number of events to wait if no improvement and then stop the training</span>
<span class="sd">        score_function (Callable):</span>
<span class="sd">            It should be a function taking a single argument, an `ignite.engine.Engine` object,</span>
<span class="sd">            and return a score `float`. An improvement is considered if the score is higher.</span>
<span class="sd">        trainer (Engine):</span>
<span class="sd">            trainer engine to stop the run if no improvement</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        from ignite.engine import Engine, Events</span>
<span class="sd">        from ignite.handlers import EarlyStopping</span>

<span class="sd">        def score_function(engine):</span>
<span class="sd">            val_loss = engine.state.metrics[&#39;nll&#39;]</span>
<span class="sd">            return -val_loss</span>

<span class="sd">        handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)</span>
<span class="sd">        # Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset)</span>
<span class="sd">        evaluator.add_event_handler(Events.COMPLETED, handler)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">patience</span><span class="p">,</span>
        <span class="n">score_function</span><span class="p">,</span>
        <span class="n">training_engine</span><span class="p">:</span> <span class="n">Engine</span><span class="p">,</span>
        <span class="n">validation_engine</span><span class="p">:</span> <span class="n">Engine</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_of_patience_callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">out_of_patience_callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">default_out_of_patience_callback</span><span class="p">():</span>
                <span class="n">training_engine</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>

            <span class="n">out_of_patience_callback</span> <span class="o">=</span> <span class="n">default_out_of_patience_callback</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">score_function</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument score_function should be a function&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">patience</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument patience should be non-negative integer&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">score_function</span> <span class="o">=</span> <span class="n">score_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_of_patience_callback</span> <span class="o">=</span> <span class="n">out_of_patience_callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">restore_epoch</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span> <span class="o">=</span> <span class="n">training_engine</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_engine</span> <span class="o">=</span> <span class="n">validation_engine</span>
        <span class="n">validation_engine</span><span class="o">.</span><span class="n">add_event_handler</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">EPOCH_COMPLETED</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_epoch_completed</span><span class="p">)</span>
        <span class="n">training_engine</span><span class="o">.</span><span class="n">add_event_handler</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">COMPLETED</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_completed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">snapshot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">restore_best</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;RestoringEarlyStopping: Restoring best parameters. (Score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">best_score</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RestoringEarlyStopping: Restoring optimizer.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">on_epoch_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_function</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">score</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RestoringEarlyStopping: </span><span class="si">%i</span><span class="s2"> / </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RestoringEarlyStopping: Out of patience&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">restore_best</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">restore_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>

                <span class="c1"># Reset the counter in case we keep training after adjusting the model.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_of_patience_callback</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span> <span class="o">=</span> <span class="n">score</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">snapshot</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">on_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_epoch</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_epoch</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_best</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="PatienceWithSnapshot" class="doc_header"><code>class</code> <code>PatienceWithSnapshot</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L134" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>PatienceWithSnapshot</code>(<strong><code>name</code></strong>:<code>str</code>=<em><code>''</code></em>, <strong><code>patience</code></strong>, <strong><code>score_function</code></strong>, <strong><code>training_engine</code></strong>:<code>Engine</code>, <strong><code>validation_engine</code></strong>:<code>Engine</code>, <strong><code>module</code></strong>:<code>Module</code>=<em><code>None</code></em>, <strong><code>optimizer</code></strong>:<code>Optimizer</code>=<em><code>None</code></em>, <strong><code>out_of_patience_callback</code></strong>=<em><code>None</code></em>)</p>
</blockquote>
<p>PatienceSnapshot handler can be used to stop the training if no improvement after a given number of events</p>
<p>Args:
    patience (int):
        Number of events to wait if no improvement and then stop the training
    score_function (Callable):
        It should be a function taking a single argument, an <code>ignite.engine.Engine</code> object,
        and return a score <code>float</code>. An improvement is considered if the score is higher.
    trainer (Engine):
        trainer engine to stop the run if no improvement</p>
<p>Examples:</p>
<p>.. code-block:: python</p>

<pre><code>from ignite.engine import Engine, Events
from ignite.handlers import EarlyStopping

def score_function(engine):
    val_loss = engine.state.metrics['nll']
    return -val_loss

handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)
# Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset)
evaluator.add_event_handler(Events.COMPLETED, handler)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">PatienceWithSnapshot</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;PatienceSnapshot handler can be used to stop the training if no improvement after a given number of events</span>

<span class="sd">    Args:</span>
<span class="sd">        patience (int):</span>
<span class="sd">            Number of events to wait if no improvement and then stop the training</span>
<span class="sd">        score_function (Callable):</span>
<span class="sd">            It should be a function taking a single argument, an `ignite.engine.Engine` object,</span>
<span class="sd">            and return a score `float`. An improvement is considered if the score is higher.</span>
<span class="sd">        trainer (Engine):</span>
<span class="sd">            trainer engine to stop the run if no improvement</span>

<span class="sd">    Examples:</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        from ignite.engine import Engine, Events</span>
<span class="sd">        from ignite.handlers import EarlyStopping</span>

<span class="sd">        def score_function(engine):</span>
<span class="sd">            val_loss = engine.state.metrics[&#39;nll&#39;]</span>
<span class="sd">            return -val_loss</span>

<span class="sd">        handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)</span>
<span class="sd">        # Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset)</span>
<span class="sd">        evaluator.add_event_handler(Events.COMPLETED, handler)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
        <span class="n">patience</span><span class="p">,</span>
        <span class="n">score_function</span><span class="p">,</span>
        <span class="n">training_engine</span><span class="p">:</span> <span class="n">Engine</span><span class="p">,</span>
        <span class="n">validation_engine</span><span class="p">:</span> <span class="n">Engine</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_of_patience_callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">out_of_patience_callback</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">default_out_of_patience_callback</span><span class="p">(</span><span class="n">module_state_dict</span><span class="p">,</span> <span class="n">optimizer_state_dict</span><span class="p">):</span>
                <span class="n">training_engine</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>

            <span class="n">out_of_patience_callback</span> <span class="o">=</span> <span class="n">default_out_of_patience_callback</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">score_function</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Argument score_function should be a function&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">patience</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Argument patience should be non-negative integer&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">score_function</span> <span class="o">=</span> <span class="n">score_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_of_patience_callback</span> <span class="o">=</span> <span class="n">out_of_patience_callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callback_epoch</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span> <span class="o">=</span> <span class="n">training_engine</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_engine</span> <span class="o">=</span> <span class="n">validation_engine</span>
        <span class="n">validation_engine</span><span class="o">.</span><span class="n">add_event_handler</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">EPOCH_COMPLETED</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_epoch_completed</span><span class="p">)</span>
        <span class="n">training_engine</span><span class="o">.</span><span class="n">add_event_handler</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">COMPLETED</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_completed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">is_out_of_patience</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span>

    <span class="k">def</span> <span class="nf">snapshot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_state_dict</span><span class="p">,</span> <span class="n">optimizer_state_dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">PatienceWithSnapshot: Restoring best parameters. (Score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">best_score</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">PatienceWithSnapshot: Restoring optimizer.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">on_epoch_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_function</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">score</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">PatienceWithSnapshot: %i / %i&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">PatienceWithSnapshot: Out of patience&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">PatienceWithSnapshot: Best score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">best_score</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">callback_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>

                <span class="n">module_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_module_state_dict</span><span class="p">)</span>
                <span class="n">optimizer_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_optimizer_state_dict</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_of_patience_callback</span><span class="p">(</span><span class="n">module_state_dict</span><span class="p">,</span> <span class="n">optimizer_state_dict</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_score</span> <span class="o">=</span> <span class="n">score</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">snapshot</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">on_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_epoch</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">callback_epoch</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callback_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="suggest_limit_schedule" class="doc_header"><code>suggest_limit_schedule</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L253" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>suggest_limit_schedule</code>(<strong><code>patience_schedule</code></strong>, <strong><code>max_epochs</code></strong>, <strong><code>factor</code></strong>=<em><code>2</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">suggest_limit_schedule</span><span class="p">(</span><span class="n">patience_schedule</span><span class="p">,</span> <span class="n">max_epochs</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">backward_limit_schedule</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">current_epoch</span> <span class="o">=</span> <span class="n">max_epochs</span>
    <span class="k">for</span> <span class="n">patience</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">patience_schedule</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">-=</span> <span class="n">factor</span> <span class="o">*</span> <span class="n">patience</span>
        <span class="n">backward_limit_schedule</span> <span class="o">+=</span> <span class="p">[</span><span class="n">current_epoch</span><span class="p">]</span>

    <span class="n">limit_schedule</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">backward_limit_schedule</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">limit_schedule</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">limit_schedule</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">patience</span> <span class="ow">in</span> <span class="n">patience_schedule</span><span class="p">:</span>
            <span class="n">current_epoch</span> <span class="o">+=</span> <span class="n">factor</span> <span class="o">*</span> <span class="n">patience</span>
            <span class="n">limit_schedule</span> <span class="o">+=</span> <span class="p">[</span><span class="n">current_epoch</span><span class="p">]</span>
        <span class="n">max_epochs</span> <span class="o">=</span> <span class="n">current_epoch</span>

    <span class="k">return</span> <span class="n">limit_schedule</span><span class="p">,</span> <span class="n">max_epochs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">suggest_limit_schedule</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="mi">120</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>([60, 100], 120)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">suggest_limit_schedule</span><span class="p">([</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="mi">120</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>([60, 100, 140], 140)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ModuleOptimizerSnapshotter" class="doc_header"><code>class</code> <code>ModuleOptimizerSnapshotter</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L273" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ModuleOptimizerSnapshotter</code>(<strong><code>module</code></strong>:<code>Module</code>, <strong><code>optimizer</code></strong>:<code>Optimizer</code>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ReduceLROnPlateauWithSchedule" class="doc_header"><code>class</code> <code>ReduceLROnPlateauWithSchedule</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L302" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ReduceLROnPlateauWithSchedule</code>(<strong><code>optimizer</code></strong>:<code>Optimizer</code>, <strong><code>factor_schedule</code></strong>:<code>[&lt;class 'float'&gt;]</code>, <strong><code>patience_schedule</code></strong>:<code>[&lt;class 'int'&gt;]</code>, <strong><code>limit_schedule</code></strong>:<code>[&lt;class 'int'&gt;]</code>, <strong><code>end_callback</code></strong>:<code>Callable</code>, <strong><code>next_era_callback</code></strong>:<code>Callable</code>, <strong><code>mode</code></strong>=<em><code>'min'</code></em>, <strong><code>threshold</code></strong>=<em><code>0.0001</code></em>, <strong><code>threshold_mode</code></strong>=<em><code>'rel'</code></em>, <strong><code>cooldown</code></strong>=<em><code>0</code></em>, <strong><code>min_lr</code></strong>=<em><code>0</code></em>, <strong><code>eps</code></strong>=<em><code>1e-08</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>, <strong><code>module</code></strong>:<code>Module</code>=<em><code>None</code></em>)</p>
</blockquote>
<p>Reduce learning rate when a metric has stopped improving.
Models often benefit from reducing the learning rate by a factor
of 2-10 once learning stagnates. This scheduler reads a metrics
quantity and if no improvement is seen for a 'patience' number
of epochs, the learning rate is reduced.</p>
<p>Args:
    optimizer (Optimizer): Wrapped optimizer.
    mode (str): One of <code>min</code>, <code>max</code>. In <code>min</code> mode, lr will
        be reduced when the quantity monitored has stopped
        decreasing; in <code>max</code> mode it will be reduced when the
        quantity monitored has stopped increasing. Default: 'min'.
    factor (float): Factor by which the learning rate will be
        reduced. new_lr = lr <em> factor. Default: 0.1.
    patience (int): Number of epochs with no improvement after
        which learning rate will be reduced. For example, if
        <code>patience = 2</code>, then we will ignore the first 2 epochs
        with no improvement, and will only decrease the LR after the
        3rd epoch if the loss still hasn't improved then.
        Default: 10.
    threshold (float): Threshold for measuring the new optimum,
        to only focus on significant changes. Default: 1e-4.
    threshold_mode (str): One of <code>rel</code>, <code>abs</code>. In <code>rel</code> mode,
        dynamic_threshold = best </em> ( 1 + threshold ) in 'max'
        mode or best * ( 1 - threshold ) in <code>min</code> mode.
        In <code>abs</code> mode, dynamic_threshold = best + threshold in
        <code>max</code> mode or best - threshold in <code>min</code> mode. Default: 'rel'.
    cooldown (int): Number of epochs to wait before resuming
        normal operation after lr has been reduced. Default: 0.
    min_lr (float or list): A scalar or a list of scalars. A
        lower bound on the learning rate of all param groups
        or each group respectively. Default: 0.
    eps (float): Minimal decay applied to lr. If the difference
        between new and old lr is smaller than eps, the update is
        ignored. Default: 1e-8.
    verbose (bool): If <code>True</code>, prints a message to stdout for
        each update. Default: <code>False</code>.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
&gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')
&gt;&gt;&gt; for epoch in range(10):
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     val_loss = validate(...)
&gt;&gt;&gt;     # Note that step should be called after validate()
&gt;&gt;&gt;     scheduler.step(val_loss)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="lr_step_after_epoch" class="doc_header"><code>lr_step_after_epoch</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L519" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>lr_step_after_epoch</code>(<strong><code>trainer</code></strong>, <strong><code>scheduler</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ReduceLROnPlateauWithScheduleWrapper" class="doc_header"><code>class</code> <code>ReduceLROnPlateauWithScheduleWrapper</code><a href="https://github.com/blackhc/batchbald_redux/tree/master/batchbald_redux/restoring_early_stopping.py#L548" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ReduceLROnPlateauWithScheduleWrapper</code>(<strong><code>optimizer</code></strong>, <strong><code>output_transform</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>metrics_transform</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>factor_schedule</code></strong>:<code>[&lt;class 'float'&gt;]</code>, <strong><code>patience_schedule</code></strong>:<code>[&lt;class 'int'&gt;]</code>, <strong><code>limit_schedule</code></strong>:<code>[&lt;class 'int'&gt;]</code>, <strong><code>next_era_callback</code></strong>:<code>Callable</code>, <strong><code>end_callback</code></strong>:<code>Callable</code>, <strong><code>mode</code></strong>=<em><code>'min'</code></em>, <strong><code>threshold</code></strong>=<em><code>0.0001</code></em>, <strong><code>threshold_mode</code></strong>=<em><code>'rel'</code></em>, <strong><code>cooldown</code></strong>=<em><code>0</code></em>, <strong><code>min_lr</code></strong>=<em><code>0</code></em>, <strong><code>eps</code></strong>=<em><code>1e-08</code></em>, <strong><code>verbose</code></strong>=<em><code>False</code></em>, <strong><code>module</code></strong>:<code>Module</code>=<em><code>None</code></em>) :: <a href="/batchbald_redux/restoring_early_stopping.html#ReduceLROnPlateauWithSchedule"><code>ReduceLROnPlateauWithSchedule</code></a></p>
</blockquote>
<p>Reduce learning rate when a metric has stopped improving.
Models often benefit from reducing the learning rate by a factor
of 2-10 once learning stagnates. This scheduler reads a metrics
quantity and if no improvement is seen for a 'patience' number
of epochs, the learning rate is reduced.</p>
<p>Args:
    optimizer (Optimizer): Wrapped optimizer.
    mode (str): One of <code>min</code>, <code>max</code>. In <code>min</code> mode, lr will
        be reduced when the quantity monitored has stopped
        decreasing; in <code>max</code> mode it will be reduced when the
        quantity monitored has stopped increasing. Default: 'min'.
    factor (float): Factor by which the learning rate will be
        reduced. new_lr = lr <em> factor. Default: 0.1.
    patience (int): Number of epochs with no improvement after
        which learning rate will be reduced. For example, if
        <code>patience = 2</code>, then we will ignore the first 2 epochs
        with no improvement, and will only decrease the LR after the
        3rd epoch if the loss still hasn't improved then.
        Default: 10.
    threshold (float): Threshold for measuring the new optimum,
        to only focus on significant changes. Default: 1e-4.
    threshold_mode (str): One of <code>rel</code>, <code>abs</code>. In <code>rel</code> mode,
        dynamic_threshold = best </em> ( 1 + threshold ) in 'max'
        mode or best * ( 1 - threshold ) in <code>min</code> mode.
        In <code>abs</code> mode, dynamic_threshold = best + threshold in
        <code>max</code> mode or best - threshold in <code>min</code> mode. Default: 'rel'.
    cooldown (int): Number of epochs to wait before resuming
        normal operation after lr has been reduced. Default: 0.
    min_lr (float or list): A scalar or a list of scalars. A
        lower bound on the learning rate of all param groups
        or each group respectively. Default: 0.
    eps (float): Minimal decay applied to lr. If the difference
        between new and old lr is smaller than eps, the update is
        ignored. Default: 1e-8.
    verbose (bool): If <code>True</code>, prints a message to stdout for
        each update. Default: <code>False</code>.</p>
<p>Example:</p>

<pre><code>&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
&gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')
&gt;&gt;&gt; for epoch in range(10):
&gt;&gt;&gt;     train(...)
&gt;&gt;&gt;     val_loss = validate(...)
&gt;&gt;&gt;     # Note that step should be called after validate()
&gt;&gt;&gt;     scheduler.step(val_loss)</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">ModuleOptimizerSnapshotter</span><span class="p">:</span>
    <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span>
    <span class="n">module_state_dict</span><span class="p">:</span> <span class="n">Optional</span>
    <span class="n">optimizer_state_dict</span><span class="p">:</span> <span class="n">Optional</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="o">=</span> <span class="n">module</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">module_state_dict</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_state_dict</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">snapshot</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="n">keep_vars</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>

            <span class="c1"># Only take a snapshot of the optimizer if we also have a module.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_state_dict</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">module_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">module</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">module_state_dict</span><span class="p">))</span>

            <span class="c1"># Only restore if we also restore a module (otherwise... messy?)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_state_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_state_dict</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">ReduceLROnPlateauWithSchedule</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reduce learning rate when a metric has stopped improving.</span>
<span class="sd">    Models often benefit from reducing the learning rate by a factor</span>
<span class="sd">    of 2-10 once learning stagnates. This scheduler reads a metrics</span>
<span class="sd">    quantity and if no improvement is seen for a &#39;patience&#39; number</span>
<span class="sd">    of epochs, the learning rate is reduced.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        mode (str): One of `min`, `max`. In `min` mode, lr will</span>
<span class="sd">            be reduced when the quantity monitored has stopped</span>
<span class="sd">            decreasing; in `max` mode it will be reduced when the</span>
<span class="sd">            quantity monitored has stopped increasing. Default: &#39;min&#39;.</span>
<span class="sd">        factor (float): Factor by which the learning rate will be</span>
<span class="sd">            reduced. new_lr = lr * factor. Default: 0.1.</span>
<span class="sd">        patience (int): Number of epochs with no improvement after</span>
<span class="sd">            which learning rate will be reduced. For example, if</span>
<span class="sd">            `patience = 2`, then we will ignore the first 2 epochs</span>
<span class="sd">            with no improvement, and will only decrease the LR after the</span>
<span class="sd">            3rd epoch if the loss still hasn&#39;t improved then.</span>
<span class="sd">            Default: 10.</span>
<span class="sd">        threshold (float): Threshold for measuring the new optimum,</span>
<span class="sd">            to only focus on significant changes. Default: 1e-4.</span>
<span class="sd">        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,</span>
<span class="sd">            dynamic_threshold = best * ( 1 + threshold ) in &#39;max&#39;</span>
<span class="sd">            mode or best * ( 1 - threshold ) in `min` mode.</span>
<span class="sd">            In `abs` mode, dynamic_threshold = best + threshold in</span>
<span class="sd">            `max` mode or best - threshold in `min` mode. Default: &#39;rel&#39;.</span>
<span class="sd">        cooldown (int): Number of epochs to wait before resuming</span>
<span class="sd">            normal operation after lr has been reduced. Default: 0.</span>
<span class="sd">        min_lr (float or list): A scalar or a list of scalars. A</span>
<span class="sd">            lower bound on the learning rate of all param groups</span>
<span class="sd">            or each group respectively. Default: 0.</span>
<span class="sd">        eps (float): Minimal decay applied to lr. If the difference</span>
<span class="sd">            between new and old lr is smaller than eps, the update is</span>
<span class="sd">            ignored. Default: 1e-8.</span>
<span class="sd">        verbose (bool): If ``True``, prints a message to stdout for</span>
<span class="sd">            each update. Default: ``False``.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, &#39;min&#39;)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     train(...)</span>
<span class="sd">        &gt;&gt;&gt;     val_loss = validate(...)</span>
<span class="sd">        &gt;&gt;&gt;     # Note that step should be called after validate()</span>
<span class="sd">        &gt;&gt;&gt;     scheduler.step(val_loss)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">factor_schedule</span><span class="p">:</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">patience_schedule</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">limit_schedule</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">end_callback</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">next_era_callback</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">threshold_mode</span><span class="o">=</span><span class="s2">&quot;rel&quot;</span><span class="p">,</span>
        <span class="n">cooldown</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">min_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">factor</span> <span class="o">&gt;=</span> <span class="mf">1.0</span> <span class="k">for</span> <span class="n">factor</span> <span class="ow">in</span> <span class="n">factor_schedule</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Factor should be &lt; 1.0.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor_schedule</span> <span class="o">=</span> <span class="n">factor_schedule</span>

        <span class="c1"># Attach optimizer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not an Optimizer&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">snapshotter</span> <span class="o">=</span> <span class="n">ModuleOptimizerSnapshotter</span><span class="p">(</span><span class="n">module</span><span class="o">=</span><span class="n">module</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_lr</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">min_lr</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> min_lrs, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">min_lr</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_lr</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">era</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience_schedule</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">factor_schedule</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">limit_schedule</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">end_callback</span> <span class="o">=</span> <span class="n">end_callback</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">next_era_callback</span> <span class="o">=</span> <span class="n">next_era_callback</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patience_schedule</span> <span class="o">=</span> <span class="n">patience_schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit_schedule</span> <span class="o">=</span> <span class="n">limit_schedule</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span> <span class="o">=</span> <span class="n">cooldown</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">=</span> <span class="n">threshold_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># the worse value for the chosen mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="n">threshold_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Resets num_bad_epochs counter and cooldown counter.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># convert `metrics` to float, in case it&#39;s a zero-dim Tensor</span>
        <span class="n">current</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">EPOCH_DEPRECATION_WARNING</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">=</span> <span class="n">epoch</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_better</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best</span> <span class="o">=</span> <span class="n">current</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span> <span class="o">=</span> <span class="n">epoch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">snapshotter</span><span class="o">.</span><span class="n">snapshot</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">current</span><span class="si">}</span><span class="s2"> worse than </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="si">}</span><span class="s2">, patience: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="si">}</span><span class="s2">!&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_cooldown</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># ignore any bad epochs in cooldown</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">snapshotter</span><span class="o">.</span><span class="n">module</span><span class="p">:</span>
              <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Restoring best snapshot: best_score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">best</span><span class="si">}</span><span class="s2"> @ epoch </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">best_epoch</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
              <span class="bp">self</span><span class="o">.</span><span class="n">snapshotter</span><span class="o">.</span><span class="n">restore</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_lr</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_bad_epochs</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_last_lr</span> <span class="o">=</span> <span class="p">[</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_reduce_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">era</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">era</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">patience_schedule</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_callback</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">end_callback</span><span class="p">()</span>
            <span class="k">return</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience_schedule</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">era</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit_schedule</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">era</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">next_era_callback</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">next_era_callback</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">old_lr</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lrs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">old_lr</span> <span class="o">-</span> <span class="n">new_lr</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">{:5d}</span><span class="s2">: reducing learning rate&quot;</span> <span class="s2">&quot; of group </span><span class="si">{}</span><span class="s2"> to </span><span class="si">{:.4e}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">era</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">factor_schedule</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">factor_schedule</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">era</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">in_cooldown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cooldown_counter</span> <span class="o">&gt;</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">is_better</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">best</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">==</span> <span class="s2">&quot;rel&quot;</span><span class="p">:</span>
            <span class="n">rel_epsilon</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">best</span> <span class="o">*</span> <span class="n">rel_epsilon</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">==</span> <span class="s2">&quot;abs&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">best</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">==</span> <span class="s2">&quot;rel&quot;</span><span class="p">:</span>
            <span class="n">rel_epsilon</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">+</span> <span class="mf">1.0</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">best</span> <span class="o">*</span> <span class="n">rel_epsilon</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># mode == &#39;max&#39; and epsilon_mode == &#39;abs&#39;:</span>
            <span class="k">return</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">best</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span>

    <span class="k">def</span> <span class="nf">_init_is_better</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode &quot;</span> <span class="o">+</span> <span class="n">mode</span> <span class="o">+</span> <span class="s2">&quot; is unknown!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">threshold_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;rel&quot;</span><span class="p">,</span> <span class="s2">&quot;abs&quot;</span><span class="p">}:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;threshold mode &quot;</span> <span class="o">+</span> <span class="n">threshold_mode</span> <span class="o">+</span> <span class="s2">&quot; is unknown!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span> <span class="o">=</span> <span class="n">inf</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># mode == &#39;max&#39;:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mode_worse</span> <span class="o">=</span> <span class="o">-</span><span class="n">inf</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span> <span class="o">=</span> <span class="n">threshold_mode</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">value</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="o">!=</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_is_better</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold</span><span class="p">,</span> <span class="n">threshold_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">threshold_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">lr_step_after_epoch</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">ReduceLROnPlateauWithScheduleWrapper</span><span class="p">):</span>

        <span class="nd">@trainer</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">EPOCH_COMPLETED</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">lr_step</span><span class="p">(</span><span class="n">engine</span><span class="p">):</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">engine</span><span class="p">)</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">ReduceLROnPlateauWithSchedule</span><span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;Use ReduceLROnPlateauWrapper instead of torch.optim.lr_scheduler.ReduceLROnPlateau!&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span>
        <span class="p">)</span>

        <span class="nd">@trainer</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">EPOCH_COMPLETED</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">lr_step</span><span class="p">(</span><span class="n">engine</span><span class="p">):</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingWarmRestarts</span><span class="p">):</span>

        <span class="nd">@trainer</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">GET_BATCH_COMPLETED</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">lr_step</span><span class="p">(</span><span class="n">engine</span><span class="p">):</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="n">engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">iteration</span> <span class="o">/</span> <span class="n">engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">epoch_length</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="nd">@trainer</span><span class="o">.</span><span class="n">on</span><span class="p">(</span><span class="n">Events</span><span class="o">.</span><span class="n">EPOCH_COMPLETED</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">lr_step</span><span class="p">(</span><span class="n">engine</span><span class="p">):</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ReduceLROnPlateauWithScheduleWrapper</span><span class="p">(</span><span class="n">ReduceLROnPlateauWithSchedule</span><span class="p">):</span>
    <span class="n">output_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span>
    <span class="n">metrics_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">output_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">metrics_transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">factor_schedule</span><span class="p">:</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">patience_schedule</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">limit_schedule</span><span class="p">:</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span>
        <span class="n">next_era_callback</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">end_callback</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">threshold</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
        <span class="n">threshold_mode</span><span class="o">=</span><span class="s2">&quot;rel&quot;</span><span class="p">,</span>
        <span class="n">cooldown</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">min_lr</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">factor_schedule</span><span class="o">=</span><span class="n">factor_schedule</span><span class="p">,</span>
            <span class="n">patience_schedule</span><span class="o">=</span><span class="n">patience_schedule</span><span class="p">,</span>
            <span class="n">limit_schedule</span><span class="o">=</span><span class="n">limit_schedule</span><span class="p">,</span>
            <span class="n">end_callback</span><span class="o">=</span><span class="n">end_callback</span><span class="p">,</span>
            <span class="n">next_era_callback</span><span class="o">=</span><span class="n">next_era_callback</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
            <span class="n">threshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">,</span>
            <span class="n">threshold_mode</span><span class="o">=</span><span class="n">threshold_mode</span><span class="p">,</span>
            <span class="n">cooldown</span><span class="o">=</span><span class="n">cooldown</span><span class="p">,</span>
            <span class="n">min_lr</span><span class="o">=</span><span class="n">min_lr</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="n">module</span><span class="o">=</span><span class="n">module</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics_transform</span> <span class="o">=</span> <span class="n">metrics_transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_transform</span> <span class="o">=</span> <span class="n">output_transform</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_transform</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_transform</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_transform</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_transform</span>

    <span class="c1"># noinspection PyMethodOverriding</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">engine</span><span class="p">:</span> <span class="n">Engine</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_transform</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_transform</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">output</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics_transform</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics_transform</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">metrics</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

