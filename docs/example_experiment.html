---

title: Example Experiment


keywords: fastai
sidebar: home_sidebar

summary: "Experiment using Repeated MNIST and BatchBALD vs BALD vs random sampling"
description: "Experiment using Repeated MNIST and BatchBALD vs BALD vs random sampling"
nb_path: "06_example_experiment.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 06_example_experiment.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This notebook ties everything together and runs an AL loop.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">blackhc.project.script</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Appended /home/blackhc/PycharmProjects/blackhc.batchbald/src to paths
Switched to directory /home/blackhc/PycharmProjects/blackhc.batchbald
%load_ext autoreload
%autoreload 2
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="nn">batchbald_redux</span> <span class="kn">import</span> <span class="n">active_learning</span><span class="p">,</span> <span class="n">batchbald</span><span class="p">,</span> <span class="n">consistent_mc_dropout</span><span class="p">,</span> <span class="n">joint_entropy</span><span class="p">,</span> <span class="n">repeated_mnist</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's define our Bayesian CNN model that we will use to train MNIST.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">BayesianCNN</span><span class="p">(</span><span class="n">consistent_mc_dropout</span><span class="o">.</span><span class="n">BayesianModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1_drop</span> <span class="o">=</span> <span class="n">consistent_mc_dropout</span><span class="o">.</span><span class="n">ConsistentMCDropout2d</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2_drop</span> <span class="o">=</span> <span class="n">consistent_mc_dropout</span><span class="o">.</span><span class="n">ConsistentMCDropout2d</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1_drop</span> <span class="o">=</span> <span class="n">consistent_mc_dropout</span><span class="o">.</span><span class="n">ConsistentMCDropout</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">mc_forward_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="nb">input</span><span class="p">)),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="nb">input</span><span class="p">)),</span> <span class="mi">2</span><span class="p">))</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1_drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="nb">input</span><span class="p">)))</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">input</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Grab our dataset, we'll use Repeated-MNIST. We will acquire to samples for each class for our initial training set.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">repeated_mnist</span><span class="o">.</span><span class="n">create_repeated_MNIST_dataset</span><span class="p">(</span><span class="n">num_repetitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">add_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">num_initial_samples</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">initial_samples</span> <span class="o">=</span> <span class="n">active_learning</span><span class="o">.</span><span class="n">get_balanced_sample_indices</span><span class="p">(</span>
    <span class="n">repeated_mnist</span><span class="o">.</span><span class="n">get_targets</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
    <span class="n">n_per_digit</span><span class="o">=</span><span class="n">num_initial_samples</span><span class="o">/</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this example, we are going to take two shortcuts that will reduce the performance:</p>
<ul>
<li>we discard most of the training set and only keep 20k samples; and</li>
<li>we don't implement early stopping or epoch-wise training.</li>
</ul>
<p>Instead, we always train by drawing 24576 many samples from the training set. This will overfit in the beginning and underfit later, but it still is sufficient to achieve 90% accuracy with 105 samples in the training set.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_training_samples</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">acquisition_batch_size</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_inference_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_test_inference_samples</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="n">test_batch_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">scoring_batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">training_iterations</span> <span class="o">=</span> <span class="mi">4096</span> <span class="o">*</span> <span class="mi">6</span>

<span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;use_cuda: </span><span class="si">{</span><span class="n">use_cuda</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>

<span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;num_workers&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="p">{}</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">test_batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">active_learning_data</span> <span class="o">=</span> <span class="n">active_learning</span><span class="o">.</span><span class="n">ActiveLearningData</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>

<span class="c1"># Split off the initial samples first.</span>
<span class="n">active_learning_data</span><span class="o">.</span><span class="n">acquire</span><span class="p">(</span><span class="n">initial_samples</span><span class="p">)</span>

<span class="c1"># THIS REMOVES MOST OF THE POOL DATA. UNCOMMENT THIS TO TAKE ALL UNLABELLED DATA INTO ACCOUNT!</span>
<span class="n">active_learning_data</span><span class="o">.</span><span class="n">extract_dataset_from_pool</span><span class="p">(</span><span class="mi">40000</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">active_learning_data</span><span class="o">.</span><span class="n">training_dataset</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">active_learning</span><span class="o">.</span><span class="n">RandomFixedLengthSampler</span><span class="p">(</span>
        <span class="n">active_learning_data</span><span class="o">.</span><span class="n">training_dataset</span><span class="p">,</span> <span class="n">training_iterations</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">pool_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">active_learning_data</span><span class="o">.</span><span class="n">pool_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">scoring_batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="c1"># Run experiment</span>
<span class="n">test_accs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">added_indices</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">initial</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">active_learning_data</span><span class="o">.</span><span class="n">training_dataset</span><span class="p">),</span>
            <span class="n">total</span><span class="o">=</span><span class="n">max_training_samples</span><span class="p">,</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Training Set Size&quot;</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">BayesianCNN</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># Train</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Test</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Testing&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span>
                <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">num_test_inference_samples</span><span class="p">),</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">num_test_inference_samples</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>

            <span class="n">prediction</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">prediction</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">test_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="n">percentage_correct</span> <span class="o">=</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">test_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">percentage_correct</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set: Average loss: </span><span class="si">{:.4f}</span><span class="s2">, Accuracy: </span><span class="si">{}</span><span class="s2">/</span><span class="si">{}</span><span class="s2"> (</span><span class="si">{:.2f}</span><span class="s2">%)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">),</span> <span class="n">percentage_correct</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_learning_data</span><span class="o">.</span><span class="n">training_dataset</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">max_training_samples</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="c1"># Acquire pool predictions</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">active_learning_data</span><span class="o">.</span><span class="n">pool_dataset</span><span class="p">)</span>
    <span class="n">logits_N_K_C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">num_inference_samples</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span>
                               <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span>
                               <span class="n">pin_memory</span><span class="o">=</span><span class="n">use_cuda</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="n">tqdm</span><span class="p">(</span><span class="n">pool_loader</span><span class="p">,</span>
                     <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Evaluating Acquisition Set&quot;</span><span class="p">,</span>
                     <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="n">lower</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">pool_loader</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">upper</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">lower</span> <span class="o">+</span> <span class="n">pool_loader</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
            <span class="n">logits_N_K_C</span><span class="p">[</span><span class="n">lower</span><span class="p">:</span><span class="n">upper</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">model</span><span class="p">(</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">num_inference_samples</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">(),</span>
                                            <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">candidate_batch</span> <span class="o">=</span> <span class="n">batchbald</span><span class="o">.</span><span class="n">get_batchbald_batch</span><span class="p">(</span><span class="n">logits_N_K_C</span><span class="o">.</span><span class="n">exp_</span><span class="p">(),</span>
                                                        <span class="n">acquisition_batch_size</span><span class="p">,</span>
                                                        <span class="n">num_samples</span><span class="p">,</span>
                                                        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span>
                                                        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="n">targets</span> <span class="o">=</span> <span class="n">repeated_mnist</span><span class="o">.</span><span class="n">get_targets</span><span class="p">(</span><span class="n">active_learning_data</span><span class="o">.</span><span class="n">pool_dataset</span><span class="p">)</span>
    <span class="n">dataset_indices</span> <span class="o">=</span> <span class="n">active_learning_data</span><span class="o">.</span><span class="n">get_dataset_indices</span><span class="p">(</span>
        <span class="n">candidate_batch</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dataset indices: &quot;</span><span class="p">,</span> <span class="n">dataset_indices</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Scores: &quot;</span><span class="p">,</span> <span class="n">candidate_batch</span><span class="o">.</span><span class="n">scores</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Labels: &quot;</span><span class="p">,</span> <span class="n">targets</span><span class="p">[</span><span class="n">candidate_batch</span><span class="o">.</span><span class="n">indices</span><span class="p">])</span>

    <span class="n">active_learning_data</span><span class="o">.</span><span class="n">acquire</span><span class="p">(</span><span class="n">candidate_batch</span><span class="o">.</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">added_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataset_indices</span><span class="p">)</span>
    <span class="n">pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset_indices</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>use_cuda: True
Test set: Average loss: 1.8308, Accuracy: 6338/10000 (63.38%)
Dataset indices:  [ 8289  3582 53863 25823  8257]
Scores:  [1.3557736757796552, 2.519825665002863, 3.4085050062563003, 3.9695525038521966, 4.2919329737575325]
Labels:  tensor([0, 2, 3, 0, 2])
Test set: Average loss: 1.4175, Accuracy: 6985/10000 (69.85%)
Dataset indices:  [52012 41383 13682 42198  6185]
Scores:  [1.3061743472090992, 2.3644483581086218, 3.221506615442796, 3.8194846029077567, 4.183660860241003]
Labels:  tensor([8, 0, 8, 4, 3])
Test set: Average loss: 1.2743, Accuracy: 7269/10000 (72.69%)
Dataset indices:  [11657 37137 14866 28222 34614]
Scores:  [1.2659351842478004, 2.3131665302748754, 3.202945514749244, 3.8116075718264586, 4.159970164562334]
Labels:  tensor([0, 5, 7, 6, 2])
Test set: Average loss: 1.1519, Accuracy: 7435/10000 (74.35%)
Dataset indices:  [39411 13642 19396  8488 16077]
Scores:  [1.3163655603620135, 2.405610005940731, 3.1817393501560343, 3.758214079939448, 4.122080265207265]
Labels:  tensor([2, 5, 5, 6, 6])
Test set: Average loss: 0.9635, Accuracy: 7809/10000 (78.09%)
Dataset indices:  [40057  4606 55743 26444 37870]
Scores:  [1.209489071500093, 2.283754189343815, 3.102376195203646, 3.6698826219743843, 4.038958571081122]
Labels:  tensor([5, 9, 3, 1, 8])
Test set: Average loss: 0.7743, Accuracy: 8067/10000 (80.67%)
Dataset indices:  [ 2748 25910 24223 32954 20110]
Scores:  [1.123391331671529, 2.140858614880564, 3.012231409824741, 3.66999233531654, 4.095480093557085]
Labels:  tensor([2, 1, 8, 5, 4])
Test set: Average loss: 0.6680, Accuracy: 8267/10000 (82.67%)
Dataset indices:  [56615 37249 50461   384 32509]
Scores:  [1.2427624719236614, 2.2830006810904986, 3.1605394537163587, 3.7497607635339576, 4.128859502902091]
Labels:  tensor([3, 5, 7, 7, 8])
Test set: Average loss: 0.6514, Accuracy: 8397/10000 (83.97%)
Dataset indices:  [47695 30925 50010  3916 45114]
Scores:  [1.190816694089635, 2.2611525375451773, 3.0947280938353208, 3.7115958775092626, 4.084116729465448]
Labels:  tensor([4, 2, 5, 7, 7])
Test set: Average loss: 0.5701, Accuracy: 8386/10000 (83.86%)
Dataset indices:  [38760 54950  6578 52959 44590]
Scores:  [1.1627632250527844, 2.151829399286887, 2.9537553798530665, 3.559596661670692, 3.9736198748141387]
Labels:  tensor([9, 8, 6, 2, 7])
Test set: Average loss: 0.5139, Accuracy: 8582/10000 (85.82%)
Dataset indices:  [12281 20820 20859 29132 52972]
Scores:  [1.1276134714636667, 2.1698819851983324, 3.0257048656424925, 3.6544162493463603, 4.056693027547438]
Labels:  tensor([2, 9, 8, 8, 3])
Test set: Average loss: 0.5993, Accuracy: 8367/10000 (83.67%)
Dataset indices:  [49202 59080 19590 14699 11295]
Scores:  [1.0977984880766969, 2.0814510505992336, 2.908458520545308, 3.517485210170112, 3.926052066453105]
Labels:  tensor([5, 9, 5, 9, 0])
Test set: Average loss: 0.5477, Accuracy: 8621/10000 (86.21%)
Dataset indices:  [17756 42467 49580 34902 14705]
Scores:  [1.2460383110021542, 2.2770103994311666, 3.181935528898873, 3.7992051764099135, 4.157236509734414]
Labels:  tensor([8, 8, 2, 2, 0])
Test set: Average loss: 0.6217, Accuracy: 8466/10000 (84.66%)
Dataset indices:  [28860 17010  5217 47426 25117]
Scores:  [1.1420065931224535, 2.163338791178303, 3.0353145026285113, 3.689406154914269, 4.095223498725142]
Labels:  tensor([4, 3, 4, 3, 8])
Test set: Average loss: 0.4452, Accuracy: 8754/10000 (87.54%)
Dataset indices:  [59674 40066 29303 30878 43852]
Scores:  [1.0879233508208421, 2.083260109080535, 2.9151471744333244, 3.5294256346467683, 3.9514164379630943]
Labels:  tensor([4, 4, 8, 6, 2])
Test set: Average loss: 0.4564, Accuracy: 8763/10000 (87.63%)
Dataset indices:  [59314 53260 52697 27356  8668]
Scores:  [1.0931448747101902, 2.09011830748354, 2.9307625527521264, 3.570223588524582, 4.0151883704296445]
Labels:  tensor([9, 4, 3, 2, 5])
Test set: Average loss: 0.4157, Accuracy: 8803/10000 (88.03%)
Dataset indices:  [24440 59390 18412 32178  5852]
Scores:  [1.0443426003599554, 2.0138641259236483, 2.86321617007789, 3.505478145903256, 3.9422873447609907]
Labels:  tensor([0, 2, 0, 4, 3])
Test set: Average loss: 0.3696, Accuracy: 8889/10000 (88.89%)
Dataset indices:  [31184 47652  5129 28731 51261]
Scores:  [1.0962478190100569, 2.085744839210468, 2.9357022931217345, 3.5651083882428876, 3.9906569316999683]
Labels:  tensor([9, 2, 2, 2, 4])
Test set: Average loss: 0.3524, Accuracy: 9009/10000 (90.09%)
Dataset indices:  [39151 25387 31637 56391 22675]
Scores:  [1.0255197078227165, 1.9606027634150682, 2.749715548469969, 3.369155913082033, 3.815295016054319]
Labels:  tensor([4, 8, 5, 3, 3])
Test set: Average loss: 0.3498, Accuracy: 8994/10000 (89.94%)
Dataset indices:  [40766 35638  7768 21390 13021]
Scores:  [1.1270052748144128, 2.1798424784191712, 3.040212246578937, 3.635184434645347, 4.036560647852484]
Labels:  tensor([4, 2, 8, 3, 5])
Test set: Average loss: 0.3283, Accuracy: 9124/10000 (91.24%)
Dataset indices:  [44202 26034  2765 36818 29489]
Scores:  [1.0196356716784996, 1.9834245958639172, 2.7933227652841905, 3.41448235566454, 3.85158791628092]
Labels:  tensor([8, 5, 0, 7, 6])
Test set: Average loss: 0.2814, Accuracy: 9234/10000 (92.34%)
Dataset indices:  [37293 18398 33126 43126 30454]
Scores:  [1.1300931680857567, 2.1483018935070217, 2.992901494349929, 3.5947497233707892, 3.9926906528787685]
Labels:  tensor([3, 4, 9, 3, 6])
Test set: Average loss: 0.2732, Accuracy: 9264/10000 (92.64%)
Dataset indices:  [23086 37414  3070  8214  3218]
Scores:  [1.054147318045934, 2.0399407506273706, 2.8545137655874995, 3.479438083830503, 3.9201075658821574]
Labels:  tensor([8, 5, 1, 7, 4])
Test set: Average loss: 0.2742, Accuracy: 9282/10000 (92.82%)
Dataset indices:  [35864 29899 37655 24620  7923]
Scores:  [1.0425523144612032, 1.9722955492461425, 2.769826348292811, 3.409673106042364, 3.855570195163713]
Labels:  tensor([5, 3, 2, 9, 8])
Test set: Average loss: 0.2854, Accuracy: 9261/10000 (92.61%)
Dataset indices:  [43206  5315 38509 40466 28512]
Scores:  [1.0095551073232742, 1.9773855734476835, 2.7935712557665253, 3.4312428375675297, 3.859673478664067]
Labels:  tensor([5, 3, 2, 8, 5])
Test set: Average loss: 0.3089, Accuracy: 9146/10000 (91.46%)
Dataset indices:  [34946 34520 49523 53873 33812]
Scores:  [1.0336427348167723, 1.9970519915069573, 2.797597657927694, 3.42035327890408, 3.8533839231792015]
Labels:  tensor([8, 6, 7, 4, 6])
Test set: Average loss: 0.3097, Accuracy: 9186/10000 (91.86%)
Dataset indices:  [44898  2761 57882 50317 41453]
Scores:  [0.9905177855898044, 1.9297613872862898, 2.7047249345431563, 3.3479105773782156, 3.7841506750832803]
Labels:  tensor([2, 8, 0, 3, 3])
Test set: Average loss: 0.2967, Accuracy: 9241/10000 (92.41%)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_initial_samples</span><span class="p">,</span> <span class="n">max_training_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">acquisition_batch_size</span><span class="p">),</span> <span class="n">test_accs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;# training samples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span><span class="mi">150</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

