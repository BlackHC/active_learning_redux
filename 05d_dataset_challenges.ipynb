{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Challenges\n",
    "> “Whoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you.”\n",
    ">\n",
    "> ― Friedrich Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model real-world use cases better, we need:\n",
    "* redundant/duplicated data;\n",
    "* noisy labels (emulating noisy oracles);\n",
    "* class imbalance;\n",
    "* out-of-distribution data/outliers included in the unlabelled data;\n",
    "* noisy or ambiguous samples.\n",
    "\n",
    "RepeatedMNIST takes care of the first and last challenge in a very specific way. \n",
    "This chapter takes care of the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import bisect\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class _AliasDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset with an easier to understand alias.\n",
    "    \n",
    "    And convenience operators.\n",
    "    \"\"\"\n",
    "    dataset: data.Dataset\n",
    "    alias: str\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, alias: str):\n",
    "        self.dataset = dataset\n",
    "        self.alias = alias\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.alias\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return _AliasDataset(data.ConcatDataset([self, other]), f\"{self} + {other}\")\n",
    "    \n",
    "    def __mul__(self, factor):\n",
    "        return SubsetDataset(self, factor=factor, seed=0)\n",
    "    \n",
    "    def __rmul__(self, factor):\n",
    "        return SubsetDataset(self, factor=factor, seed=0)\n",
    "\n",
    "\n",
    "class NamedDataset(_AliasDataset):\n",
    "    def __init__(self, dataset: data.Dataset, name: str):\n",
    "        super().__init__(dataset, name)\n",
    "\n",
    "\n",
    "def get_base_dataset(dataset, index):\n",
    "    if isinstance(dataset, NamedDataset):\n",
    "        return dataset\n",
    "    elif isinstance(dataset, data.ConcatDataset):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return get_base_dataset(dataset.datasets[dataset_idx], sample_idx)\n",
    "    elif isinstance(dataset, _AliasDataset):\n",
    "        return get_base_dataset(dataset.dataset, index)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTDataset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "\n",
    "MNIST = NamedDataset(FastMNIST(root=\"data/\", download=True, device=\"cpu\"), name=\"MNISTDataset\")\n",
    "\n",
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class OverridenTargetDataset(_AliasDataset):\n",
    "    indices_set: set\n",
    "    reverse_indices: dict\n",
    "    new_targets: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, indices: list, new_targets: list):\n",
    "        self.indices_set = set(indices)\n",
    "        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}\n",
    "        self.new_targets = new_targets\n",
    "        \n",
    "        super().__init__(dataset, f\"OverridenTargetDataset({var(self)})\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if idx not in self.indices_set:\n",
    "            return data, label\n",
    "\n",
    "        ridx = self.reverse_indices[idx]\n",
    "        new_y = self.new_targets[ridx]\n",
    "        return data, new_y\n",
    "\n",
    "\n",
    "class CorruptedLabelsDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    implementation: OverridenTargetDataset\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset: data.Dataset, *, size_corrupted: Union[float, int], num_classes: int, seed: int, device=None\n",
    "    ):\n",
    "        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"CorruptedLabelsDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        N = len(dataset)\n",
    "\n",
    "        if size_corrupted > 1:\n",
    "            num_corrupted = size_corrupted\n",
    "        else:\n",
    "            num_corrupted = int(N * size_corrupted)\n",
    "\n",
    "        indices = generator.choice(N, size=num_corrupted, replace=False)\n",
    "        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)\n",
    "\n",
    "        self.implementation = OverridenTargetDataset(\n",
    "            dataset, indices=indices, new_targets=torch.as_tensor(new_targets, device=device)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.implementation[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.implementation)\n",
    "\n",
    "\n",
    "class RandomLabelsDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    new_labels: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, seed: int, device=None):\n",
    "        options = dict(num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"RandomLabelsDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        N = len(dataset)\n",
    "\n",
    "        self.new_labels = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.new_labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CorruptedLabelsDataset(dataset=ZeroDataset, {'size_corrupted': 0.5, 'num_classes': 10, 'seed': 1})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_dataset = NamedDataset(data.TensorDataset(torch.zeros(10), torch.zeros(10)), \"ZeroDataset\")\n",
    "\n",
    "corrupted_labels_dataset = CorruptedLabelsDataset(zero_dataset, size_corrupted=0.5, num_classes=10, seed=1)\n",
    "\n",
    "assert list(corrupted_labels_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "]\n",
    "\n",
    "corrupted_labels_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomLabelsDataset(ZeroDataset, {'num_classes': 10, 'seed': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_dataset = RandomLabelsDataset(zero_dataset, num_classes=10, seed=2)\n",
    "\n",
    "assert list(corrupted_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(1)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(6)),\n",
    "]\n",
    "\n",
    "corrupted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator):\n",
    "    class_counts = list(class_counts)\n",
    "\n",
    "    subset_indices = []\n",
    "\n",
    "    remaining_samples = sum(class_counts)\n",
    "\n",
    "    indices = generator.permutation(len(dataset))\n",
    "    for index in indices:\n",
    "        _, y = dataset[index]\n",
    "\n",
    "        if class_counts[y] > 0:\n",
    "            subset_indices.append(index)\n",
    "            class_counts[y] -= 1\n",
    "            remaining_samples -= 1\n",
    "\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "\n",
    "    return subset_indices\n",
    "\n",
    "\n",
    "class ImbalancedDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):\n",
    "        options = dict(class_counts=class_counts, seed=seed)\n",
    "        super().__init__(dataset, f\"ImbalancedDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class ImbalancedClassSplitDataset(_AliasDataset):\n",
    "    dataset: data.Dataset\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, majority_percentage: int, minority_percentage: int, seed: int):\n",
    "        assert (num_classes % 2) == 0\n",
    "\n",
    "        super().__init__(dataset, None)\n",
    "\n",
    "        num_samples_per_class = len(dataset) // num_classes\n",
    "        num_samples_majority = num_samples_per_class * majority_percentage // 100\n",
    "        num_samples_minority = num_samples_per_class * minority_percentage // 100\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        class_counts = [num_samples_majority] * (num_classes // 2) + [num_samples_minority] * (num_classes // 2)\n",
    "        class_counts = generator.permuted(class_counts)\n",
    "\n",
    "        self.options = dict(\n",
    "            num_classes=num_classes, majority_percentage=majority_percentage, seed=seed, class_counts=class_counts\n",
    "        )\n",
    "        self.alias = f\"ImbalancedDataset(dataset={self.dataset}, {self.options})\"\n",
    "\n",
    "        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dataset = NamedDataset(data.TensorDataset(torch.arange(9), torch.as_tensor(list(range(3)) * 3)), \"123\")\n",
    "\n",
    "imbalanced_indices = get_class_indices(three_dataset, class_counts=[3, 0, 0], generator=np.random.default_rng())\n",
    "\n",
    "three_dataset[imbalanced_indices][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 2, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset=123, {'class_counts': [1, 2, 3], 'seed': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbalanced_dataset = ImbalancedDataset(three_dataset, class_counts=[1, 2, 3], seed=2)\n",
    "print(imbalanced_dataset[:][1])\n",
    "imbalanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset=MNISTDataset, {'num_classes': 10, 'majority_percentage': 80, 'seed': 1, 'class_counts': array([1200, 4800, 1200, 4800, 4800, 4800, 1200, 1200, 1200, 4800])})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImbalancedClassSplitDataset(MNIST, num_classes=10, majority_percentage=80, minority_percentage=20, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing in OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "# Convert label dataset to one hot\n",
    "class OneHotDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    targets: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):\n",
    "        options = dict(num_classes=num_classes)\n",
    "\n",
    "        super().__init__(dataset, f\"OneHotDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        N = len(dataset)\n",
    "        targets = torch.zeros(len(dataset), num_classes, dtype=dtype, device=device)\n",
    "        for i, (_, label) in enumerate(dataset):\n",
    "            targets[i, label] = 1.0\n",
    "\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.targets[idx]\n",
    "\n",
    "\n",
    "class SubsetDataset(_AliasDataset):\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):\n",
    "        options = dict(size=size, factor=factor, seed=seed)\n",
    "        super().__init__(dataset, f\"SubsetDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)\n",
    "        if size is not None:\n",
    "            subset_size = size\n",
    "            if seed == 0:\n",
    "                self.alias = f\"~{dataset}[:{size}]\"\n",
    "        elif factor is not None:\n",
    "            subset_size = int(len(dataset) * factor)\n",
    "            if seed == 0:\n",
    "                self.alias = f\"~{dataset} * {factor}\"\n",
    "\n",
    "        self.indices = generator.choice(len(dataset), size=subset_size, replace=subset_size > len(dataset))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class ConstantTargetDataset(_AliasDataset):\n",
    "    target: object\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, target: object):\n",
    "        super().__init__(dataset, f\"ConstantTargetDataset(dataset={dataset}, {target})\")\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.target\n",
    "\n",
    "\n",
    "def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int, device: str = None):\n",
    "    target = torch.ones(num_classes, device=device) / num_classes\n",
    "    result = ConstantTargetDataset(dataset, target)\n",
    "    result.options = dict(num_classes=num_classes)\n",
    "    result.alias = f\"UniformTargetDataset({dataset}, {result.options})\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To an OOD dataset, one can either use:\n",
    "```\n",
    "MNIST+OOD*0.5\n",
    "```\n",
    "and then use `get_base_dataset(dataset, index) == OOD` to check whether a picked sample is OOD.\n",
    "\n",
    "Alternatively, we can use:\n",
    "```\n",
    "OneHotDataset(MNIST) + UniformTargetDataset(OOD * 0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTDataset * 0.1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotDataset(MNISTDataset * 0.1, {'num_classes': 10})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_MNIST = OneHotDataset(MNIST * 0.1, num_classes=10)\n",
    "print(one_hot_MNIST[0][1])\n",
    "\n",
    "one_hot_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniformTargetDataset(OneHotDataset(MNISTDataset * 0.1, {'num_classes': 10}), {'num_classes': 10})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniformTargetDataset(one_hot_MNIST, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that for large datasets, precomputing the noise to be added can use up a lot of memory (doubling the dataset size). Creating a new random generator for each sample is too slow, so it might be worth creating an entirely new dataset.\n",
    "\n",
    "However, we support exporting and importing datasets, so simply storing and loading the dataset is an option.\n",
    "\n",
    "> Tip: Do not use this on very large datasets (e.g. ImageNet)... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class AdditiveGaussianNoise(_AliasDataset):\n",
    "    noise: torch.Tensor\n",
    "    options: dict\n",
    "        \n",
    "    def __init__(self, dataset: data.Dataset, sigma: float):\n",
    "        sample = dataset[0][0]\n",
    "        self.noise = torch.randn(len(dataset), *sample.shape, device=sample.device)\n",
    "        self.options = dict(sigma=sigma)\n",
    "        \n",
    "        super().__init__(dataset, f\"AdditiveGaussianNoise(dataset={dataset}, {self.options})\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample, target = self.dataset[idx]\n",
    "        return sample + self.noise[idx], target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdditiveGaussianNoise(dataset=ZeroDataset, {'sigma': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(0.4824), tensor(0.)),\n",
       " (tensor(-0.9370), tensor(0.)),\n",
       " (tensor(0.0229), tensor(0.)),\n",
       " (tensor(0.2592), tensor(0.)),\n",
       " (tensor(1.4626), tensor(0.)),\n",
       " (tensor(0.6883), tensor(0.)),\n",
       " (tensor(0.0902), tensor(0.)),\n",
       " (tensor(0.8162), tensor(0.)),\n",
       " (tensor(0.5490), tensor(0.)),\n",
       " (tensor(-0.2185), tensor(0.))]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_zero = AdditiveGaussianNoise(zero_dataset, sigma=1)\n",
    "\n",
    "print(noisy_zero)\n",
    "\n",
    "list(noisy_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Datasets\n",
    "\n",
    "Finally, to make it easier to use datasets across Python versions, we allow dataset exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def dataset_to_tensors(dataset):\n",
    "    samples = []\n",
    "    targets = []\n",
    "        \n",
    "    for sample, target in dataset:\n",
    "        samples.append(sample.to(device=\"cpu\", non_blocking=True))\n",
    "        targets.append(target.to(device=\"cpu\", non_blocking=True))\n",
    "    \n",
    "    samples = torch.stack(samples)\n",
    "    targets = torch.stack(targets)\n",
    "    \n",
    "    return samples, targets\n",
    "        \n",
    "        \n",
    "def get_dataset_state_dict(dataset):\n",
    "    dataset_alias = repr(dataset)\n",
    "    \n",
    "    samples, targets = dataset_to_tensors(dataset)\n",
    "    \n",
    "    state_dict = dict(alias=dataset_alias, samples=samples,targets=targets)\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "\n",
    "class ImportedDataset(_AliasDataset):        \n",
    "    def __init__(self, state_dict, device=None):\n",
    "        tensor_dataset = data.TensorDataset(state_dict[\"samples\"], state_dict[\"targets\"])\n",
    "        super().__init__(tensor_dataset, state_dict[\"alias\"])\n",
    "           \n",
    "\n",
    "def save_dataset(dataset: data.Dataset, f, **kwargs):\n",
    "    torch.save(get_dataset_state_dict(dataset), f, **kwargs)\n",
    "    \n",
    "    \n",
    "def load_dataset(f, map_location=None, **kwargs):\n",
    "    state_dict = torch.load(f, map_location=map_location, **kwargs)\n",
    "    dataset = ImportedDataset(state_dict)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_dataset = NamedDataset(data.TensorDataset(torch.arange(0, 10), torch.arange(90, 100)), \"LinearDataset\")\n",
    "\n",
    "samples, targets = dataset_to_tensors(linear_dataset)\n",
    "\n",
    "assert all(samples == torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n",
    "assert all(targets == torch.tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alias': 'LinearDataset',\n",
       " 'samples': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'targets': tensor([90, 91, 92, 93, 94, 95, 96, 97, 98, 99])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dataset_state_dict(linear_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(linear_dataset, \"linear.dataset\")\n",
    "\n",
    "loaded_linear_dataset = load_dataset(\"linear.dataset\")\n",
    "\n",
    "loaded_samples, loaded_targets = dataset_to_tensors(loaded_linear_dataset)\n",
    "\n",
    "assert all(loaded_samples == samples)\n",
    "assert all(loaded_targets == targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
