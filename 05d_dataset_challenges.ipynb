{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Challenges\n",
    "> “Whoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you.”\n",
    ">\n",
    "> ― Friedrich Nietzsche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dataset_challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model real-world use cases better, we need:\n",
    "* redundant/duplicated data;\n",
    "* noisy labels (emulating noisy oracles);\n",
    "* class imbalance;\n",
    "* out-of-distribution data/outliers included in the unlabelled data;\n",
    "* noisy or ambiguous samples.\n",
    "\n",
    "RepeatedMNIST takes care of the first and last challenge in a very specific way. \n",
    "This chapter takes care of the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import bisect\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class _NamedDataset(data.Dataset):\n",
    "    dataset: data.Dataset\n",
    "    name: str\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, name: str):\n",
    "        self.dataset = dataset\n",
    "        self.name = name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return _NamedDataset(data.ConcatDataset([self, other]), f\"{self} + {other}\")\n",
    "\n",
    "    def __mul__(self, factor):\n",
    "        return SubsetDataset(self, factor=factor, seed=0)\n",
    "\n",
    "\n",
    "class NamedDataset(_NamedDataset):\n",
    "    def __init__(self, dataset: data.Dataset, name: str):\n",
    "        super().__init__(dataset, name)\n",
    "\n",
    "\n",
    "def get_base_dataset(dataset, index):\n",
    "    if isinstance(dataset, NamedDataset):\n",
    "        return dataset\n",
    "    elif isinstance(dataset, data.ConcatDataset):\n",
    "        if idx < 0:\n",
    "            if -idx > len(self):\n",
    "                raise ValueError(\"absolute value of index should not exceed dataset length\")\n",
    "            idx = len(self) + idx\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return get_base_dataset(dataset.datasets[dataset_idx], sample_idx)\n",
    "    elif isinstance(dataset, _NamedDataset):\n",
    "        return get_base_dataset(dataset.dataset, index)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTDataset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "\n",
    "MNIST = NamedDataset(FastMNIST(root=\"data/\", download=True, device=\"cuda\"), name=\"MNISTDataset\")\n",
    "\n",
    "MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverridenTargetDataset(data.Dataset):\n",
    "    indices_set: set\n",
    "    reverse_indices: dict\n",
    "    new_targets: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, indices: list, new_targets: list):\n",
    "        self.dataset = dataset\n",
    "        self.indices_set = set(indices)\n",
    "        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}\n",
    "        self.new_targets = new_targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "\n",
    "        if idx not in self.indices_set:\n",
    "            return data, label\n",
    "\n",
    "        ridx = self.reverse_indices[idx]\n",
    "        new_y = self.new_targets[ridx]\n",
    "        return data, new_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"OverridenTargetDataset({var(self)})\"\n",
    "\n",
    "\n",
    "class CorruptedLabelsDataset(_NamedDataset):\n",
    "    dataset: data.Dataset\n",
    "    options: dict\n",
    "    implementation: OverridenTargetDataset\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset: data.Dataset, *, size_corrupted: Union[float, int], num_classes: int, seed: int, device=None\n",
    "    ):\n",
    "        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"CorruptedLabelsDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        N = len(dataset)\n",
    "\n",
    "        if size_corrupted > 1:\n",
    "            num_corrupted = size_corrupted\n",
    "        else:\n",
    "            num_corrupted = int(N * size_corrupted)\n",
    "\n",
    "        indices = generator.choice(N, size=num_corrupted, replace=False)\n",
    "        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)\n",
    "\n",
    "        self.implementation = OverridenTargetDataset(\n",
    "            dataset, indices=indices, new_targets=torch.as_tensor(new_targets, device=device)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.implementation[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.implementation)\n",
    "\n",
    "\n",
    "class RandomLabelsDataset(_NamedDataset):\n",
    "    options: dict\n",
    "    new_labels: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, seed: int, device=None):\n",
    "        options = dict(num_classes=num_classes, seed=seed)\n",
    "\n",
    "        super().__init__(dataset, f\"RandomLabelsDataset({dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        N = len(dataset)\n",
    "\n",
    "        self.new_labels = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.new_labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CorruptedLabelsDataset(dataset=ZeroDataset, {'size_corrupted': 0.5, 'num_classes': 10, 'seed': 1})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_dataset = NamedDataset(data.TensorDataset(torch.zeros(10), torch.zeros(10)), \"ZeroDataset\")\n",
    "\n",
    "corrupted_labels_dataset = CorruptedLabelsDataset(zero_dataset, size_corrupted=0.5, num_classes=10, seed=1)\n",
    "\n",
    "assert list(corrupted_labels_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(0.0)),\n",
    "]\n",
    "\n",
    "corrupted_labels_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomLabelsDataset(ZeroDataset, {'num_classes': 10, 'seed': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_dataset = RandomLabelsDataset(zero_dataset, num_classes=10, seed=2)\n",
    "\n",
    "assert list(corrupted_dataset) == [\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(1)),\n",
    "    (torch.tensor(0.0), torch.tensor(2)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(8)),\n",
    "    (torch.tensor(0.0), torch.tensor(4)),\n",
    "    (torch.tensor(0.0), torch.tensor(0)),\n",
    "    (torch.tensor(0.0), torch.tensor(3)),\n",
    "    (torch.tensor(0.0), torch.tensor(6)),\n",
    "]\n",
    "\n",
    "corrupted_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator):\n",
    "    class_counts = list(class_counts)\n",
    "\n",
    "    subset_indices = []\n",
    "\n",
    "    remaining_samples = sum(class_counts)\n",
    "\n",
    "    indices = generator.permutation(len(dataset))\n",
    "    for index in indices:\n",
    "        _, y = dataset[index]\n",
    "\n",
    "        if class_counts[y] > 0:\n",
    "            subset_indices.append(index)\n",
    "            class_counts[y] -= 1\n",
    "            remaining_samples -= 1\n",
    "\n",
    "            if remaining_samples <= 0:\n",
    "                break\n",
    "\n",
    "    return subset_indices\n",
    "\n",
    "\n",
    "class ImbalancedDataset(_NamedDataset):\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):\n",
    "        options = dict(class_counts=class_counts, seed=seed)\n",
    "        super().__init__(dataset, f\"ImbalancedDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class ImbalancedClassSplitDataset(_NamedDataset):\n",
    "    dataset: data.Dataset\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, majority_percentage: int, seed: int):\n",
    "        assert (num_classes % 2) == 0\n",
    "\n",
    "        super().__init__(dataset, None)\n",
    "\n",
    "        N_class = len(dataset) // num_classes\n",
    "        N_majority = N_class * majority_percentage // 100\n",
    "        N_minority = N_class * (100 - majority_percentage) // 100\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        class_counts = [N_majority] * (num_classes // 2) + [N_minority] * (num_classes // 2)\n",
    "        class_counts = generator.permuted(class_counts)\n",
    "\n",
    "        self.options = dict(\n",
    "            num_classes=num_classes, majority_percentage=majority_percentage, seed=seed, class_counts=class_counts\n",
    "        )\n",
    "        self.name = f\"ImbalancedDataset(dataset={self.dataset}, {self.options})\"\n",
    "\n",
    "        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 0, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_dataset = NamedDataset(data.TensorDataset(torch.arange(9), torch.as_tensor(list(range(3)) * 3)), \"123\")\n",
    "\n",
    "imbalanced_indices = get_class_indices(three_dataset, class_counts=[3, 0, 0], generator=np.random.default_rng())\n",
    "\n",
    "three_dataset[imbalanced_indices][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 0, 2, 2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset=123, {'class_counts': [1, 2, 3], 'seed': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbalanced_dataset = ImbalancedDataset(three_dataset, class_counts=[1, 2, 3], seed=2)\n",
    "print(imbalanced_dataset[:][1])\n",
    "imbalanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImbalancedDataset(dataset=MNISTDataset, {'num_classes': 10, 'majority_percentage': 80, 'seed': 1, 'class_counts': array([1200, 4800, 1200, 4800, 4800, 4800, 1200, 1200, 1200, 4800])})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImbalancedClassSplitDataset(MNIST, num_classes=10, majority_percentage=80, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing in OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "# Convert label dataset to one hot\n",
    "class OneHotDataset(_NamedDataset):\n",
    "    options: dict\n",
    "    targets: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):\n",
    "        options = dict(num_classes=num_classes)\n",
    "\n",
    "        super().__init__(dataset, f\"OneHotDataset({dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        N = len(dataset)\n",
    "        targets = torch.zeros(len(dataset), num_classes, dtype=dtype, device=device)\n",
    "        for i, (_, label) in enumerate(dataset):\n",
    "            targets[i, label] = 1.0\n",
    "\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.targets[idx]\n",
    "\n",
    "\n",
    "class SubsetDataset(_NamedDataset):\n",
    "    dataset: data.Dataset\n",
    "    options: dict\n",
    "    indices: list\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):\n",
    "        options = dict(size=size, factor=factor, seed=seed)\n",
    "        super().__init__(dataset, f\"SubsetDataset(dataset={dataset}, {options})\")\n",
    "        self.options = options\n",
    "\n",
    "        generator = np.random.default_rng(seed)\n",
    "\n",
    "        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)\n",
    "        if size is not None:\n",
    "            subset_size = size\n",
    "        elif factor is not None:\n",
    "            subset_size = int(len(dataset) * factor)\n",
    "            if seed == 0:\n",
    "                self.name = f\"{dataset} * {factor}\"\n",
    "\n",
    "        self.indices = generator.choice(len(dataset), size=subset_size, replace=subset_size > len(dataset))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "class ConstantTargetDataset(_NamedDataset):\n",
    "    target: object\n",
    "\n",
    "    def __init__(self, dataset: data.Dataset, target: object):\n",
    "        super().__init__(dataset, f\"ConstantTargetDataset({dataset}, {target})\")\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, _ = self.dataset[idx]\n",
    "        return data, self.target\n",
    "\n",
    "\n",
    "def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int, device: str = None):\n",
    "    target = torch.ones(num_classes, device=device) / num_classes\n",
    "    result = ConstantTargetDataset(dataset, target)\n",
    "    result.options = dict(num_classes=num_classes)\n",
    "    result.name = f\"UniformTargetDataset({dataset}, {result.options})\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To an OOD dataset, one can either use:\n",
    "```\n",
    "MNIST+OOD*0.5\n",
    "```\n",
    "and then use `get_base_dataset(dataset, index) == OOD` to check whether a picked sample is OOD.\n",
    "\n",
    "Alternatively, we can use:\n",
    "```\n",
    "OneHotDataset(MNIST) + UniformTargetDataset(OOD * 0.5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTDataset * 0.1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotDataset(MNISTDataset * 0.1, {'num_classes': 10})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_MNIST = OneHotDataset(MNIST * 0.1, num_classes=10)\n",
    "print(one_hot_MNIST[0][1])\n",
    "\n",
    "one_hot_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniformTargetDataset(OneHotDataset(MNISTDataset * 0.1, {'num_classes': 10}), {'num_classes': 10})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniformTargetDataset(one_hot_MNIST, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: add AdditiveGaussianNoise.\n",
    "\n",
    "The problem is that for large datasets, this can use up a lot of memory. Creating a new random generator for each sample could be too slow (as we are creating batches), so it might be worth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
