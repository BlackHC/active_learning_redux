{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVHN/CIFAR-10 OOD Experiment\n",
    "> Can we get better by training on our assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp svhn_cifar10_ood_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import dataclasses\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from blackhc.project import is_run_from_ipython\n",
    "from blackhc.project.experiment import embedded_experiments\n",
    "\n",
    "import batchbald_redux.acquisition_functions as acquisition_functions\n",
    "from batchbald_redux.acquisition_functions import (\n",
    "    CandidateBatchComputer,\n",
    "    EvalCandidateBatchComputer,\n",
    ")\n",
    "from batchbald_redux.black_box_model_training import evaluate\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    get_base_dataset_index,\n",
    "    get_target,\n",
    ")\n",
    "from batchbald_redux.di import DependencyInjection\n",
    "from batchbald_redux.experiment_data import ExperimentData, ExperimentDataConfig, OoDDatasetConfig\n",
    "from batchbald_redux.resnet_models import Cifar10ModelTrainer\n",
    "from batchbald_redux.train_eval_model import (\n",
    "    TrainEvalModel,\n",
    "    TrainSelfDistillationEvalModel,\n",
    ")\n",
    "from batchbald_redux.trained_model import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UnifiedExperiment:\n",
    "    seed: int\n",
    "\n",
    "    id_dataset_name: str\n",
    "    ood_dataset_name: str\n",
    "    ood_exposure: bool\n",
    "    initial_training_set_size: int = 0\n",
    "    validation_set_size: int = 1024\n",
    "    evaluation_set_size: int = 1024\n",
    "    id_repetitions: float = 1\n",
    "    ood_repetitions: float = 1\n",
    "    add_dataset_noise: bool = False\n",
    "    validation_split_random_state: int = 0\n",
    "\n",
    "    acquisition_size: int = 5\n",
    "    max_training_set: int = 200\n",
    "\n",
    "    num_pool_samples: int = 20\n",
    "    num_validation_samples: int = 20\n",
    "    num_training_samples: int = 1\n",
    "\n",
    "    device: str = \"cuda\"\n",
    "    acquisition_function: Union[\n",
    "        Type[CandidateBatchComputer], Type[EvalCandidateBatchComputer]\n",
    "    ] = acquisition_functions.BALD\n",
    "    train_eval_model: Type[TrainEvalModel] = TrainSelfDistillationEvalModel\n",
    "    model_trainer_factory: Type[ModelTrainer] = Cifar10ModelTrainer\n",
    "\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    def load_experiment_data(self) -> ExperimentData:\n",
    "        di = DependencyInjection(vars(self), [])\n",
    "        odc: OoDDatasetConfig = di.create_dataclass_type(OoDDatasetConfig)\n",
    "        edc: ExperimentDataConfig = di.create_dataclass_type(ExperimentDataConfig, ood_dataset_config=odc)\n",
    "        return edc.load()\n",
    "\n",
    "    # Simple Dependency Injection\n",
    "    def create_acquisition_function(self):\n",
    "        di = DependencyInjection(vars(self))\n",
    "        return di.create_dataclass_type(self.acquisition_function)\n",
    "\n",
    "    def create_train_eval_model(self, runtime_config) -> TrainEvalModel:\n",
    "        config = {**vars(self), **runtime_config}\n",
    "        di = DependencyInjection(config, [])\n",
    "        return di.create_dataclass_type(self.train_eval_model)\n",
    "\n",
    "    def create_model_trainer(self) -> ModelTrainer:\n",
    "        di = DependencyInjection(vars(self))\n",
    "        return di.create_dataclass_type(self.model_trainer_factory)\n",
    "\n",
    "    def run(self, store):\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # Active Learning setup\n",
    "        data = self.load_experiment_data()\n",
    "        store[\"dataset_info\"] = dict(training=repr(data.active_learning.base_dataset), test=repr(data.test_dataset))\n",
    "        store[\"initial_training_set_indices\"] = data.initial_training_set_indices\n",
    "        store[\"evaluation_set_indices\"] = data.evaluation_set_indices\n",
    "\n",
    "        model_trainer = self.create_model_trainer()\n",
    "\n",
    "        train_loader = model_trainer.get_train_dataloader(data.active_learning.training_dataset)\n",
    "        pool_loader = model_trainer.get_evaluation_dataloader(data.active_learning.pool_dataset)\n",
    "        validation_loader = model_trainer.get_evaluation_dataloader(data.validation_dataset)\n",
    "        test_loader = model_trainer.get_evaluation_dataloader(data.test_dataset)\n",
    "\n",
    "        store[\"active_learning_steps\"] = []\n",
    "        active_learning_steps = store[\"active_learning_steps\"]\n",
    "\n",
    "        acquisition_function = self.create_acquisition_function()\n",
    "\n",
    "        num_iterations = 0\n",
    "        max_iterations = int(1.5 * (self.max_training_set - self.initial_training_set_size) / self.acquisition_size)\n",
    "\n",
    "        # Active Training Loop\n",
    "        while True:\n",
    "            training_set_size = len(data.active_learning.training_dataset)\n",
    "            print(f\"Training set size {training_set_size}:\")\n",
    "\n",
    "            # iteration_log = dict(training={}, pool_training={}, evaluation_metrics=None, acquisition=None)\n",
    "            active_learning_steps.append({})\n",
    "            iteration_log = active_learning_steps[-1]\n",
    "\n",
    "            iteration_log[\"training\"] = {}\n",
    "\n",
    "            if self.ood_exposure:\n",
    "                loss = torch.nn.KLDivLoss(log_target=False, reduction=\"batchmean\")\n",
    "                validation_loss = torch.nn.NLLLoss()\n",
    "            else:\n",
    "                loss = validation_loss = torch.nn.NLLLoss()\n",
    "\n",
    "            trained_model = model_trainer.get_trained(train_loader=train_loader, train_augmentations=data.train_augmentations,\n",
    "                                                      validation_loader=validation_loader,\n",
    "                                                      log=iteration_log[\"training\"], loss=loss,\n",
    "                                                      validation_loss=validation_loss)\n",
    "\n",
    "            evaluation_metrics = evaluate(model=trained_model, num_samples=self.num_validation_samples,\n",
    "                                          loader=test_loader, device=self.device)\n",
    "            iteration_log[\"evaluation_metrics\"] = evaluation_metrics\n",
    "            print(f\"Perf after training {evaluation_metrics}\")\n",
    "\n",
    "            if training_set_size >= self.max_training_set or num_iterations >= max_iterations:\n",
    "                print(\"Done.\")\n",
    "                break\n",
    "\n",
    "            if isinstance(acquisition_function, CandidateBatchComputer):\n",
    "                candidate_batch = acquisition_function.compute_candidate_batch(trained_model, pool_loader, self.device)\n",
    "            elif isinstance(acquisition_function, EvalCandidateBatchComputer):\n",
    "                if self.evaluation_set_size:\n",
    "                    eval_dataset = data.evaluation_dataset\n",
    "                else:\n",
    "                    eval_dataset = data.active_learning.pool_dataset\n",
    "\n",
    "                train_eval_model = self.create_train_eval_model(\n",
    "                    dict(\n",
    "                        model_trainer=model_trainer,\n",
    "                        training_dataset=data.active_learning.training_dataset,\n",
    "                        train_augmentations=data.train_augmentations,\n",
    "                        eval_dataset=eval_dataset,\n",
    "                        validation_loader=validation_loader,\n",
    "                        trained_model=trained_model,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                iteration_log[\"eval_training\"] = {}\n",
    "                trained_eval_model = train_eval_model(device=self.device, training_log=iteration_log[\"eval_training\"])\n",
    "\n",
    "                candidate_batch = acquisition_function.compute_candidate_batch(\n",
    "                    trained_model, trained_eval_model, pool_loader, device=self.device\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown acquisition function {acquisition_function}!\")\n",
    "\n",
    "            candidate_global_dataset_indices = []\n",
    "            candidate_labels = []\n",
    "            for index in candidate_batch.indices:\n",
    "                base_di = get_base_dataset_index(data.active_learning.pool_dataset, index)\n",
    "                dataset_type = \"ood\" if base_di.dataset == data.ood_dataset else \"id\"\n",
    "                candidate_global_dataset_indices.append((dataset_type, base_di.index))\n",
    "                label = get_target(data.active_learning.pool_dataset, index)\n",
    "                candidate_labels.append(label)\n",
    "\n",
    "            iteration_log[\"acquisition\"] = dict(\n",
    "                indices=candidate_global_dataset_indices, labels=candidate_labels, scores=candidate_batch.scores\n",
    "            )\n",
    "\n",
    "            print(candidate_batch)\n",
    "            print(candidate_global_dataset_indices)\n",
    "\n",
    "            if self.ood_exposure:\n",
    "                data.active_learning.acquire(candidate_batch.indices)\n",
    "            else:\n",
    "                data.active_learning.acquire(\n",
    "                    [index for index, label in zip(candidate_batch.indices, candidate_labels) if label != -1]\n",
    "                )\n",
    "\n",
    "            ls = \", \".join(f\"{label} ({score:.4})\" for label, score in zip(candidate_labels, candidate_batch.scores))\n",
    "            print(f\"Acquiring (label, score)s: {ls}\")\n",
    "\n",
    "            num_iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: OoDDatasetConfig(\n",
      "\tood_dataset_name=SVHN,\n",
      "\tood_repetitions=1,\n",
      "\tood_exposure=True\n",
      ")\n",
      "Creating: ExperimentDataConfig(\n",
      "\tid_dataset_name=CIFAR-10,\n",
      "\tid_repetitions=1,\n",
      "\tinitial_training_set_size=0,\n",
      "\tvalidation_set_size=1024,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tevaluation_set_size=100,\n",
      "\tadd_dataset_noise=False,\n",
      "\tdevice=cuda,\n",
      "\tood_dataset_config=OoDDatasetConfig(ood_dataset_name='SVHN', ood_repetitions=1, ood_exposure=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "Creating: Cifar10ModelTrainer(\n",
      "\tdevice=cuda,\n",
      "\tnum_training_samples=1,\n",
      "\tnum_validation_samples=20,\n",
      "\tmax_training_epochs=1,\n",
      "\tmin_samples_per_epoch=5056\n",
      ")\n",
      "Creating: EvalBALD(\n",
      "\tacquisition_size=10,\n",
      "\tnum_pool_samples=2\n",
      ")\n",
      "Training set size 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/multiprocessing/queues.py\", line 235, in _feed\n",
      "    close()\n",
      "  File \"/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/multiprocessing/queues.py\", line 266, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.1, 'crossentropy': tensor(2.3507)}\n",
      "Creating: TrainSelfDistillationEvalModel(\n",
      "\tnum_pool_samples=2,\n",
      "\ttraining_dataset=<torch.utils.data.dataset.Subset object at 0x7ff8e6e7d490>,\n",
      "\teval_dataset=Evaluation Set (100 samples),\n",
      "\tvalidation_loader=<torch.utils.data.dataloader.DataLoader object at 0x7ff8e6e7d3a0>,\n",
      "\ttraining_batch_size=128,\n",
      "\ttrained_model=TrainedBayesianModel(model=BayesianResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): Identity()\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): ConsistentMCDropout(p=0.5)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConsistentMCDropout(p=0.5)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")),\n",
      "\tmodel_trainer=Cifar10ModelTrainer(device='cuda', num_training_samples=1, num_validation_samples=20, max_training_epochs=1, patience_schedule=(6, 4, 2), factor_schedule=(0.1,), min_samples_per_epoch=5056, num_training_batch_size=128, num_evaluation_batch_size=512),\n",
      "\tmin_samples_per_epoch=5056,\n",
      "\ttrain_augmentations=Sequential(\n",
      "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
      "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
      ")\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "get_trained() missing 1 required keyword-only argument: 'log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_130538/1618670301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_130538/1843298017.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0miteration_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_training\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0mtrained_eval_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_log\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_training\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 candidate_batch = acquisition_function.compute_candidate_batch(\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/train_eval_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, training_log, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         trained_model = self.model_trainer.get_distilled(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mprediction_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_eval_self_distillation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtrain_augmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_augmentations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/resnet_models.py\u001b[0m in \u001b[0;36mget_distilled\u001b[0;34m(self, prediction_loader, train_augmentations, validation_loader, log)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLDivLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batchmean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         return self.get_trained(train_loader=prediction_loader, train_augmentations=train_augmentations,\n\u001b[0m\u001b[1;32m    391\u001b[0m                                 validation_loader=validation_loader, loss=loss, validation_loss=validation_loss)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_trained() missing 1 required keyword-only argument: 'log'"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = UnifiedExperiment(\n",
    "    ood_exposure=True,\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    ood_dataset_name=\"SVHN\",\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.EvalBALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=True,\n",
      "\tid_dataset_name=CIFAR-10,\n",
      "\tood_dataset_name=SVHN,\n",
      "\tinitial_training_set_size=0,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=100,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "Creating: EvalBALD(\n",
      "\tacquisition_size=10\n",
      ")\n",
      "Training set size 0:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5734101319e1459e97b63f1f262fc7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current run is terminating due to exception: integer division or modulo by zero.\n",
      "Engine run is terminating due to exception: integer division or modulo by zero.\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_108335/4225534907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_108335/3435168888.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             train(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/black_box_model_training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, training_samples, validation_samples, train_loader, validation_loader, patience, max_epochs, device, training_log, loss, validation_loss, optimizer, prefer_accuracy, train_augmentations)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# Kick everything off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Current run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/active_learning.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Sample slightly more indices to avoid biasing towards start of dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# Have the same number of duplicates for each sample.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_length\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = OodExperiment(\n",
    "    uniform_ood=True,\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    ood_dataset_name=\"SVHN\",\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.EvalBALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=False,\n",
      "\tid_dataset_name=SVHN,\n",
      "\tood_dataset_name=CIFAR-10,\n",
      "\tinitial_training_set_size=0,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=1024,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cpu\n",
      ")\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(Augmentation Node) + ('CIFAR-10 (Train, seed=0, 50000 samples)' | constant_target{'target': tensor(-1), 'num_classes': 10})\n",
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=True,\n",
      "\tid_dataset_name=CIFAR-10,\n",
      "\tood_dataset_name=SVHN,\n",
      "\tinitial_training_set_size=0,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=1024,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cpu\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "(Augmentation Node | one_hot_targets{'num_classes': 10}) + ('SVHN (Train, seed=0, 73257 samples)' | uniform_targets{'num_classes': 10})\n"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "print(\n",
    "    OodExperiment(seed=0, device=\"cpu\", id_dataset_name=\"SVHN\", ood_dataset_name=\"CIFAR-10\", uniform_ood=False)\n",
    "    .load_experiment_data()\n",
    "    .active_learning.base_dataset\n",
    ")\n",
    "print(\n",
    "    OodExperiment(seed=1, device=\"cpu\", id_dataset_name=\"CIFAR-10\", ood_dataset_name=\"SVHN\", uniform_ood=True)\n",
    "    .load_experiment_data()\n",
    "    .active_learning.base_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(Augmentation Node | one_hot_targets{'num_classes': 10}) + ('SVHN (Train, seed=0, 73257 samples)' | uniform_targets{'num_classes': 10})\",\n",
       "  'test': \"'CIFAR-10 (Test)'\"},\n",
       " 'initial_training_set_indices': [12980,\n",
       "  44617,\n",
       "  6984,\n",
       "  21168,\n",
       "  33976,\n",
       "  35571,\n",
       "  33058,\n",
       "  43729,\n",
       "  26944,\n",
       "  24745,\n",
       "  66,\n",
       "  14046,\n",
       "  46542,\n",
       "  39478,\n",
       "  6000,\n",
       "  5915,\n",
       "  39360,\n",
       "  20774,\n",
       "  27084,\n",
       "  44464],\n",
       " 'evaluation_set_indices': [3812,\n",
       "  42704,\n",
       "  6729,\n",
       "  38942,\n",
       "  48125,\n",
       "  16968,\n",
       "  5652,\n",
       "  4045,\n",
       "  10740,\n",
       "  19606,\n",
       "  37164,\n",
       "  33354,\n",
       "  47307,\n",
       "  17878,\n",
       "  26665,\n",
       "  40819,\n",
       "  14805,\n",
       "  201,\n",
       "  47956,\n",
       "  44739,\n",
       "  15578,\n",
       "  36667,\n",
       "  5551,\n",
       "  23088,\n",
       "  32496,\n",
       "  5705,\n",
       "  23255,\n",
       "  25559,\n",
       "  11975,\n",
       "  44032,\n",
       "  47518,\n",
       "  36303,\n",
       "  18452,\n",
       "  34447,\n",
       "  24821,\n",
       "  36157,\n",
       "  48089,\n",
       "  25120,\n",
       "  44689,\n",
       "  6509,\n",
       "  11001,\n",
       "  6995,\n",
       "  10899,\n",
       "  36881,\n",
       "  7002,\n",
       "  19049,\n",
       "  13388,\n",
       "  40737,\n",
       "  9210,\n",
       "  22684,\n",
       "  45656,\n",
       "  5604,\n",
       "  9134,\n",
       "  35979,\n",
       "  19757,\n",
       "  43627,\n",
       "  35248,\n",
       "  23566,\n",
       "  727,\n",
       "  34909,\n",
       "  25443,\n",
       "  45862,\n",
       "  30730,\n",
       "  9611,\n",
       "  43077,\n",
       "  23902,\n",
       "  9541,\n",
       "  38859,\n",
       "  13973,\n",
       "  27923,\n",
       "  21547,\n",
       "  47739,\n",
       "  13909,\n",
       "  624,\n",
       "  25621,\n",
       "  30173,\n",
       "  37807,\n",
       "  7053,\n",
       "  30625,\n",
       "  10450,\n",
       "  23760,\n",
       "  26549,\n",
       "  2676,\n",
       "  11659,\n",
       "  40405,\n",
       "  46217,\n",
       "  5207,\n",
       "  28673,\n",
       "  12645,\n",
       "  45982,\n",
       "  3184,\n",
       "  4581,\n",
       "  16953,\n",
       "  7585,\n",
       "  45975,\n",
       "  28441,\n",
       "  10956,\n",
       "  22017,\n",
       "  21698,\n",
       "  2107],\n",
       " 'active_learning_steps': [{'training': {'epochs': []}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=False,\n",
      "\tid_dataset_type=<class 'batchbald_redux.fast_mnist.FastMNIST'>,\n",
      "\tood_dataset_type=<class 'batchbald_redux.fast_mnist.FastFashionMNIST'>,\n",
      "\tinitial_training_set_size=20,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=100,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Creating: BALD(\n",
      "\tacquisition_size=10\n",
      ")\n",
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b9fac304cd48d0b525701825b08f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.6328125, 'crossentropy': 2.2057557106018066}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.6328125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6557, 'crossentropy': 2.067915026473999}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/237712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateBatch(scores=[0.6930213868618011, 0.6930111590772867, 0.6930059990845621, 0.6929988877382129, 0.6929858811199665, 0.69298055768013, 0.6929786503314972, 0.6929723918437958, 0.6929657161235809, 0.6929287277162075], indices=[37754, 20802, 5448, 184, 51966, 13296, 26425, 46199, 21745, 4910])\n",
      "[('id', 46785), ('id', 36269), ('id', 47902), ('id', 26859), ('id', 58402), ('id', 34634), ('id', 46751), ('id', 46535), ('id', 21910), ('id', 5884)]\n",
      "Acquiring (label, score)s: 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.6929)\n",
      "Training set size 30:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a259e542feb48079e2e0afcebfe22f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.611328125, 'crossentropy': 2.4593361616134644}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.611328125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6549, 'crossentropy': 2.1096548225402834}\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(FastMNIST (train; 58976 samples)) + ('FastFashionMNIST Train (60000 samples)' | constant_target{'target': tensor(-1, device='cuda:0'), 'num_classes': 10})\",\n",
       "  'test': \"'FastMNIST Test (10000 samples)'\"},\n",
       " 'initial_training_set_indices': [53434,\n",
       "  8533,\n",
       "  14640,\n",
       "  39579,\n",
       "  30392,\n",
       "  58125,\n",
       "  37915,\n",
       "  3091,\n",
       "  57520,\n",
       "  43803,\n",
       "  44119,\n",
       "  52296,\n",
       "  58226,\n",
       "  40334,\n",
       "  46037,\n",
       "  22015,\n",
       "  22304,\n",
       "  43812,\n",
       "  12640,\n",
       "  53689],\n",
       " 'evaluation_set_indices': [29974,\n",
       "  55573,\n",
       "  35472,\n",
       "  44048,\n",
       "  48031,\n",
       "  5616,\n",
       "  10110,\n",
       "  47420,\n",
       "  56990,\n",
       "  34198,\n",
       "  3792,\n",
       "  5715,\n",
       "  15969,\n",
       "  32775,\n",
       "  19757,\n",
       "  34588,\n",
       "  28991,\n",
       "  47417,\n",
       "  26501,\n",
       "  12108,\n",
       "  5573,\n",
       "  48032,\n",
       "  40646,\n",
       "  43252,\n",
       "  2404,\n",
       "  36797,\n",
       "  29079,\n",
       "  40018,\n",
       "  37047,\n",
       "  41512,\n",
       "  45567,\n",
       "  801,\n",
       "  10664,\n",
       "  52801,\n",
       "  42890,\n",
       "  32972,\n",
       "  45974,\n",
       "  20801,\n",
       "  23496,\n",
       "  5803,\n",
       "  10508,\n",
       "  46870,\n",
       "  49549,\n",
       "  306,\n",
       "  38725,\n",
       "  13074,\n",
       "  19689,\n",
       "  27135,\n",
       "  16068,\n",
       "  18137,\n",
       "  2728,\n",
       "  43321,\n",
       "  29950,\n",
       "  380,\n",
       "  27254,\n",
       "  50466,\n",
       "  31965,\n",
       "  24052,\n",
       "  44454,\n",
       "  20076,\n",
       "  21423,\n",
       "  58741,\n",
       "  27145,\n",
       "  38430,\n",
       "  37354,\n",
       "  49986,\n",
       "  4321,\n",
       "  12610,\n",
       "  34482,\n",
       "  35794,\n",
       "  396,\n",
       "  50036,\n",
       "  46861,\n",
       "  57811,\n",
       "  53831,\n",
       "  49304,\n",
       "  51555,\n",
       "  29614,\n",
       "  767,\n",
       "  23451,\n",
       "  49512,\n",
       "  26479,\n",
       "  50997,\n",
       "  1774,\n",
       "  44803,\n",
       "  55187,\n",
       "  30013,\n",
       "  33736,\n",
       "  49169,\n",
       "  46464,\n",
       "  31444,\n",
       "  52440,\n",
       "  33486,\n",
       "  2206,\n",
       "  15675,\n",
       "  54426,\n",
       "  9574,\n",
       "  54012,\n",
       "  28833,\n",
       "  44428],\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.6328125,\n",
       "      'crossentropy': 2.2057557106018066}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6557,\n",
       "    'crossentropy': 2.067915026473999},\n",
       "   'acquisition': {'indices': [('id', 46785),\n",
       "     ('id', 36269),\n",
       "     ('id', 47902),\n",
       "     ('id', 26859),\n",
       "     ('id', 58402),\n",
       "     ('id', 34634),\n",
       "     ('id', 46751),\n",
       "     ('id', 46535),\n",
       "     ('id', 21910),\n",
       "     ('id', 5884)],\n",
       "    'labels': [tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0')],\n",
       "    'scores': [0.6930213868618011,\n",
       "     0.6930111590772867,\n",
       "     0.6930059990845621,\n",
       "     0.6929988877382129,\n",
       "     0.6929858811199665,\n",
       "     0.69298055768013,\n",
       "     0.6929786503314972,\n",
       "     0.6929723918437958,\n",
       "     0.6929657161235809,\n",
       "     0.6929287277162075]}},\n",
       "  {'training': {'epochs': [{'accuracy': 0.611328125,\n",
       "      'crossentropy': 2.4593361616134644}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6549,\n",
       "    'crossentropy': 2.1096548225402834}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "experiment = OodExperiment(\n",
    "    uniform_ood=False,\n",
    "    id_dataset_type=FastMNIST,\n",
    "    ood_dataset=FastFashionMNIST,\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.BALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    temperature=5,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=False,\n",
      "\tid_dataset_type=<class 'batchbald_redux.fast_mnist.FastMNIST'>,\n",
      "\tood_dataset_type=<class 'batchbald_redux.fast_mnist.FastFashionMNIST'>,\n",
      "\tinitial_training_set_size=20,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=100,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Creating: EvalBALD(\n",
      "\tacquisition_size=10\n",
      ")\n",
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b4c2912b734a5c9842167660acb396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.6328125, 'crossentropy': 2.1983988285064697}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.6328125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6559, 'crossentropy': 2.0655145431518553}\n",
      "Creating: TrainSelfDistillationEvalModel(\n",
      "\tnum_pool_samples=2,\n",
      "\tnum_training_samples=1,\n",
      "\tnum_validation_samples=20,\n",
      "\tnum_patience_epochs=3,\n",
      "\tmax_epochs=3,\n",
      "\ttraining_dataset=<torch.utils.data.dataset.Subset object at 0x7fdc2a2f6520>,\n",
      "\teval_dataset=Evaluation Set (100 samples),\n",
      "\tvalidation_loader=<torch.utils.data.dataloader.DataLoader object at 0x7fdc2a2f6f40>,\n",
      "\ttraining_batch_size=64,\n",
      "\tmodel_optimizer_factory=<class 'batchbald_redux.models.MnistOptimizerFactory'>,\n",
      "\ttrained_model=TrainedMCDropoutModel(num_samples=2, model=BayesianMNISTCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1_drop): ConsistentMCDropout2d(p=0.5)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): ConsistentMCDropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc1_drop): ConsistentMCDropout(p=0.5)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")),\n",
      "\tmin_samples_per_epoch=5056\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9540e4c3d64037b94c9f83c58c0f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 33%|###3      | 1/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.5966796875, 'crossentropy': 1.5679776072502136}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.5908203125, 'crossentropy': 1.7015655040740967}\n",
      "RestoringEarlyStopping: 1 / 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.5869140625, 'crossentropy': 1.7359371781349182}\n",
      "RestoringEarlyStopping: 2 / 3\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.5966796875)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/237712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/237712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateBatch(scores=[0.6806451752781868, 0.6790880858898163, 0.6763254124671221, 0.6758237481117249, 0.675022229552269, 0.6749783530831337, 0.6741648018360138, 0.6734838634729385, 0.6730879247188568, 0.6723472699522972], indices=[54855, 56792, 22122, 40241, 10674, 15235, 14226, 26593, 14211, 1276])\n",
      "[('id', 55525), ('id', 45346), ('id', 8446), ('id', 28278), ('id', 54369), ('id', 51180), ('id', 53366), ('id', 16103), ('id', 13247), ('id', 55629)]\n",
      "Acquiring (label, score)s: 2 (0.6806), 2 (0.6791), 2 (0.6763), 2 (0.6758), 9 (0.675), 7 (0.675), 2 (0.6742), 9 (0.6735), 9 (0.6731), 2 (0.6723)\n",
      "Training set size 30:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5332cfaf664d96a60a8995426d4f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.6474609375, 'crossentropy': 2.016913414001465}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.6474609375)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6889, 'crossentropy': 1.9067923236846924}\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(FastMNIST (train; 58976 samples)) + ('FastFashionMNIST Train (60000 samples)' | constant_target{'target': tensor(-1, device='cuda:0'), 'num_classes': 10})\",\n",
       "  'test': \"'FastMNIST Test (10000 samples)'\"},\n",
       " 'initial_training_set_indices': [53434,\n",
       "  8533,\n",
       "  14640,\n",
       "  39579,\n",
       "  30392,\n",
       "  58125,\n",
       "  37915,\n",
       "  3091,\n",
       "  57520,\n",
       "  43803,\n",
       "  44119,\n",
       "  52296,\n",
       "  58226,\n",
       "  40334,\n",
       "  46037,\n",
       "  22015,\n",
       "  22304,\n",
       "  43812,\n",
       "  12640,\n",
       "  53689],\n",
       " 'evaluation_set_indices': [29974,\n",
       "  55573,\n",
       "  35472,\n",
       "  44048,\n",
       "  48031,\n",
       "  5616,\n",
       "  10110,\n",
       "  47420,\n",
       "  56990,\n",
       "  34198,\n",
       "  3792,\n",
       "  5715,\n",
       "  15969,\n",
       "  32775,\n",
       "  19757,\n",
       "  34588,\n",
       "  28991,\n",
       "  47417,\n",
       "  26501,\n",
       "  12108,\n",
       "  5573,\n",
       "  48032,\n",
       "  40646,\n",
       "  43252,\n",
       "  2404,\n",
       "  36797,\n",
       "  29079,\n",
       "  40018,\n",
       "  37047,\n",
       "  41512,\n",
       "  45567,\n",
       "  801,\n",
       "  10664,\n",
       "  52801,\n",
       "  42890,\n",
       "  32972,\n",
       "  45974,\n",
       "  20801,\n",
       "  23496,\n",
       "  5803,\n",
       "  10508,\n",
       "  46870,\n",
       "  49549,\n",
       "  306,\n",
       "  38725,\n",
       "  13074,\n",
       "  19689,\n",
       "  27135,\n",
       "  16068,\n",
       "  18137,\n",
       "  2728,\n",
       "  43321,\n",
       "  29950,\n",
       "  380,\n",
       "  27254,\n",
       "  50466,\n",
       "  31965,\n",
       "  24052,\n",
       "  44454,\n",
       "  20076,\n",
       "  21423,\n",
       "  58741,\n",
       "  27145,\n",
       "  38430,\n",
       "  37354,\n",
       "  49986,\n",
       "  4321,\n",
       "  12610,\n",
       "  34482,\n",
       "  35794,\n",
       "  396,\n",
       "  50036,\n",
       "  46861,\n",
       "  57811,\n",
       "  53831,\n",
       "  49304,\n",
       "  51555,\n",
       "  29614,\n",
       "  767,\n",
       "  23451,\n",
       "  49512,\n",
       "  26479,\n",
       "  50997,\n",
       "  1774,\n",
       "  44803,\n",
       "  55187,\n",
       "  30013,\n",
       "  33736,\n",
       "  49169,\n",
       "  46464,\n",
       "  31444,\n",
       "  52440,\n",
       "  33486,\n",
       "  2206,\n",
       "  15675,\n",
       "  54426,\n",
       "  9574,\n",
       "  54012,\n",
       "  28833,\n",
       "  44428],\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.6328125,\n",
       "      'crossentropy': 2.1983988285064697}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6559,\n",
       "    'crossentropy': 2.0655145431518553},\n",
       "   'eval_training': {'epochs': [{'accuracy': 0.5966796875,\n",
       "      'crossentropy': 1.5679776072502136},\n",
       "     {'accuracy': 0.5908203125, 'crossentropy': 1.7015655040740967},\n",
       "     {'accuracy': 0.5869140625, 'crossentropy': 1.7359371781349182}],\n",
       "    'best_epoch': 1},\n",
       "   'acquisition': {'indices': [('id', 55525),\n",
       "     ('id', 45346),\n",
       "     ('id', 8446),\n",
       "     ('id', 28278),\n",
       "     ('id', 54369),\n",
       "     ('id', 51180),\n",
       "     ('id', 53366),\n",
       "     ('id', 16103),\n",
       "     ('id', 13247),\n",
       "     ('id', 55629)],\n",
       "    'labels': [tensor(2, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(9, device='cuda:0'),\n",
       "     tensor(7, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(9, device='cuda:0'),\n",
       "     tensor(9, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0')],\n",
       "    'scores': [0.6806451752781868,\n",
       "     0.6790880858898163,\n",
       "     0.6763254124671221,\n",
       "     0.6758237481117249,\n",
       "     0.675022229552269,\n",
       "     0.6749783530831337,\n",
       "     0.6741648018360138,\n",
       "     0.6734838634729385,\n",
       "     0.6730879247188568,\n",
       "     0.6723472699522972]}},\n",
       "  {'training': {'epochs': [{'accuracy': 0.6474609375,\n",
       "      'crossentropy': 2.016913414001465}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6889,\n",
       "    'crossentropy': 1.9067923236846924}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = OodExperiment(\n",
    "    uniform_ood=False,\n",
    "    id_dataset_type=FastMNIST,\n",
    "    ood_dataset=FastFashionMNIST,\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.EvalBALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    temperature=5,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "configs = [\n",
    "    UnifiedExperiment(\n",
    "        seed=seed + 1234,\n",
    "        ood_exposure=ood_exposure,\n",
    "        acquisition_function=acquisition_function,\n",
    "        acquisition_size=5,\n",
    "        num_pool_samples=num_pool_samples,\n",
    "        evaluation_set_size=evaluation_set_size,\n",
    "        id_dataset_name=id_dataset_name,\n",
    "        ood_dataset_name=ood_dataset_name,\n",
    "    )\n",
    "    for seed in range(3)\n",
    "    for acquisition_function in [acquisition_functions.BatchEvalBALD, acquisition_functions.BatchBALD]\n",
    "    for evaluation_set_size in [1024]\n",
    "    for num_pool_samples in [100]\n",
    "    for ood_exposure in [True, False]\n",
    "    for id_dataset_name, ood_dataset_name in [(\"CIFAR-10\", \"SVHN\"), (\"SVHN\", \"CIFAR-10\")]\n",
    "]\n",
    "\n",
    "if not is_run_from_ipython() and __name__ == \"__main__\":\n",
    "    for job_id, store in embedded_experiments(__file__, len(configs)):\n",
    "        config = configs[job_id]\n",
    "        config.seed += job_id\n",
    "        print(config)\n",
    "        store[\"config\"] = dataclasses.asdict(config)\n",
    "        store[\"log\"] = {}\n",
    "\n",
    "        try:\n",
    "            config.run(store=store)\n",
    "        except Exception:\n",
    "            store[\"exception\"] = traceback.format_exc()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "import prettyprinter\n",
    "\n",
    "prettyprinter.install_extras(include={\"dataclasses\"})\n",
    "\n",
    "prettyprinter.pprint(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
