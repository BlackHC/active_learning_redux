{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVHN/CIFAR-10 OOD Experiment\n",
    "> Can we get better by training on our assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp svhn_cifar10_ood_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import dataclasses\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from blackhc.project import is_run_from_ipython\n",
    "from blackhc.project.experiment import embedded_experiments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import batchbald_redux.acquisition_functions as acquisition_functions\n",
    "from batchbald_redux.acquisition_functions import (\n",
    "    CandidateBatchComputer,\n",
    "    EvalCandidateBatchComputer,\n",
    ")\n",
    "from batchbald_redux.active_learning import ActiveLearningData, RandomFixedLengthSampler\n",
    "from batchbald_redux.black_box_model_training import evaluate, train\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    AdditiveGaussianNoise,\n",
    "    AliasDataset,\n",
    "    NamedDataset,\n",
    "    create_named_mnist,\n",
    "    get_balanced_sample_indices_by_class,\n",
    "    get_base_dataset_index,\n",
    "    get_target,\n",
    ")\n",
    "from batchbald_redux.datasets import get_dataset\n",
    "from batchbald_redux.datasets import train_validation_split\n",
    "from batchbald_redux.di import DependencyInjection\n",
    "from batchbald_redux.fast_mnist import FastFashionMNIST, FastMNIST\n",
    "from batchbald_redux.model_optimizer_factory import ModelOptimizerFactory\n",
    "from batchbald_redux.resnet_models import Cifar10BayesianResnetFactory\n",
    "from batchbald_redux.train_eval_model import (\n",
    "    TrainEvalModel,\n",
    "    TrainSelfDistillationEvalModel,\n",
    ")\n",
    "from batchbald_redux.trained_model import TrainedMCDropoutModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentData:\n",
    "    active_learning: ActiveLearningData\n",
    "    augmentation_node: AliasDataset\n",
    "    unaugmented_train_dataset: Dataset\n",
    "    augmented_train_dataset: Dataset\n",
    "    ood_dataset: NamedDataset\n",
    "    validation_dataset: Dataset\n",
    "    test_dataset: Dataset\n",
    "    evaluation_dataset: Dataset\n",
    "    initial_training_set_indices: [int]\n",
    "    evaluation_set_indices: [int]\n",
    "        \n",
    "    def toggle_augmentations(self, augment: bool):\n",
    "        if augment:\n",
    "            self.augmentation_node.dataset = self.augmented_train_dataset\n",
    "        else:\n",
    "            self.augmentation_node.dataset = self.unaugmented_train_dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentDataConfig:\n",
    "    uniform_ood: bool\n",
    "    id_dataset_name: str\n",
    "    ood_dataset_name: str\n",
    "\n",
    "    initial_training_set_size: int\n",
    "    validation_set_size: int\n",
    "    evaluation_set_size: int\n",
    "    id_repetitions: float\n",
    "    ood_repetitions: float\n",
    "    add_dataset_noise: bool\n",
    "    validation_split_random_state: int\n",
    "\n",
    "    device: str\n",
    "\n",
    "    def load(self) -> ExperimentData:\n",
    "        return load_experiment_data(\n",
    "            id_dataset_name=self.id_dataset_name,\n",
    "            ood_dataset_name=self.ood_dataset_name,\n",
    "            initial_training_set_size=self.initial_training_set_size,\n",
    "            validation_set_size=self.validation_set_size,\n",
    "            evaluation_set_size=self.evaluation_set_size,\n",
    "            id_repetitions=self.id_repetitions,\n",
    "            ood_repetitions=self.ood_repetitions,\n",
    "            add_dataset_noise=self.add_dataset_noise,\n",
    "            uniform_ood=self.uniform_ood,\n",
    "            validation_split_random_state=self.validation_split_random_state,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    *,\n",
    "    id_dataset_name: str,\n",
    "    ood_dataset_name: str,\n",
    "    initial_training_set_size: int,\n",
    "    validation_set_size: int,\n",
    "    evaluation_set_size: int,\n",
    "    id_repetitions: float,\n",
    "    ood_repetitions: float,\n",
    "    add_dataset_noise: bool,\n",
    "    validation_split_random_state: int,\n",
    "    device: str,\n",
    "    uniform_ood: bool,\n",
    ") -> ExperimentData:    \n",
    "    split_dataset = get_dataset(id_dataset_name, root=\"data\", train_augmentation=True, validation_set_size=validation_set_size,\n",
    "                                validation_split_random_state=validation_split_random_state, normalize_like_cifar10=True)\n",
    "    unaugmented_split_dataset = get_dataset(id_dataset_name, root=\"data\", train_augmentation=True, validation_set_size=validation_set_size,\n",
    "                                validation_split_random_state=validation_split_random_state, normalize_like_cifar10=True)\n",
    "    \n",
    "    augmentation_node = AliasDataset(split_dataset.train, \"Augmentation Node\")\n",
    "    train_dataset=augmentation_node\n",
    "    \n",
    "    # If we reduce the train set, we need to do so before picking the initial train set.\n",
    "    if id_repetitions < 1:\n",
    "        train_dataset = train_dataset * id_repetitions\n",
    "\n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    initial_samples_per_class = initial_training_set_size // num_classes\n",
    "    evaluation_set_samples_per_class = evaluation_set_size // num_classes\n",
    "    samples_per_class = initial_samples_per_class + evaluation_set_samples_per_class\n",
    "    balanced_samples_indices = get_balanced_sample_indices_by_class(\n",
    "        train_dataset,\n",
    "        num_classes=num_classes,\n",
    "        samples_per_class=samples_per_class,\n",
    "        seed=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    initial_training_set_indices = [\n",
    "        idx for by_class in balanced_samples_indices.values() for idx in by_class[:initial_samples_per_class]\n",
    "    ]\n",
    "    evaluation_set_indices = [\n",
    "        idx for by_class in balanced_samples_indices.values() for idx in by_class[initial_samples_per_class:]\n",
    "    ]\n",
    "\n",
    "    # If we over-sample the train set, we do so after picking the initial train set to avoid duplicates.\n",
    "    if id_repetitions > 1:\n",
    "        train_dataset = train_dataset * id_repetitions\n",
    "\n",
    "    original_ood_dataset = get_dataset(ood_dataset_name, root=\"data\", train_augmentation=False, normalize_like_cifar10=True).train\n",
    "    if uniform_ood:\n",
    "        train_dataset = train_dataset.one_hot(device=device)\n",
    "        ood_dataset = original_ood_dataset.uniform_target(device=device)\n",
    "    else:\n",
    "        ood_dataset = original_ood_dataset.constant_target(\n",
    "            target=torch.tensor(-1, device=device), num_classes=train_dataset.get_num_classes()\n",
    "        )\n",
    "\n",
    "    if ood_repetitions != 1:\n",
    "        ood_dataset = ood_dataset * ood_repetitions\n",
    "\n",
    "    train_dataset = train_dataset + ood_dataset\n",
    "\n",
    "    if add_dataset_noise:\n",
    "        train_dataset = AdditiveGaussianNoise(train_dataset, 0.1)\n",
    "\n",
    "    active_learning_data = ActiveLearningData(train_dataset)\n",
    "\n",
    "    active_learning_data.acquire_base_indices(initial_training_set_indices)\n",
    "\n",
    "    evaluation_dataset = AliasDataset(\n",
    "        active_learning_data.extract_dataset_from_base_indices(evaluation_set_indices),\n",
    "        f\"Evaluation Set ({len(evaluation_set_indices)} samples)\",\n",
    "    )\n",
    "\n",
    "    return ExperimentData(\n",
    "        active_learning=active_learning_data,\n",
    "        augmentation_node=augmentation_node,\n",
    "        augmented_train_dataset=split_dataset.train,\n",
    "        unaugmented_train_dataset=unaugmented_split_dataset.train,\n",
    "        ood_dataset=original_ood_dataset,\n",
    "        validation_dataset=split_dataset.validation,\n",
    "        test_dataset=split_dataset.test,\n",
    "        evaluation_dataset=evaluation_dataset,\n",
    "        initial_training_set_indices=initial_training_set_indices,\n",
    "        evaluation_set_indices=evaluation_set_indices,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OodExperiment:\n",
    "    seed: int\n",
    "    uniform_ood: bool\n",
    "\n",
    "    id_dataset_name: str\n",
    "    ood_dataset_name: str\n",
    "    initial_training_set_size: int = 0\n",
    "    validation_set_size: int = 1024\n",
    "    evaluation_set_size: int = 1024\n",
    "    id_repetitions: float = 1\n",
    "    ood_repetitions: float = 1\n",
    "    add_dataset_noise: bool = False\n",
    "    validation_split_random_state: int = 0\n",
    "\n",
    "    acquisition_size: int = 5\n",
    "    max_training_set: int = 200\n",
    "    num_pool_samples: int = 20\n",
    "    num_validation_samples: int = 20\n",
    "    num_training_samples: int = 1\n",
    "    num_patience_epochs: int = 3\n",
    "    max_training_epochs: int = 60\n",
    "    training_batch_size: int = 64\n",
    "    device: str = \"cuda\"\n",
    "    min_samples_per_epoch: int = 5056\n",
    "    acquisition_function: Union[\n",
    "        Type[CandidateBatchComputer], Type[EvalCandidateBatchComputer]\n",
    "    ] = acquisition_functions.BALD\n",
    "    train_eval_model: Type[TrainEvalModel] = TrainSelfDistillationEvalModel\n",
    "    model_optimizer_factory: Type[ModelOptimizerFactory] = Cifar10BayesianResnetFactory\n",
    "    acquisition_function_args: dict = None\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    def load_experiment_data(self) -> ExperimentData:\n",
    "        di = DependencyInjection(vars(self))\n",
    "        edc: ExperimentDataConfig = di.create_dataclass_type(ExperimentDataConfig)\n",
    "        return edc.load()\n",
    "\n",
    "    # Simple Dependency Injection\n",
    "    def create_acquisition_function(self):\n",
    "        di = DependencyInjection(vars(self))\n",
    "        return di.create_dataclass_type(self.acquisition_function)\n",
    "\n",
    "    def create_train_eval_model(self, runtime_config) -> TrainEvalModel:\n",
    "        config = {**vars(self), **runtime_config}\n",
    "        di = DependencyInjection(config, [])\n",
    "        return di.create_dataclass_type(self.train_eval_model)\n",
    "\n",
    "    def run(self, store):\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # Active Learning setup\n",
    "        data = self.load_experiment_data()\n",
    "        store[\"dataset_info\"] = dict(training=repr(data.active_learning.base_dataset), test=repr(data.test_dataset))\n",
    "        store[\"initial_training_set_indices\"] = data.initial_training_set_indices\n",
    "        store[\"evaluation_set_indices\"] = data.evaluation_set_indices\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            data.active_learning.training_dataset,\n",
    "            batch_size=self.training_batch_size,\n",
    "            sampler=RandomFixedLengthSampler(data.active_learning.training_dataset, self.min_samples_per_epoch),\n",
    "            drop_last=True,\n",
    "        )\n",
    "        pool_loader = torch.utils.data.DataLoader(\n",
    "            data.active_learning.pool_dataset, batch_size=128, drop_last=False, shuffle=False\n",
    "        )\n",
    "\n",
    "        validation_loader = torch.utils.data.DataLoader(data.validation_dataset, batch_size=512, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(data.test_dataset, batch_size=512, drop_last=False)\n",
    "\n",
    "        store[\"active_learning_steps\"] = []\n",
    "        active_learning_steps = store[\"active_learning_steps\"]\n",
    "\n",
    "        acquisition_function = self.create_acquisition_function()\n",
    "\n",
    "        num_iterations = 0\n",
    "        max_iterations = int(1.5 * (self.max_training_set - self.initial_training_set_size) / self.acquisition_size)\n",
    "\n",
    "        # Active Training Loop\n",
    "        while True:\n",
    "            training_set_size = len(data.active_learning.training_dataset)\n",
    "            print(f\"Training set size {training_set_size}:\")\n",
    "\n",
    "            # iteration_log = dict(training={}, pool_training={}, evaluation_metrics=None, acquisition=None)\n",
    "            active_learning_steps.append({})\n",
    "            iteration_log = active_learning_steps[-1]\n",
    "\n",
    "            iteration_log[\"training\"] = {}\n",
    "\n",
    "            model_optimizer = self.model_optimizer_factory().create_model_optimizer()\n",
    "\n",
    "            if self.uniform_ood:\n",
    "                loss = torch.nn.KLDivLoss(log_target=False, reduction=\"batchmean\")\n",
    "                validation_loss = torch.nn.NLLLoss()\n",
    "            else:\n",
    "                loss = validation_loss = torch.nn.NLLLoss()\n",
    "                \n",
    "            data.toggle_augmentations(True)\n",
    "\n",
    "            train(\n",
    "                model=model_optimizer.model,\n",
    "                optimizer=model_optimizer.optimizer,\n",
    "                training_samples=self.num_training_samples,\n",
    "                validation_samples=self.num_validation_samples,\n",
    "                train_loader=train_loader,\n",
    "                validation_loader=validation_loader,\n",
    "                patience=self.num_patience_epochs,\n",
    "                max_epochs=self.max_training_epochs,\n",
    "                device=self.device,\n",
    "                training_log=iteration_log[\"training\"],\n",
    "                loss=loss,\n",
    "                validation_loss=validation_loss,\n",
    "            )\n",
    "            \n",
    "            data.toggle_augmentations(False)\n",
    "\n",
    "            evaluation_metrics = evaluate(\n",
    "                model=model_optimizer.model,\n",
    "                num_samples=self.num_validation_samples,\n",
    "                loader=test_loader,\n",
    "                device=self.device,\n",
    "            )\n",
    "            iteration_log[\"evaluation_metrics\"] = evaluation_metrics\n",
    "            print(f\"Perf after training {evaluation_metrics}\")\n",
    "\n",
    "            if training_set_size >= self.max_training_set or num_iterations >= max_iterations:\n",
    "                print(\"Done.\")\n",
    "                break\n",
    "\n",
    "            trained_model = TrainedMCDropoutModel(num_samples=self.num_pool_samples, model=model_optimizer.model)\n",
    "\n",
    "            if isinstance(acquisition_function, CandidateBatchComputer):\n",
    "                candidate_batch = acquisition_function.compute_candidate_batch(trained_model, pool_loader, self.device)\n",
    "            elif isinstance(acquisition_function, EvalCandidateBatchComputer):\n",
    "                current_max_epochs = iteration_log[\"training\"][\"best_epoch\"]\n",
    "\n",
    "                if self.evaluation_set_size:\n",
    "                    eval_dataset = data.evaluation_dataset\n",
    "                else:\n",
    "                    eval_dataset = data.active_learning.pool_dataset\n",
    "\n",
    "                train_eval_model = self.create_train_eval_model(\n",
    "                    dict(\n",
    "                        max_epochs=current_max_epochs + 2,\n",
    "                        training_dataset=data.active_learning.training_dataset,\n",
    "                        eval_dataset=eval_dataset,\n",
    "                        validation_loader=validation_loader,\n",
    "                        trained_model=trained_model,\n",
    "                        data=data,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                iteration_log[\"eval_training\"] = {}\n",
    "                trained_eval_model = train_eval_model(training_log=iteration_log[\"eval_training\"], device=self.device)\n",
    "\n",
    "                candidate_batch = acquisition_function.compute_candidate_batch(\n",
    "                    trained_model, trained_eval_model, pool_loader, device=self.device\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown acquisition function {acquisition_function}!\")\n",
    "\n",
    "            candidate_global_dataset_indices = []\n",
    "            candidate_labels = []\n",
    "            for index in candidate_batch.indices:\n",
    "                base_di = get_base_dataset_index(data.active_learning.pool_dataset, index)\n",
    "                dataset_type = \"ood\" if base_di.dataset == data.ood_dataset else \"id\"\n",
    "                candidate_global_dataset_indices.append((dataset_type, base_di.index))\n",
    "                label = get_target(data.active_learning.pool_dataset, index)\n",
    "                candidate_labels.append(label)\n",
    "\n",
    "            iteration_log[\"acquisition\"] = dict(\n",
    "                indices=candidate_global_dataset_indices, labels=candidate_labels, scores=candidate_batch.scores\n",
    "            )\n",
    "\n",
    "            print(candidate_batch)\n",
    "            print(candidate_global_dataset_indices)\n",
    "\n",
    "            if self.uniform_ood:\n",
    "                data.active_learning.acquire(candidate_batch.indices)\n",
    "            else:\n",
    "                data.active_learning.acquire(\n",
    "                    [index for index, label in zip(candidate_batch.indices, candidate_labels) if label != -1]\n",
    "                )\n",
    "\n",
    "            ls = \", \".join(f\"{label} ({score:.4})\" for label, score in zip(candidate_labels, candidate_batch.scores))\n",
    "            print(f\"Acquiring (label, score)s: {ls}\")\n",
    "\n",
    "            num_iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=True,\n",
      "\tid_dataset_name=CIFAR-10,\n",
      "\tood_dataset_name=SVHN,\n",
      "\tinitial_training_set_size=20,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=100,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: datadata/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datadata/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: datadata/SVHN/test_32x32.mat\n",
      "Creating: EvalBALD(\n",
      "\tacquisition_size=10\n",
      ")\n",
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b545beb2aa7c49c6b59639b751bb57b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.162109375, 'crossentropy': 8.790198802947998}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.162109375)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.1597, 'crossentropy': 9.115346867370606}\n",
      "Creating: TrainSelfDistillationEvalModel(\n",
      "\tnum_pool_samples=2,\n",
      "\tnum_training_samples=1,\n",
      "\tnum_validation_samples=20,\n",
      "\tnum_patience_epochs=3,\n",
      "\tmax_epochs=3,\n",
      "\ttraining_dataset=<torch.utils.data.dataset.Subset object at 0x7f46de6e0ee0>,\n",
      "\teval_dataset=Evaluation Set (100 samples),\n",
      "\tvalidation_loader=<torch.utils.data.dataloader.DataLoader object at 0x7f46b90485b0>,\n",
      "\ttraining_batch_size=64,\n",
      "\tmodel_optimizer_factory=<class 'batchbald_redux.resnet_models.Cifar10BayesianResnetFactory'>,\n",
      "\ttrained_model=TrainedMCDropoutModel(num_samples=2, model=BayesianResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): Identity()\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): ConsistentMCDropout(p=0.5)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConsistentMCDropout(p=0.5)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")),\n",
      "\tmin_samples_per_epoch=5056,\n",
      "\tdata=ExperimentData(active_learning=<batchbald_redux.active_learning.ActiveLearningData object at 0x7f46de6e0fd0>, augmentation_node=Augmentation Node, unaugmented_train_dataset='CIFAR-10 (Train, seed=0, 48976 samples)', augmented_train_dataset='CIFAR-10 (Train, seed=0, 48976 samples)', ood_dataset='SVHN (Train, seed=0, 73257 samples)', validation_dataset='CIFAR-10 (Validation, seed=0, 1024 samples)', test_dataset='CIFAR-10 (Test)', evaluation_dataset=Evaluation Set (100 samples), initial_training_set_indices=[12980, 44617, 6984, 21168, 33976, 35571, 33058, 43729, 26944, 24745, 66, 14046, 46542, 39478, 6000, 5915, 39360, 20774, 27084, 44464], evaluation_set_indices=[3812, 42704, 6729, 38942, 48125, 16968, 5652, 4045, 10740, 19606, 37164, 33354, 47307, 17878, 26665, 40819, 14805, 201, 47956, 44739, 15578, 36667, 5551, 23088, 32496, 5705, 23255, 25559, 11975, 44032, 47518, 36303, 18452, 34447, 24821, 36157, 48089, 25120, 44689, 6509, 11001, 6995, 10899, 36881, 7002, 19049, 13388, 40737, 9210, 22684, 45656, 5604, 9134, 35979, 19757, 43627, 35248, 23566, 727, 34909, 25443, 45862, 30730, 9611, 43077, 23902, 9541, 38859, 13973, 27923, 21547, 47739, 13909, 624, 25621, 30173, 37807, 7053, 30625, 10450, 23760, 26549, 2676, 11659, 40405, 46217, 5207, 28673, 12645, 45982, 3184, 4581, 16953, 7585, 45975, 28441, 10956, 22017, 21698, 2107])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aba708405284af88f10f0ccff5cab9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 33%|###3      | 1/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.173828125, 'crossentropy': 4.390122175216675}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.1728515625, 'crossentropy': 4.177756309509277}\n",
      "RestoringEarlyStopping: 1 / 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.154296875, 'crossentropy': 4.4122474193573}\n",
      "RestoringEarlyStopping: 2 / 3\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.173828125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/244226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/244226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/122113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/122113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/122113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/122113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateBatch(scores=[0.6887184884399176, 0.6886594989337027, 0.6886320922058076, 0.6885819195304066, 0.6885694703087211, 0.6884773476049304, 0.688474940136075, 0.688468998298049, 0.6884681128431112, 0.6884459056891501], indices=[66291, 92281, 109464, 52707, 105309, 65997, 91872, 114049, 52706, 82181])\n",
      "[('ood', 17435), ('ood', 43425), ('ood', 60608), ('ood', 3851), ('ood', 56453), ('ood', 17141), ('ood', 43016), ('ood', 65193), ('ood', 3850), ('ood', 33325)]\n",
      "Acquiring (label, score)s: tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6887), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6887), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6886), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6886), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6886), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6885), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6885), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6885), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6885), tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], device='cuda:0') (0.6884)\n",
      "Training set size 30:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c97b352e614180b2cd13034edcd3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.1845703125, 'crossentropy': 9.042139053344727}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.1845703125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.1822, 'crossentropy': 9.2006466796875}\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(Augmentation Node | one_hot_targets{'num_classes': 10}) + ('SVHN (Train, seed=0, 73257 samples)' | uniform_targets{'num_classes': 10})\",\n",
       "  'test': \"'CIFAR-10 (Test)'\"},\n",
       " 'initial_training_set_indices': [12980,\n",
       "  44617,\n",
       "  6984,\n",
       "  21168,\n",
       "  33976,\n",
       "  35571,\n",
       "  33058,\n",
       "  43729,\n",
       "  26944,\n",
       "  24745,\n",
       "  66,\n",
       "  14046,\n",
       "  46542,\n",
       "  39478,\n",
       "  6000,\n",
       "  5915,\n",
       "  39360,\n",
       "  20774,\n",
       "  27084,\n",
       "  44464],\n",
       " 'evaluation_set_indices': [3812,\n",
       "  42704,\n",
       "  6729,\n",
       "  38942,\n",
       "  48125,\n",
       "  16968,\n",
       "  5652,\n",
       "  4045,\n",
       "  10740,\n",
       "  19606,\n",
       "  37164,\n",
       "  33354,\n",
       "  47307,\n",
       "  17878,\n",
       "  26665,\n",
       "  40819,\n",
       "  14805,\n",
       "  201,\n",
       "  47956,\n",
       "  44739,\n",
       "  15578,\n",
       "  36667,\n",
       "  5551,\n",
       "  23088,\n",
       "  32496,\n",
       "  5705,\n",
       "  23255,\n",
       "  25559,\n",
       "  11975,\n",
       "  44032,\n",
       "  47518,\n",
       "  36303,\n",
       "  18452,\n",
       "  34447,\n",
       "  24821,\n",
       "  36157,\n",
       "  48089,\n",
       "  25120,\n",
       "  44689,\n",
       "  6509,\n",
       "  11001,\n",
       "  6995,\n",
       "  10899,\n",
       "  36881,\n",
       "  7002,\n",
       "  19049,\n",
       "  13388,\n",
       "  40737,\n",
       "  9210,\n",
       "  22684,\n",
       "  45656,\n",
       "  5604,\n",
       "  9134,\n",
       "  35979,\n",
       "  19757,\n",
       "  43627,\n",
       "  35248,\n",
       "  23566,\n",
       "  727,\n",
       "  34909,\n",
       "  25443,\n",
       "  45862,\n",
       "  30730,\n",
       "  9611,\n",
       "  43077,\n",
       "  23902,\n",
       "  9541,\n",
       "  38859,\n",
       "  13973,\n",
       "  27923,\n",
       "  21547,\n",
       "  47739,\n",
       "  13909,\n",
       "  624,\n",
       "  25621,\n",
       "  30173,\n",
       "  37807,\n",
       "  7053,\n",
       "  30625,\n",
       "  10450,\n",
       "  23760,\n",
       "  26549,\n",
       "  2676,\n",
       "  11659,\n",
       "  40405,\n",
       "  46217,\n",
       "  5207,\n",
       "  28673,\n",
       "  12645,\n",
       "  45982,\n",
       "  3184,\n",
       "  4581,\n",
       "  16953,\n",
       "  7585,\n",
       "  45975,\n",
       "  28441,\n",
       "  10956,\n",
       "  22017,\n",
       "  21698,\n",
       "  2107],\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.162109375,\n",
       "      'crossentropy': 8.790198802947998}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.1597,\n",
       "    'crossentropy': 9.115346867370606},\n",
       "   'eval_training': {'epochs': [{'accuracy': 0.173828125,\n",
       "      'crossentropy': 4.390122175216675},\n",
       "     {'accuracy': 0.1728515625, 'crossentropy': 4.177756309509277},\n",
       "     {'accuracy': 0.154296875, 'crossentropy': 4.4122474193573}],\n",
       "    'best_epoch': 1},\n",
       "   'acquisition': {'indices': [('ood', 17435),\n",
       "     ('ood', 43425),\n",
       "     ('ood', 60608),\n",
       "     ('ood', 3851),\n",
       "     ('ood', 56453),\n",
       "     ('ood', 17141),\n",
       "     ('ood', 43016),\n",
       "     ('ood', 65193),\n",
       "     ('ood', 3850),\n",
       "     ('ood', 33325)],\n",
       "    'labels': [tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0'),\n",
       "     tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "             0.1000], device='cuda:0')],\n",
       "    'scores': [0.6887184884399176,\n",
       "     0.6886594989337027,\n",
       "     0.6886320922058076,\n",
       "     0.6885819195304066,\n",
       "     0.6885694703087211,\n",
       "     0.6884773476049304,\n",
       "     0.688474940136075,\n",
       "     0.688468998298049,\n",
       "     0.6884681128431112,\n",
       "     0.6884459056891501]}},\n",
       "  {'training': {'epochs': [{'accuracy': 0.1845703125,\n",
       "      'crossentropy': 9.042139053344727}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.1822,\n",
       "    'crossentropy': 9.2006466796875}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = OodExperiment(\n",
    "    uniform_ood=True,\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    ood_dataset_name=\"SVHN\",\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.EvalBALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=False,\n",
      "\tid_dataset_name=SVHN,\n",
      "\tood_dataset_name=CIFAR-10,\n",
      "\tinitial_training_set_size=0,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=1024,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cpu\n",
      ")\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(Augmentation Node) + ('CIFAR-10 (Train, seed=0, 50000 samples)' | constant_target{'target': tensor(-1), 'num_classes': 10})\n",
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=True,\n",
      "\tid_dataset_name=CIFAR-10,\n",
      "\tood_dataset_name=SVHN,\n",
      "\tinitial_training_set_size=0,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=1024,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cpu\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n",
      "(Augmentation Node | one_hot_targets{'num_classes': 10}) + ('SVHN (Train, seed=0, 73257 samples)' | uniform_targets{'num_classes': 10})\n"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "print(\n",
    "    OodExperiment(seed=0, device=\"cpu\", id_dataset_name=\"SVHN\", ood_dataset_name=\"CIFAR-10\", uniform_ood=False)\n",
    "    .load_experiment_data()\n",
    "    .active_learning.base_dataset\n",
    ")\n",
    "print(\n",
    "    OodExperiment(seed=1, device=\"cpu\", id_dataset_name=\"CIFAR-10\", ood_dataset_name=\"SVHN\", uniform_ood=True)\n",
    "    .load_experiment_data()\n",
    "    .active_learning.base_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(Augmentation Node | one_hot_targets{'num_classes': 10}) + ('SVHN (Train, seed=0, 73257 samples)' | uniform_targets{'num_classes': 10})\",\n",
       "  'test': \"'CIFAR-10 (Test)'\"},\n",
       " 'initial_training_set_indices': [12980,\n",
       "  44617,\n",
       "  6984,\n",
       "  21168,\n",
       "  33976,\n",
       "  35571,\n",
       "  33058,\n",
       "  43729,\n",
       "  26944,\n",
       "  24745,\n",
       "  66,\n",
       "  14046,\n",
       "  46542,\n",
       "  39478,\n",
       "  6000,\n",
       "  5915,\n",
       "  39360,\n",
       "  20774,\n",
       "  27084,\n",
       "  44464],\n",
       " 'evaluation_set_indices': [3812,\n",
       "  42704,\n",
       "  6729,\n",
       "  38942,\n",
       "  48125,\n",
       "  16968,\n",
       "  5652,\n",
       "  4045,\n",
       "  10740,\n",
       "  19606,\n",
       "  37164,\n",
       "  33354,\n",
       "  47307,\n",
       "  17878,\n",
       "  26665,\n",
       "  40819,\n",
       "  14805,\n",
       "  201,\n",
       "  47956,\n",
       "  44739,\n",
       "  15578,\n",
       "  36667,\n",
       "  5551,\n",
       "  23088,\n",
       "  32496,\n",
       "  5705,\n",
       "  23255,\n",
       "  25559,\n",
       "  11975,\n",
       "  44032,\n",
       "  47518,\n",
       "  36303,\n",
       "  18452,\n",
       "  34447,\n",
       "  24821,\n",
       "  36157,\n",
       "  48089,\n",
       "  25120,\n",
       "  44689,\n",
       "  6509,\n",
       "  11001,\n",
       "  6995,\n",
       "  10899,\n",
       "  36881,\n",
       "  7002,\n",
       "  19049,\n",
       "  13388,\n",
       "  40737,\n",
       "  9210,\n",
       "  22684,\n",
       "  45656,\n",
       "  5604,\n",
       "  9134,\n",
       "  35979,\n",
       "  19757,\n",
       "  43627,\n",
       "  35248,\n",
       "  23566,\n",
       "  727,\n",
       "  34909,\n",
       "  25443,\n",
       "  45862,\n",
       "  30730,\n",
       "  9611,\n",
       "  43077,\n",
       "  23902,\n",
       "  9541,\n",
       "  38859,\n",
       "  13973,\n",
       "  27923,\n",
       "  21547,\n",
       "  47739,\n",
       "  13909,\n",
       "  624,\n",
       "  25621,\n",
       "  30173,\n",
       "  37807,\n",
       "  7053,\n",
       "  30625,\n",
       "  10450,\n",
       "  23760,\n",
       "  26549,\n",
       "  2676,\n",
       "  11659,\n",
       "  40405,\n",
       "  46217,\n",
       "  5207,\n",
       "  28673,\n",
       "  12645,\n",
       "  45982,\n",
       "  3184,\n",
       "  4581,\n",
       "  16953,\n",
       "  7585,\n",
       "  45975,\n",
       "  28441,\n",
       "  10956,\n",
       "  22017,\n",
       "  21698,\n",
       "  2107],\n",
       " 'active_learning_steps': [{'training': {'epochs': []}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=False,\n",
      "\tid_dataset_type=<class 'batchbald_redux.fast_mnist.FastMNIST'>,\n",
      "\tood_dataset_type=<class 'batchbald_redux.fast_mnist.FastFashionMNIST'>,\n",
      "\tinitial_training_set_size=20,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=100,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Creating: BALD(\n",
      "\tacquisition_size=10\n",
      ")\n",
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b9fac304cd48d0b525701825b08f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.6328125, 'crossentropy': 2.2057557106018066}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.6328125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6557, 'crossentropy': 2.067915026473999}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/237712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateBatch(scores=[0.6930213868618011, 0.6930111590772867, 0.6930059990845621, 0.6929988877382129, 0.6929858811199665, 0.69298055768013, 0.6929786503314972, 0.6929723918437958, 0.6929657161235809, 0.6929287277162075], indices=[37754, 20802, 5448, 184, 51966, 13296, 26425, 46199, 21745, 4910])\n",
      "[('id', 46785), ('id', 36269), ('id', 47902), ('id', 26859), ('id', 58402), ('id', 34634), ('id', 46751), ('id', 46535), ('id', 21910), ('id', 5884)]\n",
      "Acquiring (label, score)s: 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.693), 0 (0.6929)\n",
      "Training set size 30:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a259e542feb48079e2e0afcebfe22f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.611328125, 'crossentropy': 2.4593361616134644}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.611328125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6549, 'crossentropy': 2.1096548225402834}\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(FastMNIST (train; 58976 samples)) + ('FastFashionMNIST Train (60000 samples)' | constant_target{'target': tensor(-1, device='cuda:0'), 'num_classes': 10})\",\n",
       "  'test': \"'FastMNIST Test (10000 samples)'\"},\n",
       " 'initial_training_set_indices': [53434,\n",
       "  8533,\n",
       "  14640,\n",
       "  39579,\n",
       "  30392,\n",
       "  58125,\n",
       "  37915,\n",
       "  3091,\n",
       "  57520,\n",
       "  43803,\n",
       "  44119,\n",
       "  52296,\n",
       "  58226,\n",
       "  40334,\n",
       "  46037,\n",
       "  22015,\n",
       "  22304,\n",
       "  43812,\n",
       "  12640,\n",
       "  53689],\n",
       " 'evaluation_set_indices': [29974,\n",
       "  55573,\n",
       "  35472,\n",
       "  44048,\n",
       "  48031,\n",
       "  5616,\n",
       "  10110,\n",
       "  47420,\n",
       "  56990,\n",
       "  34198,\n",
       "  3792,\n",
       "  5715,\n",
       "  15969,\n",
       "  32775,\n",
       "  19757,\n",
       "  34588,\n",
       "  28991,\n",
       "  47417,\n",
       "  26501,\n",
       "  12108,\n",
       "  5573,\n",
       "  48032,\n",
       "  40646,\n",
       "  43252,\n",
       "  2404,\n",
       "  36797,\n",
       "  29079,\n",
       "  40018,\n",
       "  37047,\n",
       "  41512,\n",
       "  45567,\n",
       "  801,\n",
       "  10664,\n",
       "  52801,\n",
       "  42890,\n",
       "  32972,\n",
       "  45974,\n",
       "  20801,\n",
       "  23496,\n",
       "  5803,\n",
       "  10508,\n",
       "  46870,\n",
       "  49549,\n",
       "  306,\n",
       "  38725,\n",
       "  13074,\n",
       "  19689,\n",
       "  27135,\n",
       "  16068,\n",
       "  18137,\n",
       "  2728,\n",
       "  43321,\n",
       "  29950,\n",
       "  380,\n",
       "  27254,\n",
       "  50466,\n",
       "  31965,\n",
       "  24052,\n",
       "  44454,\n",
       "  20076,\n",
       "  21423,\n",
       "  58741,\n",
       "  27145,\n",
       "  38430,\n",
       "  37354,\n",
       "  49986,\n",
       "  4321,\n",
       "  12610,\n",
       "  34482,\n",
       "  35794,\n",
       "  396,\n",
       "  50036,\n",
       "  46861,\n",
       "  57811,\n",
       "  53831,\n",
       "  49304,\n",
       "  51555,\n",
       "  29614,\n",
       "  767,\n",
       "  23451,\n",
       "  49512,\n",
       "  26479,\n",
       "  50997,\n",
       "  1774,\n",
       "  44803,\n",
       "  55187,\n",
       "  30013,\n",
       "  33736,\n",
       "  49169,\n",
       "  46464,\n",
       "  31444,\n",
       "  52440,\n",
       "  33486,\n",
       "  2206,\n",
       "  15675,\n",
       "  54426,\n",
       "  9574,\n",
       "  54012,\n",
       "  28833,\n",
       "  44428],\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.6328125,\n",
       "      'crossentropy': 2.2057557106018066}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6557,\n",
       "    'crossentropy': 2.067915026473999},\n",
       "   'acquisition': {'indices': [('id', 46785),\n",
       "     ('id', 36269),\n",
       "     ('id', 47902),\n",
       "     ('id', 26859),\n",
       "     ('id', 58402),\n",
       "     ('id', 34634),\n",
       "     ('id', 46751),\n",
       "     ('id', 46535),\n",
       "     ('id', 21910),\n",
       "     ('id', 5884)],\n",
       "    'labels': [tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0'),\n",
       "     tensor(0, device='cuda:0')],\n",
       "    'scores': [0.6930213868618011,\n",
       "     0.6930111590772867,\n",
       "     0.6930059990845621,\n",
       "     0.6929988877382129,\n",
       "     0.6929858811199665,\n",
       "     0.69298055768013,\n",
       "     0.6929786503314972,\n",
       "     0.6929723918437958,\n",
       "     0.6929657161235809,\n",
       "     0.6929287277162075]}},\n",
       "  {'training': {'epochs': [{'accuracy': 0.611328125,\n",
       "      'crossentropy': 2.4593361616134644}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6549,\n",
       "    'crossentropy': 2.1096548225402834}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "experiment = OodExperiment(\n",
    "    uniform_ood=False,\n",
    "    id_dataset_type=FastMNIST,\n",
    "    ood_dataset=FastFashionMNIST,\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.BALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    temperature=5,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: ExperimentDataConfig(\n",
      "\tuniform_ood=False,\n",
      "\tid_dataset_type=<class 'batchbald_redux.fast_mnist.FastMNIST'>,\n",
      "\tood_dataset_type=<class 'batchbald_redux.fast_mnist.FastFashionMNIST'>,\n",
      "\tinitial_training_set_size=20,\n",
      "\tvalidation_set_size=1024,\n",
      "\tevaluation_set_size=100,\n",
      "\tid_repetitions=1,\n",
      "\tood_repetitions=1,\n",
      "\tadd_dataset_noise=False,\n",
      "\tvalidation_split_random_state=0,\n",
      "\tdevice=cuda\n",
      ")\n",
      "Creating: EvalBALD(\n",
      "\tacquisition_size=10\n",
      ")\n",
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b4c2912b734a5c9842167660acb396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.6328125, 'crossentropy': 2.1983988285064697}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.6328125)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6559, 'crossentropy': 2.0655145431518553}\n",
      "Creating: TrainSelfDistillationEvalModel(\n",
      "\tnum_pool_samples=2,\n",
      "\tnum_training_samples=1,\n",
      "\tnum_validation_samples=20,\n",
      "\tnum_patience_epochs=3,\n",
      "\tmax_epochs=3,\n",
      "\ttraining_dataset=<torch.utils.data.dataset.Subset object at 0x7fdc2a2f6520>,\n",
      "\teval_dataset=Evaluation Set (100 samples),\n",
      "\tvalidation_loader=<torch.utils.data.dataloader.DataLoader object at 0x7fdc2a2f6f40>,\n",
      "\ttraining_batch_size=64,\n",
      "\tmodel_optimizer_factory=<class 'batchbald_redux.models.MnistOptimizerFactory'>,\n",
      "\ttrained_model=TrainedMCDropoutModel(num_samples=2, model=BayesianMNISTCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1_drop): ConsistentMCDropout2d(p=0.5)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): ConsistentMCDropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (fc1_drop): ConsistentMCDropout(p=0.5)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")),\n",
      "\tmin_samples_per_epoch=5056\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9540e4c3d64037b94c9f83c58c0f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " 33%|###3      | 1/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.5966796875, 'crossentropy': 1.5679776072502136}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.5908203125, 'crossentropy': 1.7015655040740967}\n",
      "RestoringEarlyStopping: 1 / 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.5869140625, 'crossentropy': 1.7359371781349182}\n",
      "RestoringEarlyStopping: 2 / 3\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.5966796875)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/237712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/237712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/118856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateBatch(scores=[0.6806451752781868, 0.6790880858898163, 0.6763254124671221, 0.6758237481117249, 0.675022229552269, 0.6749783530831337, 0.6741648018360138, 0.6734838634729385, 0.6730879247188568, 0.6723472699522972], indices=[54855, 56792, 22122, 40241, 10674, 15235, 14226, 26593, 14211, 1276])\n",
      "[('id', 55525), ('id', 45346), ('id', 8446), ('id', 28278), ('id', 54369), ('id', 51180), ('id', 53366), ('id', 16103), ('id', 13247), ('id', 55629)]\n",
      "Acquiring (label, score)s: 2 (0.6806), 2 (0.6791), 2 (0.6763), 2 (0.6758), 9 (0.675), 7 (0.675), 2 (0.6742), 9 (0.6735), 9 (0.6731), 2 (0.6723)\n",
      "Training set size 30:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5332cfaf664d96a60a8995426d4f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/79]   1%|1          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch metrics: {'accuracy': 0.6474609375, 'crossentropy': 2.016913414001465}\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: 0.6474609375)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/20]   5%|5          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.6889, 'crossentropy': 1.9067923236846924}\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_info': {'training': \"(FastMNIST (train; 58976 samples)) + ('FastFashionMNIST Train (60000 samples)' | constant_target{'target': tensor(-1, device='cuda:0'), 'num_classes': 10})\",\n",
       "  'test': \"'FastMNIST Test (10000 samples)'\"},\n",
       " 'initial_training_set_indices': [53434,\n",
       "  8533,\n",
       "  14640,\n",
       "  39579,\n",
       "  30392,\n",
       "  58125,\n",
       "  37915,\n",
       "  3091,\n",
       "  57520,\n",
       "  43803,\n",
       "  44119,\n",
       "  52296,\n",
       "  58226,\n",
       "  40334,\n",
       "  46037,\n",
       "  22015,\n",
       "  22304,\n",
       "  43812,\n",
       "  12640,\n",
       "  53689],\n",
       " 'evaluation_set_indices': [29974,\n",
       "  55573,\n",
       "  35472,\n",
       "  44048,\n",
       "  48031,\n",
       "  5616,\n",
       "  10110,\n",
       "  47420,\n",
       "  56990,\n",
       "  34198,\n",
       "  3792,\n",
       "  5715,\n",
       "  15969,\n",
       "  32775,\n",
       "  19757,\n",
       "  34588,\n",
       "  28991,\n",
       "  47417,\n",
       "  26501,\n",
       "  12108,\n",
       "  5573,\n",
       "  48032,\n",
       "  40646,\n",
       "  43252,\n",
       "  2404,\n",
       "  36797,\n",
       "  29079,\n",
       "  40018,\n",
       "  37047,\n",
       "  41512,\n",
       "  45567,\n",
       "  801,\n",
       "  10664,\n",
       "  52801,\n",
       "  42890,\n",
       "  32972,\n",
       "  45974,\n",
       "  20801,\n",
       "  23496,\n",
       "  5803,\n",
       "  10508,\n",
       "  46870,\n",
       "  49549,\n",
       "  306,\n",
       "  38725,\n",
       "  13074,\n",
       "  19689,\n",
       "  27135,\n",
       "  16068,\n",
       "  18137,\n",
       "  2728,\n",
       "  43321,\n",
       "  29950,\n",
       "  380,\n",
       "  27254,\n",
       "  50466,\n",
       "  31965,\n",
       "  24052,\n",
       "  44454,\n",
       "  20076,\n",
       "  21423,\n",
       "  58741,\n",
       "  27145,\n",
       "  38430,\n",
       "  37354,\n",
       "  49986,\n",
       "  4321,\n",
       "  12610,\n",
       "  34482,\n",
       "  35794,\n",
       "  396,\n",
       "  50036,\n",
       "  46861,\n",
       "  57811,\n",
       "  53831,\n",
       "  49304,\n",
       "  51555,\n",
       "  29614,\n",
       "  767,\n",
       "  23451,\n",
       "  49512,\n",
       "  26479,\n",
       "  50997,\n",
       "  1774,\n",
       "  44803,\n",
       "  55187,\n",
       "  30013,\n",
       "  33736,\n",
       "  49169,\n",
       "  46464,\n",
       "  31444,\n",
       "  52440,\n",
       "  33486,\n",
       "  2206,\n",
       "  15675,\n",
       "  54426,\n",
       "  9574,\n",
       "  54012,\n",
       "  28833,\n",
       "  44428],\n",
       " 'active_learning_steps': [{'training': {'epochs': [{'accuracy': 0.6328125,\n",
       "      'crossentropy': 2.1983988285064697}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6559,\n",
       "    'crossentropy': 2.0655145431518553},\n",
       "   'eval_training': {'epochs': [{'accuracy': 0.5966796875,\n",
       "      'crossentropy': 1.5679776072502136},\n",
       "     {'accuracy': 0.5908203125, 'crossentropy': 1.7015655040740967},\n",
       "     {'accuracy': 0.5869140625, 'crossentropy': 1.7359371781349182}],\n",
       "    'best_epoch': 1},\n",
       "   'acquisition': {'indices': [('id', 55525),\n",
       "     ('id', 45346),\n",
       "     ('id', 8446),\n",
       "     ('id', 28278),\n",
       "     ('id', 54369),\n",
       "     ('id', 51180),\n",
       "     ('id', 53366),\n",
       "     ('id', 16103),\n",
       "     ('id', 13247),\n",
       "     ('id', 55629)],\n",
       "    'labels': [tensor(2, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(9, device='cuda:0'),\n",
       "     tensor(7, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0'),\n",
       "     tensor(9, device='cuda:0'),\n",
       "     tensor(9, device='cuda:0'),\n",
       "     tensor(2, device='cuda:0')],\n",
       "    'scores': [0.6806451752781868,\n",
       "     0.6790880858898163,\n",
       "     0.6763254124671221,\n",
       "     0.6758237481117249,\n",
       "     0.675022229552269,\n",
       "     0.6749783530831337,\n",
       "     0.6741648018360138,\n",
       "     0.6734838634729385,\n",
       "     0.6730879247188568,\n",
       "     0.6723472699522972]}},\n",
       "  {'training': {'epochs': [{'accuracy': 0.6474609375,\n",
       "      'crossentropy': 2.016913414001465}],\n",
       "    'best_epoch': 1},\n",
       "   'evaluation_metrics': {'accuracy': 0.6889,\n",
       "    'crossentropy': 1.9067923236846924}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = OodExperiment(\n",
    "    uniform_ood=False,\n",
    "    id_dataset_type=FastMNIST,\n",
    "    ood_dataset=FastFashionMNIST,\n",
    "    seed=1,\n",
    "    max_training_epochs=1,\n",
    "    max_training_set=20 + 10,\n",
    "    acquisition_function=acquisition_functions.EvalBALD,\n",
    "    evaluation_set_size=100,\n",
    "    acquisition_size=10,\n",
    "    num_pool_samples=2,\n",
    "    temperature=5,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "configs = [\n",
    "    OodExperiment(\n",
    "        seed=seed + 1234,\n",
    "        uniform_ood=uniform_ood,\n",
    "        acquisition_function=acquisition_function,\n",
    "        acquisition_size=5,\n",
    "        num_pool_samples=num_pool_samples,\n",
    "        evaluation_set_size=evaluation_set_size,\n",
    "        id_dataset_name=id_dataset_name,\n",
    "        ood_dataset_name=ood_dataset_name,\n",
    "    )\n",
    "    for seed in range(3)\n",
    "    for acquisition_function in [acquisition_functions.BatchEvalBALD, acquisition_functions.BatchBALD]\n",
    "    for evaluation_set_size in [1024]\n",
    "    for num_pool_samples in [100]\n",
    "    for uniform_ood in [True, False]\n",
    "    for id_dataset_name, ood_dataset_name in [(\"CIFAR-10\", \"SVHN\"), (\"SVHN\", \"CIFAR-10\")]\n",
    "]\n",
    "\n",
    "if not is_run_from_ipython() and __name__ == \"__main__\":\n",
    "    for job_id, store in embedded_experiments(__file__, len(configs)):\n",
    "        config = configs[job_id]\n",
    "        config.seed += job_id\n",
    "        print(config)\n",
    "        store[\"config\"] = dataclasses.asdict(config)\n",
    "        store[\"log\"] = {}\n",
    "\n",
    "        try:\n",
    "            config.run(store=store)\n",
    "        except Exception:\n",
    "            store[\"exception\"] = traceback.format_exc()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1234,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchEvalBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=True,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='CIFAR-10',\n",
      "        ood_dataset_name='SVHN',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    ),\n",
      "    OodExperiment(\n",
      "        seed=1235,\n",
      "        uniform_ood=False,\n",
      "        id_dataset_name='SVHN',\n",
      "        ood_dataset_name='CIFAR-10',\n",
      "        num_pool_samples=100,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.BatchBALD\n",
      "    )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "import prettyprinter\n",
    "\n",
    "prettyprinter.install_extras(include={\"dataclasses\"})\n",
    "\n",
    "prettyprinter.pprint(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
