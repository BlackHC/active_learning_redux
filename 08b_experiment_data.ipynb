{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appreciated-stephen",
   "metadata": {},
   "source": [
    "# Experiment Data (Setup)\n",
    "> No empirical experiments without data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-cedar",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from batchbald_redux.active_learning import ActiveLearningData\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    AdditiveGaussianNoise,\n",
    "    AliasDataset,\n",
    "    NamedDataset,\n",
    "    get_balanced_sample_indices_by_class,\n",
    ")\n",
    "from batchbald_redux.datasets import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentData:\n",
    "    active_learning: ActiveLearningData\n",
    "    validation_dataset: Dataset\n",
    "    evaluation_dataset: Dataset\n",
    "    test_dataset: Dataset\n",
    "\n",
    "    train_augmentations: nn.Module\n",
    "\n",
    "    initial_training_set_indices: List[int]\n",
    "    evaluation_set_indices: List[int]\n",
    "\n",
    "    ood_dataset: NamedDataset\n",
    "\n",
    "    device: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OoDDatasetConfig:\n",
    "    ood_dataset_name: str\n",
    "    ood_repetitions: float\n",
    "    ood_exposure: bool\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentDataConfig:\n",
    "    id_dataset_name: str\n",
    "    id_repetitions: float\n",
    "\n",
    "    initial_training_set_size: int\n",
    "\n",
    "    validation_set_size: int\n",
    "    validation_split_random_state: int\n",
    "\n",
    "    evaluation_set_size: int\n",
    "\n",
    "    add_dataset_noise: bool\n",
    "\n",
    "    ood_dataset_config: Optional[OoDDatasetConfig]\n",
    "\n",
    "    device: str\n",
    "\n",
    "    def load(self) -> ExperimentData:\n",
    "        return load_experiment_data(\n",
    "            id_dataset_name=self.id_dataset_name,\n",
    "            id_repetitions=self.id_repetitions,\n",
    "            initial_training_set_size=self.initial_training_set_size,\n",
    "            validation_set_size=self.validation_set_size,\n",
    "            validation_split_random_state=self.validation_split_random_state,\n",
    "            evaluation_set_size=self.evaluation_set_size,\n",
    "            add_dataset_noise=self.add_dataset_noise,\n",
    "            ood_dataset_config=self.ood_dataset_config,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "\n",
    "def load_experiment_data(\n",
    "    *,\n",
    "    id_dataset_name: str,\n",
    "    id_repetitions: float,\n",
    "    initial_training_set_size: int,\n",
    "    validation_set_size: int,\n",
    "    validation_split_random_state: int,\n",
    "    evaluation_set_size: int,\n",
    "    add_dataset_noise: bool,\n",
    "    ood_dataset_config: Optional[OoDDatasetConfig],\n",
    "    device: str,\n",
    ") -> ExperimentData:\n",
    "    split_dataset = get_dataset(\n",
    "        id_dataset_name,\n",
    "        root=\"data\",\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "        normalize_like_cifar10=True,\n",
    "        device_hint=device,\n",
    "    )\n",
    "\n",
    "    train_dataset = split_dataset.train\n",
    "\n",
    "    # TODO: add hook here to further process the train dataset?\n",
    "\n",
    "    # If we reduce the train set, we need to do so before picking the initial train set.\n",
    "    if id_repetitions < 1:\n",
    "        train_dataset = train_dataset * id_repetitions\n",
    "\n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    initial_samples_per_class = initial_training_set_size // num_classes\n",
    "    evaluation_set_samples_per_class = evaluation_set_size // num_classes\n",
    "    samples_per_class = initial_samples_per_class + evaluation_set_samples_per_class\n",
    "    balanced_samples_indices = get_balanced_sample_indices_by_class(\n",
    "        train_dataset,\n",
    "        num_classes=num_classes,\n",
    "        samples_per_class=samples_per_class,\n",
    "        seed=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    initial_training_set_indices = [\n",
    "        idx for by_class in balanced_samples_indices.values() for idx in by_class[:initial_samples_per_class]\n",
    "    ]\n",
    "    evaluation_set_indices = [\n",
    "        idx for by_class in balanced_samples_indices.values() for idx in by_class[initial_samples_per_class:]\n",
    "    ]\n",
    "\n",
    "    # If we over-sample the train set, we do so after picking the initial train set to avoid duplicates.\n",
    "    if id_repetitions > 1:\n",
    "        train_dataset = train_dataset * id_repetitions\n",
    "\n",
    "    if ood_dataset_config:\n",
    "        odd_split_dataset = get_dataset(ood_dataset_config.ood_dataset_name, root=\"data\", normalize_like_cifar10=True, device_hint=device)\n",
    "        assert split_dataset.device == odd_split_dataset.device, (f\"ID dataset resides on {split_dataset.device}, while OOD dataset is on {odd_split_dataset.device};\"\n",
    "                                                                  \"try to put both on \\\"cpu\\\"!\")\n",
    "        original_ood_dataset = odd_split_dataset.train\n",
    "        if ood_dataset_config.ood_exposure:\n",
    "            train_dataset = train_dataset.one_hot(device=split_dataset.device)\n",
    "            ood_dataset = original_ood_dataset.uniform_target(device=split_dataset.device, num_classes=train_dataset.get_num_classes())\n",
    "        else:\n",
    "            ood_dataset = original_ood_dataset.constant_target(\n",
    "                target=torch.tensor(-1, device=split_dataset.device), num_classes=train_dataset.get_num_classes()\n",
    "            )\n",
    "\n",
    "        if ood_dataset_config.ood_repetitions != 1:\n",
    "            ood_dataset = ood_dataset * ood_dataset_config.ood_repetitions\n",
    "\n",
    "        train_dataset = train_dataset + ood_dataset\n",
    "    else:\n",
    "        original_ood_dataset = None\n",
    "\n",
    "    if add_dataset_noise:\n",
    "        train_dataset = AdditiveGaussianNoise(train_dataset, 0.1)\n",
    "\n",
    "    active_learning_data = ActiveLearningData(train_dataset)\n",
    "\n",
    "    active_learning_data.acquire_base_indices(initial_training_set_indices)\n",
    "\n",
    "    evaluation_dataset = AliasDataset(\n",
    "        active_learning_data.extract_dataset_from_base_indices(evaluation_set_indices),\n",
    "        f\"Evaluation Set ({len(evaluation_set_indices)} samples)\",\n",
    "    )\n",
    "\n",
    "    return ExperimentData(\n",
    "        active_learning=active_learning_data,\n",
    "        validation_dataset=split_dataset.validation,\n",
    "        test_dataset=split_dataset.test,\n",
    "        evaluation_dataset=evaluation_dataset,\n",
    "        train_augmentations=split_dataset.train_augmentations,\n",
    "        initial_training_set_indices=initial_training_set_indices,\n",
    "        evaluation_set_indices=evaluation_set_indices,\n",
    "        ood_dataset=original_ood_dataset,\n",
    "        device=split_dataset.device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-migration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentData(active_learning=<batchbald_redux.active_learning.ActiveLearningData object at 0x7ff8483eaa60>, validation_dataset='MNIST (Validation, seed=0, 32 samples)', evaluation_dataset=Evaluation Set (10 samples), test_dataset='MNIST (Test)', train_augmentations=Sequential(), initial_training_set_indices=[46413, 55726, 25576, 55469, 39617, 35783, 36962, 56698, 4436, 24251, 27760, 7593, 15110, 21413, 31797, 42500, 34791, 46864, 47424, 57533], evaluation_set_indices=[43895, 47051, 56807, 13452, 39664, 38002, 53721, 37072, 18635, 52360], ood_dataset=None, device='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "load_experiment_data(\n",
    "    id_dataset_name=\"MNIST\",\n",
    "    initial_training_set_size=20,\n",
    "    validation_set_size=32,\n",
    "    evaluation_set_size=16,\n",
    "    id_repetitions=1.0,\n",
    "    add_dataset_noise=False,\n",
    "    validation_split_random_state=0,\n",
    "    ood_dataset_config=None,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExperimentData(active_learning=<batchbald_redux.active_learning.ActiveLearningData object at 0x7ff70101bf10>, validation_dataset='CIFAR-10 (Validation, seed=0, 32 samples)', evaluation_dataset=Evaluation Set (10 samples), test_dataset='CIFAR-10 (Test)', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "), initial_training_set_indices=[5618, 30732, 1910, 25225, 6409, 17895, 49063, 49577, 41071, 10377, 27423, 811, 27285, 22836, 26253, 5916, 49126, 40676, 31804, 13474], evaluation_set_indices=[36153, 11586, 36207, 16977, 1000, 10548, 11403, 2005, 41796, 25579], ood_dataset=None, device='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "load_experiment_data(\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    initial_training_set_size=20,\n",
    "    validation_set_size=32,\n",
    "    evaluation_set_size=16,\n",
    "    id_repetitions=1.0,\n",
    "    add_dataset_noise=False,\n",
    "    validation_split_random_state=0,\n",
    "    ood_dataset_config=None,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ID dataset resides on cpu, while OOD dataset is on cuda;try to put both on \"cpu\"!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37920/4122909440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m load_experiment_data(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mid_dataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CIFAR-10\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minitial_training_set_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37920/3863227137.py\u001b[0m in \u001b[0;36mload_experiment_data\u001b[0;34m(id_dataset_name, id_repetitions, initial_training_set_size, validation_set_size, validation_split_random_state, evaluation_set_size, add_dataset_noise, ood_dataset_config, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mood_dataset_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0modd_split_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mood_dataset_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mood_dataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_like_cifar10\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         assert split_dataset.device == odd_split_dataset.device, (f\"ID dataset resides on {split_dataset.device}, while OOD dataset is on {odd_split_dataset.device};\"\n\u001b[0m\u001b[1;32m    114\u001b[0m                                                                   \"try to put both on \\\"cpu\\\"!\")\n\u001b[1;32m    115\u001b[0m         \u001b[0moriginal_ood_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modd_split_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ID dataset resides on cpu, while OOD dataset is on cuda;try to put both on \"cpu\"!"
     ]
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "load_experiment_data(\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    initial_training_set_size=20,\n",
    "    validation_set_size=32,\n",
    "    evaluation_set_size=16,\n",
    "    id_repetitions=1.0,\n",
    "    add_dataset_noise=False,\n",
    "    validation_split_random_state=0,\n",
    "    ood_dataset_config=OoDDatasetConfig(ood_dataset_name=\"MNIST\", ood_repetitions=1., ood_exposure=False),\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# THIS OUGHT TO CRASH! BUT TELL YOU WHAT TO DO INSTEAD :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExperimentData(active_learning=<batchbald_redux.active_learning.ActiveLearningData object at 0x7ff6f58d5c40>, validation_dataset='CIFAR-10 (Validation, seed=0, 32 samples)', evaluation_dataset=Evaluation Set (10 samples), test_dataset='CIFAR-10 (Test)', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "), initial_training_set_indices=[5618, 30732, 1910, 25225, 6409, 17895, 49063, 49577, 41071, 10377, 27423, 811, 27285, 22836, 26253, 5916, 49126, 40676, 31804, 13474], evaluation_set_indices=[36153, 11586, 36207, 16977, 1000, 10548, 11403, 2005, 41796, 25579], ood_dataset='MNIST (Train, seed=0, 60000 samples)', device='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "load_experiment_data(\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    initial_training_set_size=20,\n",
    "    validation_set_size=32,\n",
    "    evaluation_set_size=16,\n",
    "    id_repetitions=1.0,\n",
    "    add_dataset_noise=False,\n",
    "    validation_split_random_state=0,\n",
    "    ood_dataset_config=OoDDatasetConfig(ood_dataset_name=\"MNIST\", ood_repetitions=1., ood_exposure=False),\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExperimentData(active_learning=<batchbald_redux.active_learning.ActiveLearningData object at 0x7f434cd5ca90>, validation_dataset='CIFAR-10 (Validation, seed=0, 32 samples)', evaluation_dataset=Evaluation Set (10 samples), test_dataset='CIFAR-10 (Test)', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "), initial_training_set_indices=[5618, 30732, 1910, 25225, 6409, 17895, 49063, 49577, 41071, 10377, 27423, 811, 27285, 22836, 26253, 5916, 49126, 40676, 31804, 13474], evaluation_set_indices=[36153, 11586, 36207, 16977, 1000, 10548, 11403, 2005, 41796, 25579], ood_dataset='CIFAR-100 (Train, seed=0, 50000 samples)')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "load_experiment_data(\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    initial_training_set_size=20,\n",
    "    validation_set_size=32,\n",
    "    evaluation_set_size=16,\n",
    "    id_repetitions=1.0,\n",
    "    add_dataset_noise=False,\n",
    "    validation_split_random_state=0,\n",
    "    ood_dataset_config=OoDDatasetConfig(ood_dataset_name=\"CIFAR-100\", ood_repetitions=1., ood_exposure=False),\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-northeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExperimentData(active_learning=<batchbald_redux.active_learning.ActiveLearningData object at 0x7f434db5f2e0>, validation_dataset='CIFAR-10 (Validation, seed=0, 32 samples)', evaluation_dataset=Evaluation Set (10 samples), test_dataset='CIFAR-10 (Test)', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "), initial_training_set_indices=[5618, 30732, 1910, 25225, 6409, 17895, 49063, 49577, 41071, 10377, 27423, 811, 27285, 22836, 26253, 5916, 49126, 40676, 31804, 13474], evaluation_set_indices=[36153, 11586, 36207, 16977, 1000, 10548, 11403, 2005, 41796, 25579], ood_dataset='CIFAR-100 (Train, seed=0, 50000 samples)')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "load_experiment_data(\n",
    "    id_dataset_name=\"CIFAR-10\",\n",
    "    initial_training_set_size=20,\n",
    "    validation_set_size=32,\n",
    "    evaluation_set_size=16,\n",
    "    id_repetitions=1.0,\n",
    "    add_dataset_noise=False,\n",
    "    validation_split_random_state=0,\n",
    "    ood_dataset_config=OoDDatasetConfig(ood_dataset_name=\"CIFAR-100\", ood_repetitions=1., ood_exposure=True),\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-finder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
