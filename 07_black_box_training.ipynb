{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Model Training\n",
    "> \"Learning is not attained by chance, it must be sought for with ardor and attended to with diligence.” \n",
    ">\n",
    "> &mdash; <cite>Abigail Adams</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at a simple example experiment, it is worth looking at the big picture. The big picture requires us to train different models with differently-sized datasets.\n",
    "\n",
    "We don't want to worry about fine-tuning training too much. Because we cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp black_box_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Log as much as possible by default.\n",
    "* Avoid magic numbers. Magic numbers don't work very well when everything keeps changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from ignite.contrib.engines.common import (\n",
    "    add_early_stopping_by_val_score,\n",
    "    setup_common_training_handlers,\n",
    ")\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from torch import nn\n",
    "\n",
    "from batchbald_redux.consistent_mc_dropout import (\n",
    "    GeometricMeanPrediction,\n",
    "    SamplerModel,\n",
    "    multi_sample_loss,\n",
    ")\n",
    "from batchbald_redux.restoring_early_stopping import RestoringEarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "\n",
    "def train(\n",
    "    *,\n",
    "    model,\n",
    "    training_samples,\n",
    "    validation_samples,\n",
    "    train_loader,\n",
    "    validation_loader,\n",
    "    patience: Optional[int],\n",
    "    max_epochs: int,\n",
    "    device: str,\n",
    "    training_log: dict,\n",
    "    loss=None,\n",
    "    validation_loss=None,\n",
    "    optimizer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    :param model:\n",
    "    :param train_loader:\n",
    "    :param val_loader:\n",
    "    :param metric_loader: We compute metrics for debugging and introspection purposes with this data.\n",
    "    :param patience: How many epochs to wait for early-stopping.\n",
    "    :param max_epochs:\n",
    "    :param tb_log_dir:\n",
    "    :param device:\n",
    "    :return: Optimizer that was used for training.\n",
    "    \"\"\"\n",
    "    if loss is None:\n",
    "        loss = nn.NLLLoss()\n",
    "    if validation_loss is None:\n",
    "        validation_loss = loss\n",
    "\n",
    "    train_model = SamplerModel(model, training_samples)\n",
    "    validation_model = GeometricMeanPrediction(SamplerModel(model, validation_samples))\n",
    "\n",
    "    # Move model to device before creating the optimizer\n",
    "    train_model.to(device)\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)\n",
    "\n",
    "    trainer = create_supervised_trainer(train_model, optimizer, multi_sample_loss(loss), device=device)\n",
    "\n",
    "    metrics = create_metrics(validation_loss)\n",
    "\n",
    "    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def compute_metrics(engine):\n",
    "        validation_evaluator.run(validation_loader)\n",
    "\n",
    "    # Only to look nicer.\n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, \"crossentropy\")\n",
    "\n",
    "    setup_common_training_handlers(trainer, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL)\n",
    "\n",
    "    ProgressBar(persist=False).attach(\n",
    "        validation_evaluator,\n",
    "        metric_names=\"all\",\n",
    "        event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),\n",
    "    )\n",
    "\n",
    "    training_log[\"epochs\"] = []\n",
    "    epochs_log = training_log[\"epochs\"]\n",
    "\n",
    "    # Logging\n",
    "    @validation_evaluator.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        metrics = dict(engine.state.metrics)\n",
    "        epochs_log.append(metrics)\n",
    "\n",
    "    # Add early stopping\n",
    "    if patience is not None:\n",
    "        early_stopping = RestoringEarlyStopping(\n",
    "            patience=patience,\n",
    "            score_function=lambda: float(-validation_evaluator.state.metrics[\"crossentropy\"]),\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            training_engine=trainer,\n",
    "            validation_engine=validation_evaluator,\n",
    "        )\n",
    "    else:\n",
    "        early_stopping = None\n",
    "\n",
    "    # Kick everything off\n",
    "    trainer.run(train_loader, max_epochs=max_epochs)\n",
    "\n",
    "    if early_stopping:\n",
    "        training_log[\"best_epoch\"] = early_stopping.best_epoch\n",
    "\n",
    "    # Return the optimizer in case we want to continue training.\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def evaluate(*, model, num_samples, loader, device, loss=None):\n",
    "    evaluation_model = GeometricMeanPrediction(SamplerModel(model, num_samples))\n",
    "\n",
    "    if loss is None:\n",
    "        loss = nn.NLLLoss()\n",
    "\n",
    "    metrics = create_metrics(loss)\n",
    "\n",
    "    evaluator = create_supervised_evaluator(evaluation_model, metrics=metrics, device=device)\n",
    "\n",
    "    ProgressBar(persist=False).attach(\n",
    "        evaluator,\n",
    "        metric_names=\"all\",\n",
    "        event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),\n",
    "    )\n",
    "\n",
    "    # Kick everything off\n",
    "    evaluator.run(loader, max_epochs=1)\n",
    "\n",
    "    return evaluator.state.metrics\n",
    "\n",
    "\n",
    "def create_metrics(loss):\n",
    "    return {\"accuracy\": Accuracy(), \"crossentropy\": Loss(loss)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use metrics that allow us to capture the quality of the produced uncertainty during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e38504d08d4b799e17c6e63f2c3d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: -0.11676170641109347)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epochs': [{'accuracy': 0.9723333333333334,\n",
       "   'crossentropy': 0.1676989432533582},\n",
       "  {'accuracy': 0.9791833333333333, 'crossentropy': 0.13145085282996297},\n",
       "  {'accuracy': 0.98265, 'crossentropy': 0.11676170641109347}],\n",
       " 'best_epoch': 3}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "import torch.utils.data\n",
    "\n",
    "from batchbald_redux.consistent_mc_dropout import GeometricMeanPrediction, SamplerModel\n",
    "from batchbald_redux.example_models import BayesianMNISTCNN\n",
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "from batchbald_redux.repeated_mnist import create_repeated_MNIST_dataset\n",
    "\n",
    "train_dataset, test_dataset = create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False)\n",
    "\n",
    "model = BayesianMNISTCNN()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, drop_last=False)\n",
    "\n",
    "training_log = {}\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    training_samples=1,\n",
    "    validation_samples=4,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=train_loader,\n",
    "    patience=3,\n",
    "    max_epochs=3,\n",
    "    device=\"cuda\",\n",
    "    training_log=training_log,\n",
    ")\n",
    "\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9844, 'crossentropy': 0.10267819431312382}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model=model, num_samples=4, loader=test_loader, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining predictions\n",
    "\n",
    "Sometimes, we want to obtain predictions from our models, instead of pure evaluation metrics... I know right?\n",
    "\n",
    "The following helper method registers an event handler with an Ignite Engine that stores the predictions in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def handler_save_predictions(engine, target_list):\n",
    "    @engine.on(Events.ITERATION_COMPLETED)\n",
    "    def iteration_completed(engine):\n",
    "        target_list.extend(engine.state.output[0])\n",
    "\n",
    "\n",
    "# TODO: ought to add support for toma here (and large k)\n",
    "def get_predictions(*, model, loader, device: str):\n",
    "    evaluator = create_supervised_evaluator(model, device=device)\n",
    "\n",
    "    predictions = []\n",
    "    handler_save_predictions(evaluator, predictions)\n",
    "\n",
    "    ProgressBar(persist=False).attach(\n",
    "        evaluator,\n",
    "        metric_names=\"all\",\n",
    "        event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),\n",
    "    )\n",
    "\n",
    "    evaluator.run(loader)\n",
    "\n",
    "    predictions = torch.stack(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment\n",
    "\n",
    "predictions = get_predictions(model=model, loader=test_loader, device=\"cuda\")\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%config Completer.jedi_compute_type_timeout = 10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
