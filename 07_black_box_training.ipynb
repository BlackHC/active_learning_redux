{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Model Training\n",
    "> No worries, no cry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at a simple example experiment, it is worth looking at the big picture. The big picture requires us to train different models with differently-sized datasets.\n",
    "\n",
    "We don't want to worry about fine-tuning training too much. Because we cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# default_exp black_box_model_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "* Log as much as possible by default.\n",
    "* Avoid magic numbers. Magic numbers don't work very well when everything keeps changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from ignite.contrib.engines.common import setup_common_training_handlers, \\\n",
    "    add_early_stopping_by_val_score\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#exports\n",
    "\n",
    "\n",
    "LOG_INTERVAL = 10\n",
    "HEAVY_LOG_INTERVAL = 100\n",
    "\n",
    "\n",
    "def train(*, model, train_loader, val_loader,\n",
    "          patience:int, max_epochs:int, device:str, epochs_log:list):\n",
    "    \"\"\"\n",
    "    :param model:\n",
    "    :param train_loader:\n",
    "    :param val_loader:\n",
    "    :param metric_loader: We compute metrics for debugging and introspection purposes with this data.\n",
    "    :param patience: How many epochs to wait for early-stopping.\n",
    "    :param max_epochs:\n",
    "    :param tb_log_dir:\n",
    "    :param device:\n",
    "    :return: Optimizer that was used for training.\n",
    "    \"\"\"\n",
    "    # Move model before creating optimizer\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "\n",
    "    metrics = create_metrics(criterion)\n",
    "\n",
    "    validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def compute_metrics(engine):\n",
    "        validation_evaluator.run(val_loader)\n",
    "        \n",
    "    RunningAverage(output_transform=lambda x: x).attach(trainer, 'crossentropy')\n",
    "        \n",
    "    setup_common_training_handlers(trainer, with_gpu_stats=True, log_every_iters=LOG_INTERVAL)\n",
    "        \n",
    "\n",
    "    ProgressBar(persist=False).attach(validation_evaluator, metric_names=\"all\",\n",
    "                                      event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL))\n",
    "    \n",
    "    @validation_evaluator.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        metrics = engine.state.metrics\n",
    "        epochs_log.append(metrics)\n",
    "        \n",
    "    # Add early stopping\n",
    "    add_early_stopping_by_val_score(patience, validation_evaluator, trainer, \"accuracy\")\n",
    "\n",
    "    # kick everything off\n",
    "    trainer.run(train_loader, max_epochs=max_epochs)\n",
    "\n",
    "    # Return the optimizer in case we want to continue training.\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def create_metrics(criterion):\n",
    "    return {\"accuracy\": Accuracy(), \"loss\": Loss(criterion)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We want to use metrics that allow us to capture the quality of the produced uncertainty during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b280ffe19e8c417382812b61360aa19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cf4c3dae2042afa1a9abb3d13d9052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9b41146494482db47787702357b0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fab1362a7a4f0fa3c806b9ed6446dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35642f2ffe0480b9c1054ebfc57bba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2309557e05b41c18755f05a63726afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee7ca000a04472c899b419c25efceb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=938.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.9493666666666667, 'loss': 0.17177074681284527},\n",
       " {'accuracy': 0.9616666666666667, 'loss': 0.12837001971403758},\n",
       " {'accuracy': 0.9665166666666667, 'loss': 0.11590606124342594}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "from batchbald_redux.consistent_mc_dropout import SamplerModel\n",
    "from batchbald_redux.example_models import BayesianMNISTCNN\n",
    "from batchbald_redux.repeated_mnist import create_repeated_MNIST_dataset\n",
    "import torch.utils.data\n",
    "\n",
    "train_dataset, test_dataset = create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False)\n",
    "\n",
    "model = SamplerModel(BayesianMNISTCNN(), 1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "epochs_log = []\n",
    "\n",
    "train(model=model, \n",
    "      train_loader=train_loader,\n",
    "      val_loader=train_loader, \n",
    "      patience=3, \n",
    "      max_epochs=3, \n",
    "      device=\"cuda\",\n",
    "      epochs_log=epochs_log)\n",
    "\n",
    "epochs_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining predictions\n",
    "\n",
    "Sometimes, we want to obtain predictions from our models, instead of pure evaluation metrics... I know right?\n",
    "\n",
    "The following helper method registers an event handler with an Ignite Engine that stores the predictions in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "\n",
    "def handler_save_predictions(engine, target_list):\n",
    "    @engine.on(Events.ITERATION_COMPLETED)\n",
    "    def iteration_completed(engine):\n",
    "        target_list.extend(engine.state.output[0])\n",
    "        \n",
    "def get_predictions(*, model, loader, device:str):\n",
    "    evaluator = create_supervised_evaluator(model, device=device)\n",
    "    \n",
    "    predictions = []\n",
    "    handler_save_predictions(evaluator, predictions)\n",
    "    \n",
    "    ProgressBar(persist=False).attach(evaluator, metric_names=\"all\",\n",
    "                                      event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL))\n",
    "    \n",
    "    evaluator.run(loader)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb35ce35dbc46979f2859546fc7a57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "predictions = get_predictions(model=model, loader=test_loader, device=\"cuda\")\n",
    "len(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
