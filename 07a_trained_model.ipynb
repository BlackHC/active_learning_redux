{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Model Interface\n",
    "> \"Why simple, when you can use design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch.nn\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from batchbald_redux.consistent_mc_dropout import BayesianModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainedModel:\n",
    "    \"\"\"Evaluate a trained model.\"\"\"\n",
    "\n",
    "    def get_log_probs_N_K_C_labels_N(self, loader: DataLoader, num_samples: int, device: object, storage_device: object, return_embedding: bool=False):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_log_probs_N_K_C(self, loader: DataLoader, num_samples: int, device: object, storage_device: object, return_embedding: bool=False):\n",
    "        log_probs_N_K_C, labels = self.get_log_probs_N_K_C_labels_N(loader, num_samples, device, storage_device, return_embedding=return_embedding)\n",
    "        return log_probs_N_K_C\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainedBayesianModel(TrainedModel):\n",
    "    model: BayesianModule\n",
    "\n",
    "    def get_log_probs_N_K_C_labels_N(self, loader: DataLoader, num_samples: int, device: object, storage_device: object, return_embedding: bool=False):\n",
    "        log_probs_N_K_C, labels_B = self.model.get_predictions_labels(\n",
    "            num_samples=num_samples,\n",
    "            loader=loader,\n",
    "            device=device,\n",
    "            storage_device=storage_device,\n",
    "            return_embedding=return_embedding\n",
    "        )\n",
    "\n",
    "        # NOTE: this wastes memory bandwidth, but is needed for ensembles where more than one model might not fit\n",
    "        # into memory.\n",
    "        self.model.to(\"cpu\")\n",
    "\n",
    "        return log_probs_N_K_C, labels_B\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainedBayesianEnsemble(TrainedModel):\n",
    "    models: List[TrainedModel]\n",
    "\n",
    "    def get_log_probs_N_K_C_labels_N(self, loader: DataLoader, num_samples: int, device: object, storage_device: object, return_embedding: bool=False):\n",
    "        ensemble_size = len(self.models)\n",
    "        member_num_samples = (num_samples + ensemble_size - 1) // ensemble_size\n",
    "\n",
    "        ensemble_log_probs_N_K_C = []\n",
    "        ensemble_labels_B = None\n",
    "\n",
    "        for model in self.models:\n",
    "            log_probs_N_K_C, labels_B = model.get_log_probs_N_K_C_labels_N(loader=loader,\n",
    "                                                                           num_samples=member_num_samples,\n",
    "                                                                           device=device, storage_device=storage_device, return_embedding=return_embedding)\n",
    "\n",
    "            ensemble_log_probs_N_K_C += [log_probs_N_K_C]\n",
    "            if ensemble_labels_B is not None:\n",
    "                assert torch.all(ensemble_labels_B == labels_B)\n",
    "            else:\n",
    "                ensemble_labels_B = labels_B\n",
    "\n",
    "        ensemble_log_probs_N_K_C = torch.cat(ensemble_log_probs_N_K_C, dim=1)\n",
    "        return ensemble_log_probs_N_K_C, ensemble_labels_B\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    train_batch_size: int\n",
    "    evaluation_batch_size: int\n",
    "\n",
    "    def get_train_dataloader(self, dataset: Dataset):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # test|validation|evaluation\n",
    "    def get_evaluation_dataloader(self, dataset: Dataset):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_trained(self, *, train_loader: DataLoader, train_augmentations: Optional[Module],\n",
    "                    validation_loader: DataLoader, log, loss=None, validation_loss=None) -> TrainedModel:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_distilled(self, *, prediction_loader: DataLoader, train_augmentations: Optional[Module],\n",
    "                      validation_loader: DataLoader, log) -> TrainedModel:\n",
    "        loss = torch.nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
    "        validation_loss = torch.nn.NLLLoss()\n",
    "        return self.get_trained(train_loader=prediction_loader, train_augmentations=train_augmentations,\n",
    "                                validation_loader=validation_loader, loss=loss, validation_loss=validation_loss, log=log)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BayesianEnsembleModelTrainer(ModelTrainer):\n",
    "    model_trainer: ModelTrainer\n",
    "    ensemble_size: int\n",
    "\n",
    "    def get_trained(self, *, train_loader: DataLoader, train_augmentations: Optional[Module],\n",
    "                    validation_loader: DataLoader, log, loss=None, validation_loss=None) -> TrainedBayesianEnsemble:\n",
    "        models = []\n",
    "\n",
    "        for i in range(self.ensemble_size):\n",
    "            log[i] = {}\n",
    "            model = self.model_trainer.get_trained(train_loader=train_loader, train_augmentations=train_augmentations,\n",
    "                                                   validation_loader=validation_loader, log=log[i], loss=loss,\n",
    "                                                   validation_loss=validation_loss)\n",
    "            models += [model]\n",
    "\n",
    "        return TrainedBayesianEnsemble(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
