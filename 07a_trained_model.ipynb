{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Model Interface\n",
    "> \"Why simple, when you can use design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch.nn\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from batchbald_redux.consistent_mc_dropout import BayesianModule, GradEmbeddingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainedModel:\n",
    "    \"\"\"Evaluate a trained model.\"\"\"\n",
    "\n",
    "    def get_log_probs_N_K_C_labels_N(\n",
    "        self, loader: DataLoader, num_samples: int, device: object, storage_device: object\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_log_probs_N_K_C(self, loader: DataLoader, num_samples: int, device: object, storage_device: object):\n",
    "        log_probs_N_K_C, labels = self.get_log_probs_N_K_C_labels_N(loader, num_samples, device, storage_device)\n",
    "        return log_probs_N_K_C\n",
    "\n",
    "    def get_grad_embeddings(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "        num_samples: int,\n",
    "        loss,\n",
    "        grad_embedding_type: GradEmbeddingType,\n",
    "        model_labels: bool,\n",
    "        device: object,\n",
    "        storage_device: object,\n",
    "    ):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainedBayesianModel(TrainedModel):\n",
    "    model: BayesianModule\n",
    "\n",
    "    def get_log_probs_N_K_C_labels_N(\n",
    "        self, loader: DataLoader, num_samples: int, device: object, storage_device: object\n",
    "    ):\n",
    "        log_probs_N_K_C, labels_B = self.model.get_predictions_labels(\n",
    "            num_samples=num_samples, loader=loader, device=device, storage_device=storage_device\n",
    "        )\n",
    "\n",
    "        # NOTE: this wastes memory bandwidth, but is needed for ensembles where more than one model might not fit\n",
    "        # into memory.\n",
    "        self.model.to(\"cpu\")\n",
    "\n",
    "        return log_probs_N_K_C, labels_B\n",
    "\n",
    "    def get_grad_embeddings(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "        num_samples: int,\n",
    "        loss,\n",
    "        grad_embedding_type: GradEmbeddingType,\n",
    "        model_labels: bool,\n",
    "        device: object,\n",
    "        storage_device: object,\n",
    "    ):\n",
    "        grad_embeddings_N_K_E = self.model.get_grad_embeddings(\n",
    "            num_samples=num_samples,\n",
    "            loader=loader,\n",
    "            loss=loss,\n",
    "            grad_embedding_type=grad_embedding_type,\n",
    "            model_labels=model_labels,\n",
    "            device=device,\n",
    "            storage_device=storage_device,\n",
    "        )\n",
    "        return grad_embeddings_N_K_E\n",
    "\n",
    "    def get_embeddings(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "        num_samples: int,\n",
    "        device: object,\n",
    "        storage_device: object,\n",
    "    ):\n",
    "        embeddings_N_K_E = self.model.get_grad_embeddings(\n",
    "            num_samples=num_samples,\n",
    "            loader=loader,\n",
    "            device=device,\n",
    "            storage_device=storage_device,\n",
    "        )\n",
    "        return embeddings_N_K_E\n",
    "\n",
    "@dataclass\n",
    "class TrainedBayesianEnsemble(TrainedModel):\n",
    "    models: List[TrainedModel]\n",
    "\n",
    "    def get_log_probs_N_K_C_labels_N(\n",
    "        self, loader: DataLoader, num_samples: int, device: object, storage_device: object\n",
    "    ):\n",
    "        ensemble_size = len(self.models)\n",
    "        member_num_samples = (num_samples + ensemble_size - 1) // ensemble_size\n",
    "\n",
    "        ensemble_log_probs_N_K_C = []\n",
    "        ensemble_labels_B = None\n",
    "\n",
    "        for model in self.models:\n",
    "            log_probs_N_K_C, labels_B = model.get_log_probs_N_K_C_labels_N(\n",
    "                loader=loader, num_samples=member_num_samples, device=device, storage_device=storage_device\n",
    "            )\n",
    "\n",
    "            ensemble_log_probs_N_K_C += [log_probs_N_K_C]\n",
    "            if ensemble_labels_B is not None:\n",
    "                assert torch.all(ensemble_labels_B == labels_B)\n",
    "            else:\n",
    "                ensemble_labels_B = labels_B\n",
    "\n",
    "        ensemble_log_probs_N_K_C = torch.cat(ensemble_log_probs_N_K_C, dim=1)\n",
    "        return ensemble_log_probs_N_K_C, ensemble_labels_B\n",
    "\n",
    "\n",
    "    def get_grad_embeddings(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "        num_samples: int,\n",
    "        loss,\n",
    "        grad_embedding_type: GradEmbeddingType,\n",
    "        model_labels: bool,\n",
    "        device: object,\n",
    "        storage_device: object,\n",
    "    ):\n",
    "        ensemble_size = len(self.models)\n",
    "        member_num_samples = (num_samples + ensemble_size - 1) // ensemble_size\n",
    "\n",
    "        ensemble_grad_embeddings_N_K_E = []\n",
    "\n",
    "        for model in self.models:\n",
    "            grad_embeddings_N_K_E = model.get_grad_embeddings(\n",
    "                    num_samples=member_num_samples,\n",
    "                    loader=loader,\n",
    "                    loss=loss,\n",
    "                    grad_embedding_type=grad_embedding_type,\n",
    "                    model_labels=model_labels,\n",
    "                    device=device,\n",
    "                    storage_device=storage_device,\n",
    "                )\n",
    "\n",
    "            ensemble_grad_embeddings_N_K_E += [grad_embeddings_N_K_E]\n",
    "\n",
    "        ensemble_grad_embeddings_N_K_E = torch.cat(ensemble_grad_embeddings_N_K_E, dim=1)\n",
    "        return ensemble_grad_embeddings_N_K_E\n",
    "\n",
    "\n",
    "    def get_embeddings(\n",
    "        self,\n",
    "        loader: DataLoader,\n",
    "        num_samples: int,\n",
    "        device: object,\n",
    "        storage_device: object,\n",
    "    ):\n",
    "        ensemble_size = len(self.models)\n",
    "        member_num_samples = (num_samples + ensemble_size - 1) // ensemble_size\n",
    "\n",
    "        ensemble_embeddings_N_K_E = []\n",
    "\n",
    "        for model in self.models:\n",
    "            embeddings_N_K_E = model.get_embeddings(\n",
    "                    num_samples=member_num_samples,\n",
    "                    loader=loader,\n",
    "                    device=device,\n",
    "                    storage_device=storage_device,\n",
    "                )\n",
    "\n",
    "            ensemble_embeddings_N_K_E += [embeddings_N_K_E]\n",
    "\n",
    "        ensemble_embeddings_N_K_E = torch.cat(ensemble_embeddings_N_K_E, dim=1)\n",
    "        return ensemble_embeddings_N_K_E\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    train_batch_size: int\n",
    "    evaluation_batch_size: int\n",
    "\n",
    "    def get_train_dataloader(self, dataset: Dataset):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # test|validation|evaluation\n",
    "    def get_evaluation_dataloader(self, dataset: Dataset):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_trained(\n",
    "        self,\n",
    "        *,\n",
    "        train_loader: DataLoader,\n",
    "        train_augmentations: Optional[Module],\n",
    "        validation_loader: DataLoader,\n",
    "        log,\n",
    "        loss=None,\n",
    "        validation_loss=None\n",
    "    ) -> TrainedModel:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_distilled(\n",
    "        self,\n",
    "        *,\n",
    "        prediction_loader: DataLoader,\n",
    "        train_augmentations: Optional[Module],\n",
    "        validation_loader: DataLoader,\n",
    "        log\n",
    "    ) -> TrainedModel:\n",
    "        loss = torch.nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
    "        validation_loss = torch.nn.NLLLoss()\n",
    "        return self.get_trained(\n",
    "            train_loader=prediction_loader,\n",
    "            train_augmentations=train_augmentations,\n",
    "            validation_loader=validation_loader,\n",
    "            loss=loss,\n",
    "            validation_loss=validation_loss,\n",
    "            log=log,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BayesianEnsembleModelTrainer(ModelTrainer):\n",
    "    model_trainer: ModelTrainer\n",
    "    ensemble_size: int\n",
    "\n",
    "    def get_train_dataloader(self, dataset: Dataset):\n",
    "        return self.model_trainer.get_train_dataloader(dataset)\n",
    "\n",
    "    # test|validation|evaluation\n",
    "    def get_evaluation_dataloader(self, dataset: Dataset):\n",
    "        return self.model_trainer.get_evaluation_dataloader(dataset)\n",
    "\n",
    "    def get_trained(\n",
    "        self,\n",
    "        *,\n",
    "        train_loader: DataLoader,\n",
    "        train_augmentations: Optional[Module],\n",
    "        validation_loader: DataLoader,\n",
    "        log,\n",
    "        loss=None,\n",
    "        validation_loss=None\n",
    "    ) -> TrainedBayesianEnsemble:\n",
    "        models = []\n",
    "\n",
    "        log[\"ensemble\"] = []\n",
    "        for i in range(self.ensemble_size):\n",
    "            log[\"ensemble\"].append({})\n",
    "            model = self.model_trainer.get_trained(\n",
    "                train_loader=train_loader,\n",
    "                train_augmentations=train_augmentations,\n",
    "                validation_loader=validation_loader,\n",
    "                log=log[\"ensemble\"][-1],\n",
    "                loss=loss,\n",
    "                validation_loss=validation_loss,\n",
    "            )\n",
    "            models += [model]\n",
    "\n",
    "        return TrainedBayesianEnsemble(models)\n",
    "\n",
    "\n",
    "    def get_distilled(\n",
    "        self,\n",
    "        *,\n",
    "        prediction_loader: DataLoader,\n",
    "        train_augmentations: Optional[Module],\n",
    "        validation_loader: DataLoader,\n",
    "        log\n",
    "    ) -> TrainedModel:\n",
    "        models = []\n",
    "\n",
    "        log[\"ensemble\"] = []\n",
    "        for i in range(self.ensemble_size):\n",
    "            log[\"ensemble\"].append({})\n",
    "            model = self.model_trainer.get_distilled(\n",
    "                prediction_loader=prediction_loader,\n",
    "                train_augmentations=train_augmentations,\n",
    "                validation_loader=validation_loader,\n",
    "                log=log[\"ensemble\"][-1],\n",
    "            )\n",
    "            models += [model]\n",
    "\n",
    "        return TrainedBayesianEnsemble(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
