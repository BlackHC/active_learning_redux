{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp consistent_mc_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consistent MC Dropout\n",
    "> Custom consistent dropout modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BNNs, we are going to use MC dropout.\n",
    "\n",
    "To be able to compute BatchBALD scores, we need consistent MC dropout, which uses the consistent masks for inference. That means, that we draw $K$ masks and then keep them fixed while drawing the $K$ inference samples for each input in the test set.\n",
    "\n",
    "During training, masks are redrawn for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from blackhc.progress_bar import create_progress_bar\n",
    "from toma import toma\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Module\n",
    "\n",
    "To make this work in an efficient way, we are going to define an abstract wrapper module that takes a batch `input_B` and outputs `results_B_K`.\n",
    "\n",
    "Internally, it will blow up the input batch to $(B \\cdot K) \\times \\cdots$ and then pass it to `mc_forward_impl`, which should be overriden.\n",
    "\n",
    "`ConsistentMCDropout` layers will know to reshape the inputs to $B \\times K \\times \\cdots$ and apply consistent masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c28a026c6159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBayesianModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \"\"\"A module that we can sample multiple times from given a single input batch.\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "# exports\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictionsLabels:\n",
    "    predictions: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "\n",
    "\n",
    "class BayesianModule(Module):\n",
    "    \"\"\"A module that we can sample multiple times from given a single input batch.\n",
    "\n",
    "    To be efficient, the module allows for a part of the forward pass to be deterministic.\n",
    "\n",
    "    If we sample with \"0\" samples, we do a single deterministic forward pass with disabled dropout during evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    # num MC dropout samples\n",
    "    k = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # Returns B x K x ...\n",
    "    def forward(self, input_B: torch.Tensor, k: int):\n",
    "        BayesianModule.k = k\n",
    "\n",
    "        if k == 0:\n",
    "            # No MC dropout (0 samples).\n",
    "            features_B = self.deterministic_forward_impl(input_B)\n",
    "            output_B = self.mc_forward_impl(features_B)\n",
    "            output_B_1 = BayesianModule.unflatten_tensor(output_B, 1)\n",
    "            return output_B_1\n",
    "\n",
    "        features_B = self.deterministic_forward_impl(input_B)\n",
    "        mc_features_BK = BayesianModule.mc_tensor(features_B, k)\n",
    "        mc_output_BK = self.mc_forward_impl(mc_features_BK)\n",
    "        mc_output_B_K = BayesianModule.unflatten_tensor(mc_output_BK, k)\n",
    "        return mc_output_B_K\n",
    "\n",
    "    def deterministic_forward_impl(self, input_B: torch.Tensor) -> torch.Tensor:\n",
    "        return input_B\n",
    "\n",
    "    def mc_forward_impl(self, mc_features_BK: torch.Tensor) -> torch.Tensor:\n",
    "        return mc_features_BK\n",
    "\n",
    "    @staticmethod\n",
    "    def unflatten_tensor(input: torch.Tensor, k: int):\n",
    "        input = input.view([-1, k] + list(input.shape[1:]))\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_tensor(mc_input: torch.Tensor):\n",
    "        return mc_input.flatten(0, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def mc_tensor(input: torch.tensor, k: int):\n",
    "        mc_shape = [input.shape[0], k] + list(input.shape[1:])\n",
    "        return input.unsqueeze(1).expand(mc_shape).flatten(0, 1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_no_dropout_predictions_labels(self, *, loader: data.DataLoader, device):\n",
    "        self.to(device=device)\n",
    "        self.eval()\n",
    "\n",
    "        N = len(loader.dataset)\n",
    "        predictions = None\n",
    "        labels = None\n",
    "\n",
    "        pbar = create_progress_bar(N, tqdm_args=dict(desc=\"get_predictions_labels\", leave=False))\n",
    "        pbar.start()\n",
    "\n",
    "        data_start = 0\n",
    "\n",
    "        for batch_x, batch_labels in loader:\n",
    "            batch_x = batch_x.to(device=device)\n",
    "            batch_predictions = self(batch_x, k=0)\n",
    "\n",
    "            batch_size = len(batch_predictions)\n",
    "            data_end = data_start + batch_size\n",
    "\n",
    "            # Support multi-dim labels.\n",
    "            if labels is None:\n",
    "                labels_shape = (N, *batch_labels.shape[1:])\n",
    "                labels = torch.empty(labels_shape, dtype=batch_labels.dtype, device=\"cpu\")\n",
    "            # Support multi-dim predictions.\n",
    "            if predictions is None:\n",
    "                predictions_shape = (N, *batch_predictions.shape[1:])\n",
    "                predictions = torch.empty(predictions_shape, dtype=batch_predictions.dtype, device=\"cpu\")\n",
    "\n",
    "            predictions[data_start:data_end].copy_(batch_predictions.float(), non_blocking=True)\n",
    "            labels[data_start:data_end].copy_(batch_labels, non_blocking=True)\n",
    "\n",
    "            data_start = data_end\n",
    "\n",
    "            pbar.update(batch_size)\n",
    "\n",
    "        pbar.finish()\n",
    "\n",
    "        return predictions, labels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_predictions_labels(self, *, k: int, loader: data.DataLoader, device):\n",
    "        assert_no_shuffling_no_augmentations_dataloader(loader)\n",
    "\n",
    "        if k == 0:\n",
    "            return self._get_no_dropout_predictions_labels(loader=loader, device=device)\n",
    "\n",
    "        self.to(device=device)\n",
    "\n",
    "        N = len(loader.dataset)\n",
    "        predictions = None\n",
    "        labels = None\n",
    "\n",
    "        pbar = create_progress_bar(N * k, tqdm_args=dict(desc=\"get_predictions_labels\", leave=False))\n",
    "        pbar.start()\n",
    "\n",
    "        @toma.execute.range(0, k, 128)\n",
    "        def get_prediction_batch(start, end):\n",
    "            nonlocal predictions\n",
    "            nonlocal labels\n",
    "\n",
    "            if start == 0:\n",
    "                pbar.reset()\n",
    "\n",
    "            self.eval()\n",
    "\n",
    "            num_sub_samples = end - start\n",
    "\n",
    "            data_start = 0\n",
    "            # TODO: ensure that the dataloader is not shuffling!\n",
    "            for batch_x, batch_labels in loader:\n",
    "                batch_x = batch_x.to(device=device)\n",
    "                batch_predictions = self(batch_x, num_sub_samples)\n",
    "\n",
    "                batch_size = len(batch_predictions)\n",
    "                data_end = data_start + batch_size\n",
    "\n",
    "                # Support multi-dim predictions.\n",
    "                if predictions is None:\n",
    "                    predictions_shape = (N, *batch_predictions.shape[1:])\n",
    "                    predictions = torch.empty(predictions_shape, dtype=batch_predictions.dtype, device=\"cpu\")\n",
    "                # Support multi-dim labels.\n",
    "                if labels is None:\n",
    "                    labels_shape = (N, *batch_labels.shape[1:])\n",
    "                    labels = torch.empty(labels_shape, dtype=batch_labels.dtype, device=\"cpu\")\n",
    "\n",
    "                predictions[data_start:data_end, start:end].copy_(batch_predictions.float(), non_blocking=True)\n",
    "                if start == 0:\n",
    "                    labels[data_start:data_end].copy_(batch_labels, non_blocking=True)\n",
    "\n",
    "                data_start = data_end\n",
    "\n",
    "                pbar.update(batch_size * num_sub_samples)\n",
    "\n",
    "        pbar.finish()\n",
    "\n",
    "        return predictions, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistent MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class _ConsistentMCDropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, \" \"but got {}\".format(p))\n",
    "\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"p={}\".format(self.p)\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "        if not mode:\n",
    "            self.reset_mask()\n",
    "\n",
    "    def _get_sample_mask_shape(self, sample_shape):\n",
    "        return sample_shape\n",
    "\n",
    "    def _create_mask(self, input, k):\n",
    "        mask_shape = [1, k] + list(self._get_sample_mask_shape(input.shape[1:]))\n",
    "        mask = torch.empty(mask_shape, dtype=torch.bool, device=input.device).bernoulli_(self.p)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        if self.p == 0.0:\n",
    "            return input\n",
    "\n",
    "        k = BayesianModule.k\n",
    "\n",
    "        # Disable dropout during evaluation if k=0 (i.e. no samples).\n",
    "        if not self.training and k == 0:\n",
    "            return input\n",
    "\n",
    "        if self.training:\n",
    "            # Create a new mask on each call and for each batch element.\n",
    "            k = input.shape[0] * max(k, 1)\n",
    "            mask = self._create_mask(input, k)\n",
    "        else:\n",
    "            if self.mask is None:\n",
    "                # print('recreating mask', self)\n",
    "                # Recreate mask.\n",
    "                self.mask = self._create_mask(input, k)\n",
    "\n",
    "            mask = self.mask\n",
    "\n",
    "        mc_input = BayesianModule.unflatten_tensor(input, k)\n",
    "        mc_output = mc_input.masked_fill(mask, 0) / (1 - self.p)\n",
    "\n",
    "        # Flatten MCDI, batch into one dimension again.\n",
    "        return BayesianModule.flatten_tensor(mc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class ConsistentMCDropout(_ConsistentMCDropout):\n",
    "    r\"\"\"Randomly zeroes some of the elements of the input\n",
    "    tensor with probability :attr:`p` using samples from a Bernoulli\n",
    "    distribution. The elements to zero are randomized on every forward call during training time.\n",
    "\n",
    "    During eval time, a fixed mask is picked and kept until `reset_mask()` is called.\n",
    "\n",
    "    This has proven to be an effective technique for regularization and\n",
    "    preventing the co-adaptation of neurons as described in the paper\n",
    "    `Improving neural networks by preventing co-adaptation of feature\n",
    "    detectors`_ .\n",
    "\n",
    "    Furthermore, the outputs are scaled by a factor of :math:`\\frac{1}{1-p}` during\n",
    "    training. This means that during evaluation the module simply computes an\n",
    "    identity function.\n",
    "\n",
    "    Args:\n",
    "        p: probability of an element to be zeroed. Default: 0.5\n",
    "        inplace: If set to ``True``, will do this operation in-place. Default: ``False``\n",
    "\n",
    "    Shape:\n",
    "        - Input: `Any`. Input can be of any shape\n",
    "        - Output: `Same`. Output is of the same shape as input\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Dropout(p=0.2)\n",
    "        >>> input = torch.randn(20, 16)\n",
    "        >>> output = m(input)\n",
    "\n",
    "    .. _Improving neural networks by preventing co-adaptation of feature\n",
    "        detectors: https://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ConsistentMCDropout2d(_ConsistentMCDropout):\n",
    "    r\"\"\"Randomly zeroes whole channels of the input tensor.\n",
    "    The channels to zero-out are randomized on every forward call.\n",
    "\n",
    "    During eval time, a fixed mask is picked and kept until `reset_mask()` is called.\n",
    "\n",
    "    Usually the input comes from :class:`nn.Conv2d` modules.\n",
    "\n",
    "    As described in the paper\n",
    "    `Efficient Object Localization Using Convolutional Networks`_ ,\n",
    "    if adjacent pixels within feature maps are strongly correlated\n",
    "    (as is normally the case in early convolution layers) then i.i.d. dropout\n",
    "    will not regularize the activations and will otherwise just result\n",
    "    in an effective learning rate decrease.\n",
    "\n",
    "    In this case, :func:`nn.Dropout2d` will help promote independence between\n",
    "    feature maps and should be used instead.\n",
    "\n",
    "    Args:\n",
    "        p (float, optional): probability of an element to be zero-ed.\n",
    "        inplace (bool, optional): If set to ``True``, will do this operation\n",
    "            in-place\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, H, W)`\n",
    "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Dropout2d(p=0.2)\n",
    "        >>> input = torch.randn(20, 16, 32, 32)\n",
    "        >>> output = m(input)\n",
    "\n",
    "    .. _Efficient Object Localization Using Convolutional Networks:\n",
    "       http://arxiv.org/abs/1411.4280\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_sample_mask_shape(self, sample_shape):\n",
    "        return [sample_shape[0]] + [1] * (len(sample_shape) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following defines a DNN module that can learn MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv1_drop): ConsistentMCDropout2d(p=0.5)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): ConsistentMCDropout2d(p=0.5)\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc1_drop): ConsistentMCDropout(p=0.5)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BayesianCNN(BayesianModule):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv1_drop = ConsistentMCDropout2d()\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = ConsistentMCDropout2d()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc1_drop = ConsistentMCDropout()\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def mc_forward_impl(self, input: torch.Tensor):\n",
    "        input = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(input)), 2))\n",
    "        input = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(input)), 2))\n",
    "        input = input.view(-1, 1024)\n",
    "        input = F.relu(self.fc1_drop(self.fc1(input)))\n",
    "        input = self.fc2(input)\n",
    "        input = F.log_softmax(input, dim=1)\n",
    "\n",
    "        return input\n",
    "\n",
    "\n",
    "BayesianCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional wrappers\n",
    "\n",
    "To make it easier for training in inference and training with regular frameworks, we want to keep k fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class SamplerModel(Module):\n",
    "    \"\"\"Wrap a `BayesianModule` to sample k MC dropout samples consistently.\n",
    "\n",
    "    A forward pass returns: log probs BxKxC for a batch of B inputs, K MC dropout samples, and C classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bayesian_module: BayesianModule, k: int):\n",
    "        super().__init__()\n",
    "        self.bayesian_module = bayesian_module\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        log_probs_B_K_C = self.bayesian_module(input, self.k)\n",
    "        return log_probs_B_K_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better Dropout optimization, we sometimes want to use multiple samples that we train independently (Decoder Cross-Entropy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def multi_sample_loss(loss):\n",
    "    \"\"\"Wrap a loss function to expand the targets to work together with a SamplerModel.\"\"\"\n",
    "    @wraps(loss)\n",
    "    def wrapped_loss(input_B_K_C, target_B_, *args, **kwargs):\n",
    "        assert input_B_K_C.shape[0] == target_B_.shape[0]\n",
    "        input_BK_C = input_B_K_C.flatten(0, 1)\n",
    "        target_B_K_ = target_B_[:, None].expand(input_B_K_C.shape[:2] + target_B_.shape[1:])\n",
    "        target_BK_ = target_B_K_.flatten(0, 1)\n",
    "        return loss(input_BK_C, target_BK_, *args, **kwargs)\n",
    "\n",
    "    return wrapped_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9586)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_inputs = torch.arange(100).reshape(2, 5, 10).float()\n",
    "fake_target = torch.arange(2)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "wrapped_loss = multi_sample_loss(loss)\n",
    "\n",
    "wrapped_loss(fake_inputs, fake_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For final test predictions, we want to take the geometric mean (average of logits):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def geometric_mean_loss(loss):\n",
    "    @wraps(loss)\n",
    "    def wrapped_loss(log_probs_B_K_C, target_N, *args, **kwargs):\n",
    "        return loss(log_probs_B_K_C.mean(dim=1, keepdim=False), target_N, *args, **kwargs)\n",
    "\n",
    "    return wrapped_loss\n",
    "\n",
    "\n",
    "class GeometricMeanPrediction(Module):\n",
    "    \"\"\"\n",
    "    Use a eg. SamplerModel and compute the geometric mean as \"ensemble\" prediction.\n",
    "\n",
    "    We assume we receive log probs (so after Softmax).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        log_probs_B_K_C = self.module(input)\n",
    "        return log_probs_B_K_C.mean(dim=1, keepdim=False)\n",
    "\n",
    "\n",
    "class LogProbMeanPrediction(Module):\n",
    "    \"\"\"\n",
    "    Use a eg. SamplerModel and compute the geometric mean as \"ensemble\" prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        log_probs_B_K_C = self.module(input)\n",
    "        log_mean_probs = log_probs_B_K_C.logsumexp(dim=1, keepdim=False) - np.log(log_probs_B_K_C.shape[1])\n",
    "        return log_mean_probs\n",
    "\n",
    "\n",
    "def get_log_mean_probs(log_probs_N_K_C):\n",
    "    # Arithmetic mean of the probs (valid MC dropout)\n",
    "    log_mean_probs_N_C = log_probs_N_K_C.logsumexp(dim=1, keepdim=False) - np.log(log_probs_N_K_C.shape[1])\n",
    "    return log_mean_probs_N_C\n",
    "\n",
    "\n",
    "def assert_no_shuffling_no_augmentations_dataloader(dataloader: data.DataLoader):\n",
    "    batch_x_A = None\n",
    "    batch_labels_A = None\n",
    "    batch_x_B = None\n",
    "    batch_labels_B = None\n",
    "\n",
    "    for batch_x_A, batch_labels_A in dataloader:\n",
    "        break\n",
    "\n",
    "    for batch_x_B, batch_labels_B in dataloader:\n",
    "        break\n",
    "\n",
    "    assert batch_x_A == batch_x_B, \"Batch inputs different. Augmentations enabled, or dataloader shuffles data?!\"\n",
    "    assert batch_labels_A == batch_labels_B, \"Batch labels different. Augmentations enabled, or dataloader shuffles data?!\"\n",
    "\n",
    "\n",
    "def get_bayesian_ensemble_predictions_labels(*, modules: List[BayesianModule], k: int, loader: data.DataLoader, device):\n",
    "    assert_no_shuffling_no_augmentations_dataloader(loader)\n",
    "\n",
    "    ensemble_predictions = []\n",
    "    ensemble_labels = None\n",
    "\n",
    "    for module in modules:\n",
    "        predictions, labels = module.get_predictions_labels(k=k, loader=loader, device=device)\n",
    "\n",
    "        ensemble_predictions += [predictions]\n",
    "        if ensemble_labels is not None:\n",
    "            assert torch.all(ensemble_labels == labels)\n",
    "        else:\n",
    "            ensemble_labels = labels\n",
    "\n",
    "        module.to(\"cpu\")\n",
    "\n",
    "    ensemble_predictions = torch.cat(ensemble_predictions, dim=1)\n",
    "    return ensemble_predictions, ensemble_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
