{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Eval Model\n",
    "> \"Why simple, when you can use design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from dataclasses import dataclass\n",
    "from typing import Type, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from batchbald_redux.active_learning import RandomFixedLengthSampler\n",
    "from batchbald_redux.consistent_mc_dropout import get_log_mean_probs\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    RandomLabelsDataset,\n",
    "    ReplaceTargetsDataset,\n",
    ")\n",
    "from batchbald_redux.trained_model import TrainedModel, ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class TrainEvalModel:\n",
    "    def __call__(self, *, model_trainer: ModelTrainer, training_dataset: torch.utils.data.Dataset,\n",
    "                 train_augmentations: nn.Module, eval_dataset: torch.utils.data.Dataset,\n",
    "                 validation_loader: torch.utils.data.DataLoader, trained_model: TrainedModel, device: Optional,\n",
    "                 storage_device: Optional, training_log) -> TrainedModel:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainSelfDistillationEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "\n",
    "    def __call__(self, *, model_trainer: ModelTrainer, training_dataset: torch.utils.data.Dataset,\n",
    "                 train_augmentations: nn.Module, eval_dataset: torch.utils.data.Dataset,\n",
    "                 validation_loader: torch.utils.data.DataLoader, trained_model: TrainedModel, device: Optional,\n",
    "                 storage_device: Optional, training_log):\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset([training_dataset, eval_dataset])\n",
    "        train_eval_loader = model_trainer.get_evaluation_dataloader(train_eval_dataset)\n",
    "\n",
    "        eval_log_probs_N_C = get_log_mean_probs(\n",
    "            trained_model.get_log_probs_N_K_C(train_eval_loader, num_samples=self.num_pool_samples, device=device,\n",
    "                                              storage_device=storage_device)\n",
    "        )\n",
    "\n",
    "        eval_self_distillation_dataset = ReplaceTargetsDataset(dataset=train_eval_dataset, targets=eval_log_probs_N_C)\n",
    "\n",
    "        train_eval_self_distillation_loader = model_trainer.get_train_dataloader(eval_self_distillation_dataset)\n",
    "\n",
    "        trained_model = model_trainer.get_distilled(\n",
    "            prediction_loader=train_eval_self_distillation_loader,\n",
    "            train_augmentations=train_augmentations,\n",
    "            validation_loader=validation_loader,\n",
    "            log=training_log\n",
    "        )\n",
    "\n",
    "        return trained_model\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainRandomLabelEvalModel(TrainEvalModel):\n",
    "    def __call__(self, *, model_trainer: ModelTrainer, training_dataset: torch.utils.data.Dataset,\n",
    "                 train_augmentations: nn.Module, eval_dataset: torch.utils.data.Dataset,\n",
    "                 validation_loader: torch.utils.data.DataLoader, trained_model: TrainedModel, device: Optional,\n",
    "                 storage_device: Optional, training_log):\n",
    "        # TODO: support one_hot!\n",
    "        # TODO: different seed needed!\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset(\n",
    "            [training_dataset, RandomLabelsDataset(eval_dataset, seed=0, device=storage_device)]\n",
    "        )\n",
    "        train_eval_loader = model_trainer.get_train_dataloader(train_eval_dataset)\n",
    "\n",
    "        trained_model = model_trainer.get_trained(\n",
    "            train_loader=train_eval_loader,\n",
    "            train_augmentations=train_augmentations,\n",
    "            validation_loader=validation_loader,\n",
    "            log=training_log\n",
    "        )\n",
    "\n",
    "        return trained_model\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainExplicitEvalModel(TrainEvalModel):\n",
    "    cache_explicit_eval_model: bool = False\n",
    "    _fully_trained_model: TrainedModel = None\n",
    "        \n",
    "    def __call__(self, *, model_trainer: ModelTrainer, training_dataset: torch.utils.data.Dataset,\n",
    "                 train_augmentations: nn.Module, eval_dataset: torch.utils.data.Dataset,\n",
    "                 validation_loader: torch.utils.data.DataLoader, trained_model: TrainedModel, device: Optional,\n",
    "                 storage_device: Optional, training_log):\n",
    "        # TODO: support one_hot!? For this we need to change the eval_dataset to also have one_hot applied in\n",
    "        #  ExperimentData?\n",
    "        if not self._fully_trained_model:\n",
    "            train_eval_dataset = torch.utils.data.ConcatDataset([training_dataset, eval_dataset])\n",
    "            train_eval_loader = model_trainer.get_train_dataloader(train_eval_dataset)\n",
    "\n",
    "            trained_model = model_trainer.get_trained(train_loader=train_eval_loader,\n",
    "                                                      train_augmentations=train_augmentations,\n",
    "                                                      validation_loader=validation_loader, log=training_log)\n",
    "            if self.cache_explicit_eval_model:\n",
    "                self._fully_trained_model = trained_model\n",
    "        else:\n",
    "            print(\"Using cached fully trained model!\")\n",
    "            trained_model = self._fully_trained_model\n",
    "            \n",
    "        return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
