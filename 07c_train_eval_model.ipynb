{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Model Interface\n",
    "> \"Why simple, when you can use design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from dataclasses import dataclass\n",
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from batchbald_redux.active_learning import RandomFixedLengthSampler\n",
    "from batchbald_redux.consistent_mc_dropout import get_log_mean_probs\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    RandomLabelsDataset,\n",
    "    ReplaceTargetsDataset,\n",
    ")\n",
    "from batchbald_redux.trained_model import TrainedModel, ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class TrainEvalModel:\n",
    "    def __call__(self, *, training_log, device) -> TrainedModel:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainSelfDistillationEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    trained_model: TrainedModel\n",
    "    model_trainer: ModelTrainer\n",
    "    min_samples_per_epoch: int\n",
    "    # TODO: remove the default?\n",
    "    train_augmentations: nn.Module = None\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset([self.training_dataset, self.eval_dataset])\n",
    "        train_eval_loader = torch.utils.data.DataLoader(train_eval_dataset, batch_size=512, drop_last=False)\n",
    "\n",
    "        eval_log_probs_N_C = get_log_mean_probs(\n",
    "            self.trained_model.get_log_probs_N_K_C(train_eval_loader, num_samples=self.num_pool_samples, device=device)\n",
    "        )\n",
    "\n",
    "        eval_self_distillation_dataset = ReplaceTargetsDataset(dataset=train_eval_dataset, targets=eval_log_probs_N_C)\n",
    "\n",
    "        # TODO: Dataloaders should be part of the model trainer, too!\n",
    "        train_eval_self_distillation_loader = torch.utils.data.DataLoader(\n",
    "            eval_self_distillation_dataset,\n",
    "            batch_size=self.training_batch_size,\n",
    "            sampler=RandomFixedLengthSampler(eval_self_distillation_dataset, self.min_samples_per_epoch),\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        trained_model = self.model_trainer.get_distilled(\n",
    "            prediction_loader=train_eval_self_distillation_loader,\n",
    "            train_augmentations=self.train_augmentations,\n",
    "            validation_loader=self.validation_loader,\n",
    "            log=training_log\n",
    "        )\n",
    "\n",
    "        return trained_model\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainRandomLabelEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    num_training_samples: int\n",
    "    num_validation_samples: int\n",
    "    num_patience_epochs: int\n",
    "    max_epochs: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    model_trainer: ModelTrainer\n",
    "    # TODO: remove the default?\n",
    "    train_augmentations: nn.Module = None\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        # TODO: support one_hot!\n",
    "        # TODO: different seed needed!\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset(\n",
    "            [self.training_dataset, RandomLabelsDataset(self.eval_dataset, seed=0)]\n",
    "        )\n",
    "        train_eval_loader = torch.utils.data.DataLoader(\n",
    "            train_eval_dataset, batch_size=self.training_batch_size, drop_last=True, shuffle=True\n",
    "        )\n",
    "\n",
    "        trained_model = self.model_trainer.get_distilled(\n",
    "            prediction_loader=train_eval_loader,\n",
    "            train_augmentations=self.train_augmentations,\n",
    "            validation_loader=self.validation_loader,\n",
    "            log=training_log\n",
    "        )\n",
    "\n",
    "        return trained_model\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainExplicitEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    num_training_samples: int\n",
    "    num_validation_samples: int\n",
    "    num_patience_epochs: int\n",
    "    max_epochs: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    model_trainer: ModelTrainer\n",
    "    # TODO: remove the default?\n",
    "    train_augmentations: nn.Module = None\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        # TODO: support one_hot!\n",
    "        # TODO: different seed needed!\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset([self.training_dataset, self.eval_dataset])\n",
    "        train_eval_loader = torch.utils.data.DataLoader(\n",
    "            train_eval_dataset, batch_size=self.training_batch_size, drop_last=True, shuffle=True\n",
    "        )\n",
    "\n",
    "        trained_model = self.model_trainer.get_trained(train_loader=train_eval_loader,\n",
    "                                                       train_augmentations=self.train_augmentations,\n",
    "                                                       validation_loader=self.validation_loader, log=training_log)\n",
    "\n",
    "        return trained_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
