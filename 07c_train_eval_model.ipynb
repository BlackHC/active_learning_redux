{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Model Interface\n",
    "> \"Why simple, when you can use design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train_eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "from dataclasses import dataclass\n",
    "from typing import Type\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "from batchbald_redux.active_learning import RandomFixedLengthSampler\n",
    "from batchbald_redux.black_box_model_training import train, train_with_schedule\n",
    "from batchbald_redux.consistent_mc_dropout import get_log_mean_probs\n",
    "from batchbald_redux.dataset_challenges import (\n",
    "    RandomLabelsDataset,\n",
    "    ReplaceTargetsDataset,\n",
    ")\n",
    "from batchbald_redux.model_optimizer_factory import ModelOptimizerFactory\n",
    "from batchbald_redux.trained_model import TrainedMCDropoutModel, TrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class TrainEvalModel:\n",
    "    def __call__(self, *, training_log, device) -> TrainedModel:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainSelfDistillationEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    num_training_samples: int\n",
    "    num_validation_samples: int\n",
    "    num_patience_epochs: int\n",
    "    max_epochs: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    model_optimizer_factory: Type[ModelOptimizerFactory]\n",
    "    trained_model: TrainedModel\n",
    "    min_samples_per_epoch: int\n",
    "    # TODO: remove the default?\n",
    "    train_augmentations: nn.Module = None\n",
    "    # TODO: remove the default!\n",
    "    prefer_accuracy: bool = True\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset([self.training_dataset, self.eval_dataset])\n",
    "        train_eval_loader = torch.utils.data.DataLoader(train_eval_dataset, batch_size=512, drop_last=False)\n",
    "\n",
    "        eval_log_probs_N_C = get_log_mean_probs(\n",
    "            self.trained_model.get_log_probs_N_K_C(train_eval_loader, device=device)\n",
    "        )\n",
    "\n",
    "        eval_self_distillation_dataset = ReplaceTargetsDataset(dataset=train_eval_dataset, targets=eval_log_probs_N_C)\n",
    "        train_eval_self_distillation_loader = torch.utils.data.DataLoader(\n",
    "            eval_self_distillation_dataset,\n",
    "            batch_size=self.training_batch_size,\n",
    "            sampler=RandomFixedLengthSampler(eval_self_distillation_dataset, self.min_samples_per_epoch),\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        eval_model_optimizer = self.model_optimizer_factory().create_model_optimizer()\n",
    "\n",
    "        loss = torch.nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
    "\n",
    "        train(\n",
    "            model=eval_model_optimizer.model,\n",
    "            optimizer=eval_model_optimizer.optimizer,\n",
    "            train_augmentations=self.train_augmentations,\n",
    "            loss=loss,\n",
    "            validation_loss=torch.nn.NLLLoss(),\n",
    "            training_samples=self.num_training_samples,\n",
    "            validation_samples=self.num_validation_samples,\n",
    "            train_loader=train_eval_self_distillation_loader,\n",
    "            validation_loader=self.validation_loader,\n",
    "            patience=self.num_patience_epochs,\n",
    "            max_epochs=self.max_epochs,\n",
    "            prefer_accuracy=self.prefer_accuracy,\n",
    "            device=device,\n",
    "            training_log=training_log,\n",
    "        )\n",
    "\n",
    "        return TrainedMCDropoutModel(num_samples=self.num_pool_samples, model=eval_model_optimizer.model)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainSelfDistillationEvalModelWithSchedule(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    num_training_samples: int\n",
    "    num_validation_samples: int\n",
    "    patience_schedule: [int]\n",
    "    factor_schedule: [int]\n",
    "    max_epochs: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    model_optimizer_factory: Type[ModelOptimizerFactory]\n",
    "    trained_model: TrainedModel\n",
    "    min_samples_per_epoch: int\n",
    "    prefer_accuracy: bool\n",
    "    # TODO: remove the default?\n",
    "    train_augmentations: nn.Module = None\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset([self.training_dataset, self.eval_dataset])\n",
    "        train_eval_loader = torch.utils.data.DataLoader(train_eval_dataset, batch_size=512, drop_last=False)\n",
    "\n",
    "        eval_log_probs_N_C = get_log_mean_probs(\n",
    "            self.trained_model.get_log_probs_N_K_C(train_eval_loader, device=device)\n",
    "        )\n",
    "\n",
    "        eval_self_distillation_dataset = ReplaceTargetsDataset(dataset=train_eval_dataset, targets=eval_log_probs_N_C)\n",
    "        train_eval_self_distillation_loader = torch.utils.data.DataLoader(\n",
    "            eval_self_distillation_dataset,\n",
    "            batch_size=self.training_batch_size,\n",
    "            sampler=RandomFixedLengthSampler(eval_self_distillation_dataset, self.min_samples_per_epoch),\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        eval_model_optimizer = self.model_optimizer_factory().create_model_optimizer()\n",
    "\n",
    "        loss = torch.nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
    "\n",
    "        train_with_schedule(\n",
    "            model=eval_model_optimizer.model,\n",
    "            optimizer=eval_model_optimizer.optimizer,\n",
    "            loss=loss,\n",
    "            train_augmentations=self.train_augmentations,\n",
    "            validation_loss=torch.nn.NLLLoss(),\n",
    "            training_samples=self.num_training_samples,\n",
    "            validation_samples=self.num_validation_samples,\n",
    "            train_loader=train_eval_self_distillation_loader,\n",
    "            validation_loader=self.validation_loader,\n",
    "            patience_schedule=self.patience_schedule,\n",
    "            factor_schedule=self.factor_schedule,\n",
    "            max_epochs=self.max_epochs,\n",
    "            prefer_accuracy=self.prefer_accuracy,\n",
    "            device=device,\n",
    "            training_log=training_log,\n",
    "        )\n",
    "\n",
    "        return TrainedMCDropoutModel(num_samples=self.num_pool_samples, model=eval_model_optimizer.model)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainRandomLabelEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    num_training_samples: int\n",
    "    num_validation_samples: int\n",
    "    num_patience_epochs: int\n",
    "    max_epochs: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    model_optimizer_factory: ModelOptimizerFactory\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        # TODO: support one_hot!\n",
    "        # TODO: different seed needed!\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset(\n",
    "            [self.training_dataset, RandomLabelsDataset(self.eval_dataset, seed=0)]\n",
    "        )\n",
    "        train_eval_loader = torch.utils.data.DataLoader(\n",
    "            train_eval_dataset, batch_size=self.training_batch_size, drop_last=True, shuffle=True\n",
    "        )\n",
    "\n",
    "        eval_model_optimizer = self.model_optimizer_factory.create_model_optimizer()\n",
    "\n",
    "        loss = torch.nn.NLLLoss()\n",
    "\n",
    "        train(\n",
    "            model=eval_model_optimizer.model,\n",
    "            optimizer=eval_model_optimizer.optimizer,\n",
    "            loss=loss,\n",
    "            validation_loss=loss,\n",
    "            training_samples=self.num_training_samples,\n",
    "            validation_samples=self.num_validation_samples,\n",
    "            train_loader=train_eval_loader,\n",
    "            validation_loader=self.validation_loader,\n",
    "            patience=self.num_patience_epochs,\n",
    "            max_epochs=self.max_epochs,\n",
    "            device=device,\n",
    "            training_log=training_log,\n",
    "        )\n",
    "\n",
    "        return TrainedMCDropoutModel(num_samples=self.num_pool_samples, model=eval_model_optimizer.model)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainExplicitEvalModel(TrainEvalModel):\n",
    "    num_pool_samples: int\n",
    "    num_training_samples: int\n",
    "    num_validation_samples: int\n",
    "    num_patience_epochs: int\n",
    "    max_epochs: int\n",
    "    training_dataset: torch.utils.data.Dataset\n",
    "    eval_dataset: torch.utils.data.Dataset\n",
    "    validation_loader: torch.utils.data.DataLoader\n",
    "    training_batch_size: int\n",
    "    model_optimizer_factory: ModelOptimizerFactory\n",
    "\n",
    "    def __call__(self, *, training_log, device):\n",
    "        # TODO: support one_hot!\n",
    "        # TODO: different seed needed!\n",
    "        train_eval_dataset = torch.utils.data.ConcatDataset([self.training_dataset, self.eval_dataset])\n",
    "        train_eval_loader = torch.utils.data.DataLoader(\n",
    "            train_eval_dataset, batch_size=self.training_batch_size, drop_last=True, shuffle=True\n",
    "        )\n",
    "\n",
    "        eval_model_optimizer = self.model_optimizer_factory.create_model_optimizer()\n",
    "\n",
    "        loss = torch.nn.NLLLoss()\n",
    "\n",
    "        train(\n",
    "            model=eval_model_optimizer.model,\n",
    "            optimizer=eval_model_optimizer.optimizer,\n",
    "            loss=loss,\n",
    "            validation_loss=loss,\n",
    "            training_samples=self.num_training_samples,\n",
    "            validation_samples=self.num_validation_samples,\n",
    "            train_loader=train_eval_loader,\n",
    "            validation_loader=self.validation_loader,\n",
    "            patience=self.num_patience_epochs,\n",
    "            max_epochs=self.max_epochs,\n",
    "            device=device,\n",
    "            training_log=training_log,\n",
    "        )\n",
    "\n",
    "        return TrainedMCDropoutModel(num_samples=self.num_pool_samples, model=eval_model_optimizer.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
