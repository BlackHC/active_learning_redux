{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquisition Functions\n",
    "> Wrapping wrappers wraps wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp baseline_acquisition_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Type\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from batchbald_redux.batchbald import CandidateBatch\n",
    "from batchbald_redux.trained_model import TrainedModel, TrainedBayesianModel\n",
    "from batchbald_redux.acquisition_functions import CandidateBatchComputer\n",
    "from batchbald_redux.consistent_mc_dropout import BayesianModule, SamplerModel, GradEmbeddingType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "# copied from https://github.com/JordanAsh/badge/blob/master/query_strategies/badge_sampling.py\n",
    "# by Jordan Ash\n",
    "import torch\n",
    "import pdb\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# kmeans ++ initialization\n",
    "def init_centers(X, K):\n",
    "    ind = np.argmax([np.linalg.norm(s, 2) for s in X])\n",
    "    mu = [X[ind]]\n",
    "    indsAll = [ind]\n",
    "    centInds = [0.] * len(X)\n",
    "    cent = 0\n",
    "    print('#Samps\\tTotal Distance')\n",
    "    while len(mu) < K:\n",
    "        if len(mu) == 1:\n",
    "            D2 = pairwise_distances(X, mu).ravel().astype(float)\n",
    "        else:\n",
    "            newD = pairwise_distances(X, [mu[-1]]).ravel().astype(float)\n",
    "            for i in range(len(X)):\n",
    "                if D2[i] > newD[i]:\n",
    "                    centInds[i] = cent\n",
    "                    D2[i] = newD[i]\n",
    "        print(str(len(mu)) + '\\t' + str(sum(D2)), flush=True)\n",
    "        #if sum(D2) == 0.0:\n",
    "        #    pdb.set_trace()\n",
    "        D2 = D2.ravel().astype(float)\n",
    "        Ddist = (D2 ** 2)/ sum(D2 ** 2)\n",
    "        customDist = stats.rv_discrete(name='custm', values=(np.arange(len(D2)), Ddist))\n",
    "        ind = customDist.rvs(size=1)[0]\n",
    "        while ind in indsAll: ind = customDist.rvs(size=1)[0]\n",
    "        mu.append(X[ind])\n",
    "        indsAll.append(ind)\n",
    "        cent += 1\n",
    "    return indsAll\n",
    "\n",
    "@dataclass\n",
    "class BADGE(CandidateBatchComputer):\n",
    "    def compute_candidate_batch(\n",
    "        self, model: TrainedModel, pool_loader: torch.utils.data.DataLoader, device\n",
    "    ) -> CandidateBatch:\n",
    "        grad_embeddings = model.get_grad_embeddings(pool_loader, num_samples=0, loss=torch.nn.functional.nll_loss, model_labels=True, grad_embedding_type=GradEmbeddingType.LINEAR, device=device, storage_device=\"cpu\")\n",
    "        chosen_indices = init_centers(grad_embeddings.squeeze(1).numpy(), self.acquisition_size)\n",
    "\n",
    "        return CandidateBatch(indices=chosen_indices, scores=[0.0] * len(chosen_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "# class DistilBayesianModelAdapter(torch.nn.Module):\n",
    "#     wrapped_model: BayesianModule\n",
    "#     embedding_dims: int\n",
    "#\n",
    "#     def __init__(self, wrappped_model: BayesianModule, embedding_dims: int):\n",
    "#         super().__init__()\n",
    "#\n",
    "#         self.embedding_dims = embedding_dims\n",
    "#         self.wrappped_model: BayesianModule = wrappped_model\n",
    "#\n",
    "#     def get_embedding_dim(self):\n",
    "#         return self.embedding_dims\n",
    "#\n",
    "#     def forward(self, x, last=False, freeze=False):\n",
    "#         output_B_K_C = self.wrappped_model(x, num_samples=0, return_embedding=last, freeze_encoder=freeze)\n",
    "#         return output_B_K_C.squeeze(1)\n",
    "#\n",
    "#\n",
    "# @dataclass\n",
    "# class DistilStrategyAdapter(CandidateBatchComputer):\n",
    "#     distil_strategy: Type[Strategy]\n",
    "#     distil_strategy_args: List[str]\n",
    "#\n",
    "#     def compute_candidate_batch(\n",
    "#         self, model: TrainedModel, pool_loader: torch.utils.data.DataLoader, device\n",
    "#     ) -> CandidateBatch:\n",
    "#         assert isinstance(model, TrainedBayesianModel), \"Only Bayesian models supported currently (not ensembles)!\"\n",
    "#\n",
    "#         bayesian_model: BayesianModule = model.model\n",
    "#\n",
    "#         bayesian_model.to(device=device)\n",
    "#\n",
    "#         for batch_data, _ in pool_loader:\n",
    "#             break\n",
    "#         with torch.no_grad():\n",
    "#             _, embedding = bayesian_model(batch_data.to(device=device), num_samples=0, return_embedding=True)\n",
    "#         embedding_dims = embedding.shape[2]\n",
    "#\n",
    "#         model_adapter = DistilBayesianModelAdapter(bayesian_model, embedding_dims)\n",
    "#\n",
    "#         unlabeled_dataset = LabeledToUnlabeledDataset(pool_loader.dataset)\n",
    "#         self.distil_strategy(None, None, unlabeled_x=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
