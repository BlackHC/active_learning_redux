{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> What the name says..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "\n",
    "from batchbald_redux.dataset_challenges import NamedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Dataset.__new__ = object.__new__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To speed up experiments, we are going to use Joost's FastMNIST (https://tinyurl.com/pytorch-fast-mnist), which preloads the dataset onto the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "@dataclass\n",
    "class SplitDataset:\n",
    "    input_size: int\n",
    "    num_classes: int\n",
    "    options: dict\n",
    "\n",
    "    train: data.Dataset\n",
    "    validation: data.Dataset\n",
    "    test: data.Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitDataLoader:\n",
    "    input_size: int\n",
    "    num_class: int\n",
    "    options: dict\n",
    "\n",
    "    train: data.DataLoader\n",
    "    validation: data.DataLoader\n",
    "    test: data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def train_validation_split(\n",
    "    *, full_train_dataset, full_validation_dataset, train_labels, validation_set_size, validation_split_random_state\n",
    "):\n",
    "    # Split off validation set\n",
    "    if validation_set_size > 0:\n",
    "        cv = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=validation_set_size, random_state=validation_split_random_state\n",
    "        )\n",
    "        for train_indices, validation_indices in cv.split(\n",
    "            X=np.zeros(len(full_train_dataset)), y=np.asarray(train_labels)\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "        train_dataset = data.Subset(full_train_dataset, train_indices)\n",
    "        validation_dataset = data.Subset(full_validation_dataset, validation_indices)\n",
    "    else:\n",
    "        train_dataset = data.Subset(full_train_dataset, list(range(len(full_train_dataset))))\n",
    "        validation_dataset = data.Subset(full_validation_dataset, [])\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "CIFAR10_NORMALIZE = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "\n",
    "def get_SVHN(root, validation_set_size, validation_split_random_state, normalize_like_cifar10, train_augmentation):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # NOTE: these are not correct mean and std for SVHN, but are commonly used\n",
    "    normalize = CIFAR10_NORMALIZE if normalize_like_cifar10 else transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    full_train_dataset = datasets.SVHN(root + \"data/SVHN\", split=\"train\", transform=transform, download=True)\n",
    "    full_validation_dataset = datasets.SVHN(root + \"data/SVHN\", split=\"train\", transform=transform, download=True)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        full_validation_dataset=full_validation_dataset,\n",
    "        train_labels=full_train_dataset.labels,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.SVHN(root + \"data/SVHN\", split=\"test\", transform=transform, download=True)\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "            train_augmentation=False,\n",
    "        ),\n",
    "        NamedDataset(train_dataset, f\"SVHN (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"),\n",
    "        NamedDataset(validation_dataset, f\"SVHN (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\"),\n",
    "        NamedDataset(test_dataset, \"SVHN (Test)\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CIFAR10(root, validation_set_size, validation_split_random_state, normalize_like_cifar10, train_augmentation):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            CIFAR10_NORMALIZE,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if train_augmentation:\n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                CIFAR10_NORMALIZE,\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        train_transform = test_transform\n",
    "\n",
    "    full_train_dataset = datasets.CIFAR10(root + \"data/CIFAR10\", train=True, transform=train_transform, download=True)\n",
    "    full_validation_dataset = datasets.CIFAR10(\n",
    "        root + \"data/CIFAR10\", train=True, transform=test_transform, download=True\n",
    "    )\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        full_validation_dataset=full_validation_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root + \"data/CIFAR10\", train=False, transform=test_transform, download=True)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=True,\n",
    "            train_augmentation=train_augmentation,\n",
    "        ),\n",
    "        NamedDataset(train_dataset, f\"CIFAR-10 (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"),\n",
    "        NamedDataset(validation_dataset, f\"CIFAR-10 (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\"),\n",
    "        NamedDataset(test_dataset, \"CIFAR-10 (Test)\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CIFAR100(root, validation_set_size, validation_split_random_state, normalize_like_cifar10, train_augmentation):\n",
    "    input_size = 32\n",
    "    num_classes = 100\n",
    "\n",
    "    normalize = (\n",
    "        CIFAR10_NORMALIZE\n",
    "        if normalize_like_cifar10\n",
    "        else transforms.Normalize((0.5071, 0.4866, 0.4409), (0.2673, 0.2564, 0.2762))\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if train_augmentation:\n",
    "        train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        train_transform = test_transform\n",
    "\n",
    "    full_train_dataset = datasets.CIFAR100(root + \"data/CIFAR100\", train=True, transform=train_transform, download=True)\n",
    "    full_validation_dataset = datasets.CIFAR100(\n",
    "        root + \"data/CIFAR100\", train=True, transform=test_transform, download=False\n",
    "    )\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        full_validation_dataset=full_validation_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR100(root + \"data/CIFAR100\", train=False, transform=test_transform, download=False)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "            train_augmentation=train_augmentation,\n",
    "        ),\n",
    "        NamedDataset(train_dataset, f\"CIFAR-100 (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"),\n",
    "        NamedDataset(validation_dataset, f\"CIFAR-100 (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\"),\n",
    "        NamedDataset(test_dataset, \"CIFAR-100 (Test)\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "dataset_factories = {\n",
    "    \"SVHN\": get_SVHN,\n",
    "    \"CIFAR-10\": get_CIFAR10,\n",
    "    \"CIFAR-100\": get_CIFAR100,\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    name: str,\n",
    "    *,\n",
    "    root=None,\n",
    "    validation_set_size=0,\n",
    "    validation_split_random_state=0,\n",
    "    normalize_like_cifar10=False,\n",
    "    train_augmentation=True,\n",
    "):\n",
    "    root = root if root is not None else \"./\"\n",
    "    validation_set_size = validation_set_size if validation_set_size is not None else 0\n",
    "    validation_split_random_state = validation_split_random_state if validation_split_random_state is not None else 0\n",
    "    normalize_like_cifar10 = normalize_like_cifar10 if normalize_like_cifar10 is not None else False\n",
    "    train_augmentation = train_augmentation if train_augmentation is not None else True\n",
    "\n",
    "    split_dataset = dataset_factories[name](\n",
    "        root, validation_set_size, validation_split_random_state, normalize_like_cifar10, train_augmentation\n",
    "    )\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "def get_dataloaders(split_dataset: SplitDataset, *, train_batch_size=128, eval_batch_size=512, train_shuffle=True):\n",
    "    kwargs = {\"num_workers\": 4, \"pin_memory\": True}\n",
    "\n",
    "    train_loader = data.DataLoader(split_dataset.train, batch_size=train_batch_size, shuffle=train_shuffle, **kwargs)\n",
    "\n",
    "    validation_loader = data.DataLoader(split_dataset.validation, batch_size=eval_batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = data.DataLoader(split_dataset.test, batch_size=eval_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    return SplitDataLoader(\n",
    "        split_dataset.input_size,\n",
    "        split_dataset.num_classes,\n",
    "        split_dataset.options,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        test_loader,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataloaders_by_name(\n",
    "    name: str,\n",
    "    *,\n",
    "    normalize_like_cifar10,\n",
    "    train_augmentation,\n",
    "    root=None,\n",
    "    validation_set_size=None,\n",
    "    validation_split_random_state=None,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=512,\n",
    "    train_shuffle=True,\n",
    "):\n",
    "    split_dataset = get_dataset(\n",
    "        name,\n",
    "        root=root,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "        normalize_like_cifar10=normalize_like_cifar10,\n",
    "        train_augmentation=train_augmentation,\n",
    "    )\n",
    "\n",
    "    split_dataloaders = get_dataloaders(\n",
    "        split_dataset, train_batch_size=train_batch_size, eval_batch_size=eval_batch_size, train_shuffle=train_shuffle\n",
    "    )\n",
    "\n",
    "    return split_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
