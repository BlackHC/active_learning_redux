{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> What the name says..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "\n",
    "from batchbald_redux.dataset_challenges import NamedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To speed up experiments, we are going to use Joost's FastMNIST (https://tinyurl.com/pytorch-fast-mnist), which preloads the dataset onto the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitDataset:\n",
    "    input_size: int\n",
    "    num_classes: int\n",
    "    options: dict\n",
    "\n",
    "    train: data.Dataset\n",
    "    validation: data.Dataset\n",
    "    test: data.Dataset\n",
    "\n",
    "    train_augmentations: nn.Sequential\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitDataLoader:\n",
    "    input_size: int\n",
    "    num_class: int\n",
    "    options: dict\n",
    "\n",
    "    train: data.DataLoader\n",
    "    validation: data.DataLoader\n",
    "    test: data.DataLoader\n",
    "\n",
    "    train_augmentations: nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def train_validation_split(\n",
    "    *, full_train_dataset, full_validation_dataset, train_labels, validation_set_size, validation_split_random_state\n",
    "):\n",
    "    # Split off validation set\n",
    "    if validation_set_size > 0:\n",
    "        cv = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=validation_set_size, random_state=validation_split_random_state\n",
    "        )\n",
    "        for train_indices, validation_indices in cv.split(\n",
    "            X=np.zeros(len(full_train_dataset)), y=np.asarray(train_labels)\n",
    "        ):\n",
    "            pass\n",
    "    else:\n",
    "        # Always wrap the dataset in a subset so there\n",
    "        train_indices = list(range(len(full_train_dataset)))\n",
    "        validation_indices = []\n",
    "\n",
    "    train_dataset = data.Subset(full_train_dataset, train_indices)\n",
    "    validation_dataset = data.Subset(full_validation_dataset, validation_indices)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "CIFAR10_NORMALIZE = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "\n",
    "def get_SVHN(root, validation_set_size, validation_split_random_state, normalize_like_cifar10):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # NOTE: these are not correct mean and std for SVHN, but are commonly used\n",
    "    normalize = CIFAR10_NORMALIZE if normalize_like_cifar10 else transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    full_train_dataset = datasets.SVHN(root + \"/SVHN\", split=\"train\", transform=transform, download=True)\n",
    "    full_validation_dataset = datasets.SVHN(root + \"/SVHN\", split=\"train\", transform=transform, download=True)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        full_validation_dataset=full_validation_dataset,\n",
    "        train_labels=full_train_dataset.labels,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.SVHN(root + \"/SVHN\", split=\"test\", transform=transform, download=True)\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"SVHN (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"SVHN (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"SVHN (Test)\"),\n",
    "        nn.Sequential()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CIFAR10(root, validation_set_size, validation_split_random_state, normalize_like_cifar10):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    dataset_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            CIFAR10_NORMALIZE,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_augmentations = nn.Sequential(\n",
    "        K.RandomCrop((32, 32), padding=4),\n",
    "        K.RandomHorizontalFlip(),\n",
    "    )\n",
    "\n",
    "    full_train_dataset = datasets.CIFAR10(root + \"/CIFAR10\", train=True, transform=dataset_transform, download=True)\n",
    "    full_validation_dataset = datasets.CIFAR10(\n",
    "        root + \"/CIFAR10\", train=True, transform=dataset_transform, download=True\n",
    "    )\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        full_validation_dataset=full_validation_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root + \"/CIFAR10\", train=False, transform=dataset_transform, download=True)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=True,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"CIFAR-10 (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"CIFAR-10 (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"CIFAR-10 (Test)\"),\n",
    "        train_augmentations\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CIFAR100(root, validation_set_size, validation_split_random_state, normalize_like_cifar10):\n",
    "    input_size = 32\n",
    "    num_classes = 100\n",
    "\n",
    "    normalize = (\n",
    "        CIFAR10_NORMALIZE\n",
    "        if normalize_like_cifar10\n",
    "        else transforms.Normalize((0.5071, 0.4866, 0.4409), (0.2673, 0.2564, 0.2762))\n",
    "    )\n",
    "\n",
    "    dataset_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_augmentations = nn.Sequential(\n",
    "        K.RandomCrop((32, 32), padding=4),\n",
    "        K.RandomHorizontalFlip(),\n",
    "    )\n",
    "\n",
    "    full_train_dataset = datasets.CIFAR100(root + \"/CIFAR100\", train=True, transform=dataset_transform, download=True)\n",
    "    full_validation_dataset = datasets.CIFAR100(\n",
    "        root + \"/CIFAR100\", train=True, transform=dataset_transform, download=False\n",
    "    )\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        full_validation_dataset=full_validation_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR100(root + \"/CIFAR100\", train=False, transform=dataset_transform, download=False)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"CIFAR-100 (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"CIFAR-100 (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"CIFAR-100 (Test)\"),\n",
    "        train_augmentations\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "dataset_factories = {\n",
    "    \"SVHN\": get_SVHN,\n",
    "    \"CIFAR-10\": get_CIFAR10,\n",
    "    \"CIFAR-100\": get_CIFAR100,\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    name: str,\n",
    "    *,\n",
    "    root=None,\n",
    "    validation_set_size=0,\n",
    "    validation_split_random_state=0,\n",
    "    normalize_like_cifar10=False,\n",
    "):\n",
    "    root = root if root is not None else \"./\"\n",
    "    validation_set_size = validation_set_size if validation_set_size is not None else 0\n",
    "    validation_split_random_state = validation_split_random_state if validation_split_random_state is not None else 0\n",
    "    normalize_like_cifar10 = normalize_like_cifar10 if normalize_like_cifar10 is not None else False\n",
    "\n",
    "    split_dataset = dataset_factories[name](\n",
    "        root, validation_set_size, validation_split_random_state, normalize_like_cifar10\n",
    "    )\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "def get_dataloaders(split_dataset: SplitDataset, *, train_batch_size=128, eval_batch_size=512, train_shuffle=True):\n",
    "    kwargs = {\"num_workers\": 4, \"pin_memory\": True}\n",
    "\n",
    "    train_loader = data.DataLoader(split_dataset.train, batch_size=train_batch_size, shuffle=train_shuffle, **kwargs)\n",
    "\n",
    "    validation_loader = data.DataLoader(split_dataset.validation, batch_size=eval_batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = data.DataLoader(split_dataset.test, batch_size=eval_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    return SplitDataLoader(\n",
    "        split_dataset.input_size,\n",
    "        split_dataset.num_classes,\n",
    "        split_dataset.options,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        test_loader,\n",
    "        split_dataset.train_augmentations\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataloaders_by_name(\n",
    "    name: str,\n",
    "    *,\n",
    "    normalize_like_cifar10,\n",
    "    root=None,\n",
    "    validation_set_size=None,\n",
    "    validation_split_random_state=None,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=512,\n",
    "    train_shuffle=True,\n",
    "):\n",
    "    split_dataset = get_dataset(\n",
    "        name,\n",
    "        root=root,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "        normalize_like_cifar10=normalize_like_cifar10,\n",
    "    )\n",
    "\n",
    "    split_dataloaders = get_dataloaders(\n",
    "        split_dataset, train_batch_size=train_batch_size, eval_batch_size=eval_batch_size, train_shuffle=train_shuffle\n",
    "    )\n",
    "\n",
    "    return split_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that we can load all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7f4d2c422c10>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7f4d2c34f970>, test=<torch.utils.data.dataloader.DataLoader object at 0x7f4d2c34f0d0>, train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"CIFAR-10\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=100, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7f4d2c34f400>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7f4d2c34f340>, test=<torch.utils.data.dataloader.DataLoader object at 0x7f4d2c34fd00>, train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"CIFAR-100\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7f4d7753c790>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7f4d7754da90>, test=<torch.utils.data.dataloader.DataLoader object at 0x7f4d7754db80>, train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"SVHN\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test that we can create a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=32, num_classes=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train='CIFAR-10 (Train, seed=0, 45000 samples)', validation='CIFAR-10 (Validation, seed=0, 5000 samples)', test='CIFAR-10 (Test)', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"CIFAR-10\", normalize_like_cifar10=True, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
