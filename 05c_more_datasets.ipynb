{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> What the name says..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import torch\n",
    "from ddu_dirty_mnist import DirtyMNIST, DistributionalAmbiguousMNIST\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from batchbald_redux.dataset_operations import NamedDataset, get_targets\n",
    "from batchbald_redux.cinic10 import CINIC10\n",
    "from batchbald_redux.fast_mnist import FastFashionMNIST, FastMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitDataset:\n",
    "    input_size: int\n",
    "    num_classes: int\n",
    "    options: dict\n",
    "\n",
    "    train: NamedDataset\n",
    "    validation: NamedDataset\n",
    "    test: NamedDataset\n",
    "\n",
    "    train_augmentations: nn.Sequential\n",
    "    device: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitDataLoader:\n",
    "    input_size: int\n",
    "    num_class: int\n",
    "    options: dict\n",
    "\n",
    "    train: data.DataLoader\n",
    "    validation: data.DataLoader\n",
    "    test: data.DataLoader\n",
    "\n",
    "    train_augmentations: nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def train_validation_split(*, full_train_dataset, train_labels, validation_set_size, validation_split_random_state):\n",
    "    # Split off validation set\n",
    "    if validation_set_size > 0:\n",
    "        cv = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=validation_set_size, random_state=validation_split_random_state\n",
    "        )\n",
    "        for train_indices, validation_indices in cv.split(\n",
    "            X=np.zeros(len(full_train_dataset)), y=np.asarray(torch.as_tensor(train_labels).cpu())\n",
    "        ):\n",
    "            pass\n",
    "    else:\n",
    "        # Always wrap the dataset in a subset so there\n",
    "        train_indices = list(range(len(full_train_dataset)))\n",
    "        validation_indices = []\n",
    "\n",
    "    train_dataset = data.Subset(full_train_dataset, train_indices)\n",
    "    validation_dataset = data.Subset(full_train_dataset, validation_indices)\n",
    "\n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "CIFAR10_NORMALIZE = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "CIFAR10_BATCH_NORMALIZE = K.Normalize(\n",
    "    torch.as_tensor([0.4914, 0.4822, 0.4465]), torch.as_tensor([0.2023, 0.1994, 0.2010])\n",
    ")\n",
    "CIFAR10_BATCH_DENORMALIZE = K.Denormalize(\n",
    "    torch.as_tensor([0.4914, 0.4822, 0.4465]), torch.as_tensor([0.2023, 0.1994, 0.2010])\n",
    ")\n",
    "\n",
    "CINIC10_NORMALIZE = transforms.Normalize((0.47889522, 0.47227842, 0.43047404), (0.24205776, 0.23828046, 0.25874835))\n",
    "\n",
    "\n",
    "def get_SVHN(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # NOTE: these are not correct mean and std for SVHN, but are commonly used\n",
    "    normalize = CIFAR10_NORMALIZE if normalize_like_cifar10 else transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    full_train_dataset = datasets.SVHN(root + \"/SVHN\", split=\"train\", transform=transform, download=True)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.labels,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.SVHN(root + \"/SVHN\", split=\"test\", transform=transform, download=True)\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"SVHN (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"SVHN (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"SVHN (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        \"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CIFAR10(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    dataset_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            CIFAR10_NORMALIZE,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_augmentations = nn.Sequential(\n",
    "        K.RandomCrop((32, 32), padding=4),\n",
    "        K.RandomHorizontalFlip(),\n",
    "    )\n",
    "\n",
    "    full_train_dataset = datasets.CIFAR10(root + \"/CIFAR10\", train=True, transform=dataset_transform, download=True)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root + \"/CIFAR10\", train=False, transform=dataset_transform, download=True)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=True,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"CIFAR-10 (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"CIFAR-10 (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"CIFAR-10 (Test)\"),\n",
    "        train_augmentations,\n",
    "        \"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CINIC10(\n",
    "    *, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, imagenet_only, device_hint\n",
    "):\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    input_size = 32\n",
    "    num_classes = 10\n",
    "\n",
    "    # NOTE: these are not correct mean and std for SVHN, but are commonly used\n",
    "    normalize = CIFAR10_NORMALIZE if normalize_like_cifar10 else CINIC10_NORMALIZE\n",
    "    dataset_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "    train_augmentations = nn.Sequential(\n",
    "        K.RandomCrop((32, 32), padding=4),\n",
    "        K.RandomHorizontalFlip(),\n",
    "    )\n",
    "\n",
    "    train_dataset = CINIC10(\n",
    "        root, split=\"train\", transform=dataset_transform, download=True, imagenet_only=imagenet_only\n",
    "    )\n",
    "    validation_dataset = CINIC10(\n",
    "        root, split=\"valid\", transform=dataset_transform, download=False, imagenet_only=imagenet_only\n",
    "    )\n",
    "\n",
    "    full_train_dataset = torch.utils.data.ConcatDataset([train_dataset, validation_dataset])\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=get_targets(full_train_dataset),\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = CINIC10(root, split=\"test\", transform=dataset_transform, download=False, imagenet_only=imagenet_only)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=True,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset,\n",
    "            f\"CINIC-10 (Train, imagenet_only={imagenet_only}, seed={validation_split_random_state}, {len(train_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"CINIC-10 (Validation, imagenet_only={imagenet_only}, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"CINIC-10 (Test, imagenet_only={imagenet_only})\"),\n",
    "        train_augmentations,\n",
    "        \"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_CIFAR100(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 32\n",
    "    num_classes = 100\n",
    "\n",
    "    normalize = (\n",
    "        CIFAR10_NORMALIZE\n",
    "        if normalize_like_cifar10\n",
    "        else transforms.Normalize((0.5071, 0.4866, 0.4409), (0.2673, 0.2564, 0.2762))\n",
    "    )\n",
    "\n",
    "    dataset_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_augmentations = nn.Sequential(\n",
    "        K.RandomCrop((32, 32), padding=4),\n",
    "        K.RandomHorizontalFlip(),\n",
    "    )\n",
    "\n",
    "    full_train_dataset = datasets.CIFAR100(root + \"/CIFAR100\", train=True, transform=dataset_transform, download=True)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR100(root + \"/CIFAR100\", train=False, transform=dataset_transform, download=False)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"CIFAR-100 (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"CIFAR-100 (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"CIFAR-100 (Test)\"),\n",
    "        train_augmentations,\n",
    "        \"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_MNIST(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 28\n",
    "    num_classes = 10\n",
    "\n",
    "    full_train_dataset = FastMNIST(root, train=True, download=True, device=device_hint)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = FastMNIST(\"data\", train=False, device=device_hint)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"MNIST (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"MNIST (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"MNIST (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        device_hint,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_DirtyMNIST(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 28\n",
    "    num_classes = 10\n",
    "\n",
    "    full_train_dataset = DirtyMNIST(root, train=True, download=True, device=device_hint)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset.datasets[0],\n",
    "        train_labels=full_train_dataset.datasets[0].targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = DirtyMNIST(\"data\", train=False, download=True, device=device_hint)\n",
    "\n",
    "    train_dataset = torch.utils.data.ConcatDataset([train_dataset, full_train_dataset.datasets[1]])\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"DirtyMNIST (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"MNIST (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"DirtyMNIST (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        device_hint,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_DistributionalAmbiguousMNIST(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 28\n",
    "    num_classes = 10\n",
    "\n",
    "    train_dataset = DistributionalAmbiguousMNIST(root, train=True, download=True, device=device_hint)\n",
    "\n",
    "    mnist_train_dataset = FastMNIST(root, train=True, download=True, device=device_hint)\n",
    "    _, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=mnist_train_dataset,\n",
    "        train_labels=mnist_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = DirtyMNIST(\"data\", train=False, download=True, device=device_hint)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"DirtyMNIST (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"MNIST (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"DirtyMNIST (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        device_hint,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_FashionMNIST(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 28\n",
    "    num_classes = 10\n",
    "\n",
    "    full_train_dataset = FastFashionMNIST(root, train=True, download=True, device=device_hint)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    test_dataset = FastFashionMNIST(\"data\", train=False, device=device_hint)\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"FashionMNIST (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"FashionMNIST (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"FashionMNIST (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        device_hint,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_EMNIST(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 28\n",
    "    num_classes = 47\n",
    "\n",
    "    \"\"\"\n",
    "    EMNIST ByClass: 814,255 characters. 62 unbalanced classes.\n",
    "\n",
    "    EMNIST ByMerge: 814,255 characters. 47 unbalanced classes.\n",
    "\n",
    "    EMNIST Balanced: 131,600 characters. 47 balanced classes.\n",
    "\n",
    "    EMNIST Letters: 145,600 characters. 26 balanced classes.\n",
    "\n",
    "    EMNIST Digits: 280,000 characters. 10 balanced classes.\n",
    "\n",
    "    EMNIST MNIST: 70,000 characters. 10 balanced classes.\n",
    "    \"\"\"\n",
    "\n",
    "    split = \"bymerge\"\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    full_train_dataset = datasets.EMNIST(\"data\", split=split, train=True, download=True,\n",
    "                                    transform=transform)\n",
    "\n",
    "    test_dataset = datasets.EMNIST(\"data\", split=split, train=False, transform=transform)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"EMNIST (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"EMNIST (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"EMNIST (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        device_hint,\n",
    "    )\n",
    "\n",
    "def get_balanced_EMNIST(*, root, validation_set_size, validation_split_random_state, normalize_like_cifar10, device_hint):\n",
    "    input_size = 28\n",
    "    num_classes = 47\n",
    "\n",
    "    \"\"\"\n",
    "    EMNIST ByClass: 814,255 characters. 62 unbalanced classes.\n",
    "\n",
    "    EMNIST ByMerge: 814,255 characters. 47 unbalanced classes.\n",
    "\n",
    "    EMNIST Balanced: 131,600 characters. 47 balanced classes.\n",
    "\n",
    "    EMNIST Letters: 145,600 characters. 26 balanced classes.\n",
    "\n",
    "    EMNIST Digits: 280,000 characters. 10 balanced classes.\n",
    "\n",
    "    EMNIST MNIST: 70,000 characters. 10 balanced classes.\n",
    "    \"\"\"\n",
    "\n",
    "    split = \"balanced\"\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    full_train_dataset = datasets.EMNIST(\"data\", split=split, train=True, download=True,\n",
    "                                    transform=transform)\n",
    "\n",
    "    test_dataset = datasets.EMNIST(\"data\", split=split, train=False, transform=transform)\n",
    "\n",
    "    train_dataset, validation_dataset = train_validation_split(\n",
    "        full_train_dataset=full_train_dataset,\n",
    "        train_labels=full_train_dataset.targets,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "    )\n",
    "\n",
    "    return SplitDataset(\n",
    "        input_size,\n",
    "        num_classes,\n",
    "        dict(\n",
    "            validation_split_random_state=validation_split_random_state,\n",
    "            normalize_like_cifar10=normalize_like_cifar10,\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            train_dataset, f\"EMNIST (Train, seed={validation_split_random_state}, {len(train_dataset)} samples)\"\n",
    "        ),\n",
    "        NamedDataset(\n",
    "            validation_dataset,\n",
    "            f\"EMNIST (Validation, seed={validation_split_random_state}, {len(validation_dataset)} samples)\",\n",
    "        ),\n",
    "        NamedDataset(test_dataset, \"EMNIST (Test)\"),\n",
    "        nn.Sequential(),\n",
    "        device_hint,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "dataset_factories = {\n",
    "    \"SVHN\": get_SVHN,\n",
    "    \"CIFAR-10\": get_CIFAR10,\n",
    "    \"CIFAR-100\": get_CIFAR100,\n",
    "    \"CINIC-10\": functools.partial(get_CINIC10, imagenet_only=False),\n",
    "    \"IMAGENET-CINIC-10\": functools.partial(get_CINIC10, imagenet_only=True),\n",
    "    \"MNIST\": get_MNIST,\n",
    "    \"DirtyMNIST\": get_DirtyMNIST,\n",
    "    \"DistributionalAmbiguousMNIST\": get_DistributionalAmbiguousMNIST,\n",
    "    \"EMNIST\": get_EMNIST,\n",
    "    \"BalancedEMNIST\": get_balanced_EMNIST,\n",
    "    \"FashionMNIST\": get_FashionMNIST,\n",
    "}\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    name: str,\n",
    "    *,\n",
    "    root=None,\n",
    "    validation_set_size=0,\n",
    "    validation_split_random_state=0,\n",
    "    normalize_like_cifar10=False,\n",
    "    device_hint=None,\n",
    ") -> SplitDataset:\n",
    "    root = root if root is not None else \"./\"\n",
    "    validation_set_size = validation_set_size if validation_set_size is not None else 0\n",
    "    validation_split_random_state = validation_split_random_state if validation_split_random_state is not None else 0\n",
    "    normalize_like_cifar10 = normalize_like_cifar10 if normalize_like_cifar10 is not None else False\n",
    "\n",
    "    split_dataset = dataset_factories[name](\n",
    "        root=root,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "        normalize_like_cifar10=normalize_like_cifar10,\n",
    "        device_hint=device_hint,\n",
    "    )\n",
    "    return split_dataset\n",
    "\n",
    "\n",
    "def get_dataloaders(split_dataset: SplitDataset, *, train_batch_size=128, eval_batch_size=512, train_shuffle=True):\n",
    "    # This only works well for CIFAR-10, etc and not FastMNIST, FashionMNIST!\n",
    "    kwargs = {\"num_workers\": 4, \"pin_memory\": True}\n",
    "\n",
    "    train_loader = data.DataLoader(split_dataset.train, batch_size=train_batch_size, shuffle=train_shuffle, **kwargs)\n",
    "\n",
    "    validation_loader = data.DataLoader(split_dataset.validation, batch_size=eval_batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = data.DataLoader(split_dataset.test, batch_size=eval_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    return SplitDataLoader(\n",
    "        split_dataset.input_size,\n",
    "        split_dataset.num_classes,\n",
    "        split_dataset.options,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        test_loader,\n",
    "        split_dataset.train_augmentations,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_dataloaders_by_name(\n",
    "    name: str,\n",
    "    *,\n",
    "    normalize_like_cifar10,\n",
    "    root=None,\n",
    "    validation_set_size=None,\n",
    "    validation_split_random_state=None,\n",
    "    train_batch_size=128,\n",
    "    eval_batch_size=512,\n",
    "    train_shuffle=True,\n",
    "):\n",
    "    split_dataset = get_dataset(\n",
    "        name,\n",
    "        root=root,\n",
    "        validation_set_size=validation_set_size,\n",
    "        validation_split_random_state=validation_split_random_state,\n",
    "        normalize_like_cifar10=normalize_like_cifar10,\n",
    "    )\n",
    "\n",
    "    split_dataloaders = get_dataloaders(\n",
    "        split_dataset, train_batch_size=train_batch_size, eval_batch_size=eval_batch_size, train_shuffle=train_shuffle\n",
    "    )\n",
    "\n",
    "    return split_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate that we can load all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1bbe6d0>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1bbe8e0>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1bbe700>, train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"CIFAR-10\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=100, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfd22b83d0>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfd22b88e0>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfd22b8a90>, train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"CIFAR-100\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data/SVHN/train_32x32.mat\n",
      "Using downloaded and verified file: data/SVHN/test_32x32.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfc8d9b7c0>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfc8da5760>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfc8da5850>, train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"SVHN\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=32, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fd0e467d4c0>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa83250a0>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa8325190>, train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"CINIC-10\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=28, num_class=47, options={'validation_split_random_state': 0, 'normalize_like_cifar10': False}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa8325340>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa8325ac0>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa8325b80>, train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"EMNIST\", normalize_like_cifar10=False, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=28, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': False}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa8325ca0>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fd0f428d820>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa26310a0>, train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"MNIST\", normalize_like_cifar10=False, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://github.com/BlackHC/ddu_dirty_mnist/releases/download/data-v1.0.0/amnist_samples.pt\n",
      "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/351788366/8871d700-b9a0-11eb-8967-58262fa53f00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220511%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220511T140512Z&X-Amz-Expires=300&X-Amz-Signature=5e6e71e4fcc5d32f026b3e8ce0d98aef10e1193cd56334cb8c35067f283b9c74&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=351788366&response-content-disposition=attachment%3B%20filename%3Damnist_samples.pt&response-content-type=application%2Foctet-stream to data/AmbiguousMNIST/amnist_samples.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fac437de794832aed352bd365e7219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37632824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://github.com/BlackHC/ddu_dirty_mnist/releases/download/data-v1.0.0/amnist_labels.pt\n",
      "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/351788366/7c861500-b9a0-11eb-9ce2-95aa861ab2bb?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220511%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220511T140516Z&X-Amz-Expires=300&X-Amz-Signature=8141e3f2121087f551d698c415fa724408c6d34c7ea59d9e803fb1b33684734d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=351788366&response-content-disposition=attachment%3B%20filename%3Damnist_labels.pt&response-content-type=application%2Foctet-stream to data/AmbiguousMNIST/amnist_labels.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ab1212dc444c06a9eef662945bdf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/960760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=28, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': False}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa2631910>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1bcf3a0>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1bcf490>, train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"DirtyMNIST\", normalize_like_cifar10=False, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://github.com/BlackHC/ddu_dirty_mnist/releases/download/data-v1.0.0/amnist_samples.pt\n",
      "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/351788366/8871d700-b9a0-11eb-8967-58262fa53f00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220511%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220511T142209Z&X-Amz-Expires=300&X-Amz-Signature=92542579304ce959520fb2f1d963a810cfaf86a2a933408e5fe8f50415dcb6c8&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=351788366&response-content-disposition=attachment%3B%20filename%3Damnist_samples.pt&response-content-type=application%2Foctet-stream to data/DistributionalAmbiguousMNIST/amnist_samples.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c5549353c14a148d9348cf66080be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37632824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://github.com/BlackHC/ddu_dirty_mnist/releases/download/data-v1.0.0/amnist_predictions.pt\n",
      "Downloading https://objects.githubusercontent.com/github-production-release-asset-2e65be/351788366/7d1eab80-b9a0-11eb-9615-f8f82b460fc5?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220511%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220511T142213Z&X-Amz-Expires=300&X-Amz-Signature=4bdb6b3b284b422a3f20b959029e0b6fd94ce9c6dd3ec970e9d225de58c6d78a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=351788366&response-content-disposition=attachment%3B%20filename%3Damnist_predictions.pt&response-content-type=application%2Foctet-stream to data/DistributionalAmbiguousMNIST/amnist_predictions.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a4ae5ef1a1415c9d78e5b7d46982f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/480760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataLoader(input_size=28, num_class=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': False}, train=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1bbeb50>, validation=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1b878b0>, test=<torch.utils.data.dataloader.DataLoader object at 0x7fcfa1b874c0>, train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataloaders_by_name(\"DistributionalAmbiguousMNIST\", normalize_like_cifar10=False, root=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate that we can create a validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=32, num_classes=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train='CIFAR-10 (Train, seed=0, 45000 samples)', validation='CIFAR-10 (Validation, seed=0, 5000 samples)', test='CIFAR-10 (Test)', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"CIFAR-10\", normalize_like_cifar10=True, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=28, num_classes=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train='MNIST (Train, seed=0, 55000 samples)', validation='MNIST (Validation, seed=0, 5000 samples)', test='MNIST (Test)', train_augmentations=Sequential())"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"MNIST\", normalize_like_cifar10=True, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=28, num_classes=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train='DirtyMNIST (Train, seed=0, 115000 samples)', validation='MNIST (Validation, seed=0, 5000 samples)', test='DirtyMNIST (Test)', train_augmentations=Sequential(), device=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"DirtyMNIST\", normalize_like_cifar10=True, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=28, num_classes=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train='DirtyMNIST (Train, seed=0, 6000 samples)', validation='MNIST (Validation, seed=0, 5000 samples)', test='DirtyMNIST (Test)', train_augmentations=Sequential(), device=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"DistributionalAmbiguousMNIST\", normalize_like_cifar10=True, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=32, num_classes=10, options={'validation_split_random_state': 0, 'normalize_like_cifar10': True}, train='CINIC-10 (Train, imagenet_only=True, seed=0, 135000 samples)', validation='CINIC-10 (Validation, imagenet_only=True, seed=0, 5000 samples)', test='CINIC-10 (Test, imagenet_only={imagenet_only})', train_augmentations=Sequential(\n",
       "  (0): RandomCrop(crop_size=(32, 32), padding=4, fill=0, pad_if_needed=False, padding_mode=constant, resample=BILINEAR, p=1.0, p_batch=1.0, same_on_batch=False, return_transform=False)\n",
       "  (1): RandomHorizontalFlip(p=0.5, p_batch=1.0, same_on_batch=False, return_transform=None)\n",
       "), device='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"IMAGENET-CINIC-10\", normalize_like_cifar10=True, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SplitDataset(input_size=28, num_classes=47, options={'validation_split_random_state': 0, 'normalize_like_cifar10': False}, train='EMNIST (Train, seed=0, 692932 samples)', validation='EMNIST (Validation, seed=0, 5000 samples)', test='EMNIST (Test)', train_augmentations=Sequential(), device=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "get_dataset(\"EMNIST\", normalize_like_cifar10=False, root=\"data\", validation_set_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Kornia augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow\n",
    "\n",
    "import kornia\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "split_dataloaders = get_dataloaders_by_name(\"CIFAR-10\", normalize_like_cifar10=True, root=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# slow\n",
    "\n",
    "for image_batch, labels in split_dataloaders.train:\n",
    "    break\n",
    "\n",
    "print(image_batch.shape)\n",
    "\n",
    "image_rgb: np.ndarray = kornia.tensor_to_image(CIFAR10_BATCH_DENORMALIZE(image_batch))\n",
    "fig, axs = plt.subplots(1, 1, figsize=(2, 2))\n",
    "\n",
    "axs.axis(\"off\")\n",
    "axs.imshow(image_rgb[0])\n",
    "plt.show()\n",
    "\n",
    "image_rgb: np.ndarray = kornia.tensor_to_image(\n",
    "    CIFAR10_BATCH_DENORMALIZE(split_dataloaders.train_augmentations(image_batch))\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(2, 2))\n",
    "axs.axis(\"off\")\n",
    "axs.imshow(image_rgb[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
