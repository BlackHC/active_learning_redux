{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring Early Stopping\n",
    "> “The past is never where you think you left it.”\n",
    ">\n",
    "> &mdash; <cite>Katherine Anne Porter</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp restoring_early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "from math import inf\n",
    "from typing import Callable, Optional, Dict\n",
    "\n",
    "import torch\n",
    "from ignite.engine import Engine, Events\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import EPOCH_DEPRECATION_WARNING\n",
    "\n",
    "\n",
    "class RestoringEarlyStopping(object):\n",
    "    \"\"\"RestoringEarlyStopping handler can be used to stop the training if no improvement after a given number of events\n",
    "\n",
    "    Args:\n",
    "        patience (int):\n",
    "            Number of events to wait if no improvement and then stop the training\n",
    "        score_function (Callable):\n",
    "            It should be a function taking a single argument, an `ignite.engine.Engine` object,\n",
    "            and return a score `float`. An improvement is considered if the score is higher.\n",
    "        trainer (Engine):\n",
    "            trainer engine to stop the run if no improvement\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        from ignite.engine import Engine, Events\n",
    "        from ignite.handlers import EarlyStopping\n",
    "\n",
    "        def score_function(engine):\n",
    "            val_loss = engine.state.metrics['nll']\n",
    "            return -val_loss\n",
    "\n",
    "        handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)\n",
    "        # Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset)\n",
    "        evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        patience,\n",
    "        score_function,\n",
    "        training_engine: Engine,\n",
    "        validation_engine: Engine,\n",
    "        module: torch.nn.Module = None,\n",
    "        optimizer: torch.optim.Optimizer = None,\n",
    "        out_of_patience_callback=None,\n",
    "    ):\n",
    "        if out_of_patience_callback is None:\n",
    "\n",
    "            def default_out_of_patience_callback():\n",
    "                training_engine.terminate()\n",
    "\n",
    "            out_of_patience_callback = default_out_of_patience_callback\n",
    "\n",
    "        if not callable(score_function):\n",
    "            raise TypeError(\"Argument score_function should be a function\")\n",
    "\n",
    "        if patience < 0:\n",
    "            raise ValueError(\"Argument patience should be non-negative integer\")\n",
    "\n",
    "        self.score_function = score_function\n",
    "        self.out_of_patience_callback = out_of_patience_callback\n",
    "        self.module = module\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "\n",
    "        self.best_score = None\n",
    "        self.best_epoch = None\n",
    "        self.best_module_state_dict = None\n",
    "        self.best_optimizer_state_dict = None\n",
    "        self.restore_epoch = None\n",
    "\n",
    "        self.training_engine = training_engine\n",
    "        self.validation_engine = validation_engine\n",
    "        validation_engine.add_event_handler(Events.EPOCH_COMPLETED, self.on_epoch_completed)\n",
    "        training_engine.add_event_handler(Events.COMPLETED, self.on_completed)\n",
    "\n",
    "    def snapshot(self):\n",
    "        self.best_epoch = self.training_engine.state.epoch\n",
    "        if self.module is not None:\n",
    "            self.best_module_state_dict = pickle.dumps(self.module.state_dict(keep_vars=False))\n",
    "        if self.optimizer is not None:\n",
    "            self.best_optimizer_state_dict = pickle.dumps(self.optimizer.state_dict())\n",
    "\n",
    "    def restore_best(self):\n",
    "        if self.best_module_state_dict is not None and self.module is not None:\n",
    "            print(f\"RestoringEarlyStopping: Restoring best parameters. (Score: {self.best_score})\")\n",
    "            self.module.load_state_dict(pickle.loads(self.best_module_state_dict))\n",
    "\n",
    "        if self.best_optimizer_state_dict is not None and self.optimizer is not None:\n",
    "            print(\"RestoringEarlyStopping: Restoring optimizer.\")\n",
    "            self.optimizer.load_state_dict(pickle.loads(self.best_optimizer_state_dict))\n",
    "\n",
    "    def on_epoch_completed(self, _):\n",
    "        score = self.score_function()\n",
    "\n",
    "        if self.best_score is not None and score <= self.best_score:\n",
    "            self.counter += 1\n",
    "            print(\"RestoringEarlyStopping: %i / %i\" % (self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"RestoringEarlyStopping: Out of patience\")\n",
    "                self.restore_best()\n",
    "                self.restore_epoch = self.training_engine.state.epoch\n",
    "\n",
    "                # Reset the counter in case we keep training after adjusting the model.\n",
    "                self.counter = 0\n",
    "                self.out_of_patience_callback()\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.snapshot()\n",
    "            self.counter = 0\n",
    "\n",
    "    def on_completed(self, _):\n",
    "        if self.restore_epoch is None or self.restore_epoch < self.training_engine.state.epoch:\n",
    "            self.restore_best()\n",
    "            self.restore_epoch = self.training_engine.state.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class PatienceWithSnapshot(object):\n",
    "    \"\"\"PatienceSnapshot handler can be used to stop the training if no improvement after a given number of events\n",
    "\n",
    "    Args:\n",
    "        patience (int):\n",
    "            Number of events to wait if no improvement and then stop the training\n",
    "        score_function (Callable):\n",
    "            It should be a function taking a single argument, an `ignite.engine.Engine` object,\n",
    "            and return a score `float`. An improvement is considered if the score is higher.\n",
    "        trainer (Engine):\n",
    "            trainer engine to stop the run if no improvement\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "        from ignite.engine import Engine, Events\n",
    "        from ignite.handlers import EarlyStopping\n",
    "\n",
    "        def score_function(engine):\n",
    "            val_loss = engine.state.metrics['nll']\n",
    "            return -val_loss\n",
    "\n",
    "        handler = EarlyStopping(patience=10, score_function=score_function, trainer=trainer)\n",
    "        # Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset)\n",
    "        evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        name: str = \"\",\n",
    "        patience,\n",
    "        score_function,\n",
    "        training_engine: Engine,\n",
    "        validation_engine: Engine,\n",
    "        module: torch.nn.Module = None,\n",
    "        optimizer: torch.optim.Optimizer = None,\n",
    "        out_of_patience_callback=None,\n",
    "    ):\n",
    "        if out_of_patience_callback is None:\n",
    "\n",
    "            def default_out_of_patience_callback(module_state_dict, optimizer_state_dict):\n",
    "                training_engine.terminate()\n",
    "\n",
    "            out_of_patience_callback = default_out_of_patience_callback\n",
    "\n",
    "        if not callable(score_function):\n",
    "            raise TypeError(\"Argument score_function should be a function\")\n",
    "\n",
    "        if patience < 0:\n",
    "            raise ValueError(\"Argument patience should be non-negative integer\")\n",
    "\n",
    "        self.name = name\n",
    "        self.score_function = score_function\n",
    "        self.out_of_patience_callback = out_of_patience_callback\n",
    "        self.module = module\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "\n",
    "        self.best_score = None\n",
    "        self.best_epoch = None\n",
    "        self.best_module_state_dict = None\n",
    "        self.best_optimizer_state_dict = None\n",
    "        self.callback_epoch = None\n",
    "\n",
    "        self.training_engine = training_engine\n",
    "        self.validation_engine = validation_engine\n",
    "        validation_engine.add_event_handler(Events.EPOCH_COMPLETED, self.on_epoch_completed)\n",
    "        training_engine.add_event_handler(Events.COMPLETED, self.on_completed)\n",
    "\n",
    "    def is_out_of_patience(self):\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def snapshot(self):\n",
    "        self.best_epoch = self.training_engine.state.epoch\n",
    "        if self.module is not None:\n",
    "            self.best_module_state_dict = pickle.dumps(self.module.state_dict(keep_vars=False))\n",
    "        if self.optimizer is not None:\n",
    "            self.best_optimizer_state_dict = pickle.dumps(self.optimizer.state_dict())\n",
    "\n",
    "    def restore(self, module_state_dict, optimizer_state_dict):\n",
    "        if self.best_module_state_dict is not None and self.module is not None:\n",
    "            print(f\"{self.name}PatienceWithSnapshot: Restoring best parameters. (Score: {self.best_score})\")\n",
    "            self.module.load_state_dict(pickle.loads(self.best_module_state_dict))\n",
    "\n",
    "        if self.best_optimizer_state_dict is not None and self.optimizer is not None:\n",
    "            print(f\"{self.name}PatienceWithSnapshot: Restoring optimizer.\")\n",
    "            self.optimizer.load_state_dict(pickle.loads(self.best_optimizer_state_dict))\n",
    "\n",
    "    def on_epoch_completed(self, _):\n",
    "        score = self.score_function()\n",
    "\n",
    "        if self.best_score is not None and score <= self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f\"{self.name}PatienceWithSnapshot: %i / %i\" % (self.counter, self.patience))\n",
    "            if self.counter == self.patience:\n",
    "                print(f\"{self.name}PatienceWithSnapshot: Out of patience\")\n",
    "                print(f\"{self.name}PatienceWithSnapshot: Best score: {self.best_score})\")\n",
    "                self.callback_epoch = self.training_engine.state.epoch\n",
    "\n",
    "                module_state_dict = pickle.loads(self.best_module_state_dict)\n",
    "                optimizer_state_dict = pickle.loads(self.best_optimizer_state_dict)\n",
    "                self.out_of_patience_callback(module_state_dict, optimizer_state_dict)\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.snapshot()\n",
    "            self.counter = 0\n",
    "\n",
    "    def on_completed(self, _):\n",
    "        if self.callback_epoch is None or self.callback_epoch < self.training_engine.state.epoch:\n",
    "            self.callback_epoch = self.training_engine.state.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "def suggest_limit_schedule(patience_schedule, max_epochs, factor=2):\n",
    "    backward_limit_schedule = []\n",
    "    current_epoch = max_epochs\n",
    "    for patience in reversed(patience_schedule):\n",
    "        current_epoch -= factor * patience\n",
    "        backward_limit_schedule += [current_epoch]\n",
    "\n",
    "    limit_schedule = list(reversed(backward_limit_schedule))\n",
    "    if limit_schedule[0] < 0:\n",
    "        limit_schedule = []\n",
    "        current_epoch = 0\n",
    "        for patience in patience_schedule:\n",
    "            current_epoch += factor * patience\n",
    "            limit_schedule += [current_epoch]\n",
    "        max_epochs = current_epoch\n",
    "\n",
    "    return limit_schedule, max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([60, 100], 120)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_limit_schedule([20, 10], 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([60, 100, 140], 140)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_limit_schedule([30, 20, 20], 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class ModuleOptimizerSnapshotter:\n",
    "    module: torch.nn.Module\n",
    "    optimizer: torch.optim.Optimizer\n",
    "    module_state_dict: Optional\n",
    "    optimizer_state_dict: Optional\n",
    "\n",
    "    def __init__(self, module: torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
    "      self.module = module\n",
    "      self.optimizer = optimizer\n",
    "      self.module_state_dict = None\n",
    "      self.optimizer_state_dict = None\n",
    "\n",
    "    def snapshot(self):\n",
    "        if self.module is not None:\n",
    "            self.module_state_dict = pickle.dumps(self.module.state_dict(keep_vars=False))\n",
    "\n",
    "            # Only take a snapshot of the optimizer if we also have a module.\n",
    "            if self.optimizer is not None:\n",
    "                self.optimizer_state_dict = pickle.dumps(self.optimizer.state_dict())\n",
    "\n",
    "    def restore(self):\n",
    "        if self.module_state_dict is not None and self.module is not None:\n",
    "            self.module.load_state_dict(pickle.loads(self.module_state_dict))\n",
    "\n",
    "            # Only restore if we also restore a module (otherwise... messy?)\n",
    "            if self.optimizer_state_dict is not None and self.optimizer is not None:\n",
    "                self.optimizer.load_state_dict(pickle.loads(self.optimizer_state_dict))\n",
    "\n",
    "\n",
    "class ReduceLROnPlateauWithSchedule(object):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This scheduler reads a metrics\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        mode (str): One of `min`, `max`. In `min` mode, lr will\n",
    "            be reduced when the quantity monitored has stopped\n",
    "            decreasing; in `max` mode it will be reduced when the\n",
    "            quantity monitored has stopped increasing. Default: 'min'.\n",
    "        factor (float): Factor by which the learning rate will be\n",
    "            reduced. new_lr = lr * factor. Default: 0.1.\n",
    "        patience (int): Number of epochs with no improvement after\n",
    "            which learning rate will be reduced. For example, if\n",
    "            `patience = 2`, then we will ignore the first 2 epochs\n",
    "            with no improvement, and will only decrease the LR after the\n",
    "            3rd epoch if the loss still hasn't improved then.\n",
    "            Default: 10.\n",
    "        threshold (float): Threshold for measuring the new optimum,\n",
    "            to only focus on significant changes. Default: 1e-4.\n",
    "        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
    "            dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
    "            mode or best * ( 1 - threshold ) in `min` mode.\n",
    "            In `abs` mode, dynamic_threshold = best + threshold in\n",
    "            `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
    "        cooldown (int): Number of epochs to wait before resuming\n",
    "            normal operation after lr has been reduced. Default: 0.\n",
    "        min_lr (float or list): A scalar or a list of scalars. A\n",
    "            lower bound on the learning rate of all param groups\n",
    "            or each group respectively. Default: 0.\n",
    "        eps (float): Minimal decay applied to lr. If the difference\n",
    "            between new and old lr is smaller than eps, the update is\n",
    "            ignored. Default: 1e-8.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     train(...)\n",
    "        >>>     val_loss = validate(...)\n",
    "        >>>     # Note that step should be called after validate()\n",
    "        >>>     scheduler.step(val_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        factor_schedule: [float],\n",
    "        patience_schedule: [int],\n",
    "        limit_schedule: [int],\n",
    "        end_callback: Callable,\n",
    "        next_era_callback: Callable,\n",
    "        mode=\"min\",\n",
    "        threshold=1e-4,\n",
    "        threshold_mode=\"rel\",\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        eps=1e-8,\n",
    "        verbose=False,\n",
    "        module: torch.nn.Module = None,\n",
    "    ):\n",
    "        if any(factor >= 1.0 for factor in factor_schedule):\n",
    "            raise ValueError(\"Factor should be < 1.0.\")\n",
    "        self.factor_schedule = factor_schedule\n",
    "\n",
    "        # Attach optimizer\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError(\"{} is not an Optimizer\".format(type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.snapshotter = ModuleOptimizerSnapshotter(module=module, optimizer=optimizer)\n",
    "\n",
    "        if isinstance(min_lr, list) or isinstance(min_lr, tuple):\n",
    "            if len(min_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} min_lrs, got {}\".format(len(optimizer.param_groups), len(min_lr)))\n",
    "            self.min_lrs = list(min_lr)\n",
    "        else:\n",
    "            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.era = 0\n",
    "        self.patience = patience_schedule[0]\n",
    "        self.factor = factor_schedule[0]\n",
    "        self.limit = limit_schedule[0]\n",
    "\n",
    "        self.end_callback = end_callback\n",
    "        self.next_era_callback = next_era_callback\n",
    "\n",
    "        self.patience_schedule = patience_schedule\n",
    "        self.limit_schedule = limit_schedule\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "        self.best = None\n",
    "        self.best_epoch = None\n",
    "        self.num_bad_epochs = None\n",
    "        self.mode_worse = None  # the worse value for the chosen mode\n",
    "        self.eps = eps\n",
    "        self.last_epoch = 0\n",
    "        self._init_is_better(mode=mode, threshold=threshold, threshold_mode=threshold_mode)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\n",
    "        self.best = self.mode_worse\n",
    "        self.best_epoch = None\n",
    "        self.cooldown_counter = 0\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, metrics, epoch=None):\n",
    "        # convert `metrics` to float, in case it's a zero-dim Tensor\n",
    "        current = float(metrics)\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        else:\n",
    "            warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        if self.is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.best_epoch = epoch\n",
    "            self.num_bad_epochs = 0\n",
    "            self.snapshotter.snapshot()\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}: {current} worse than {self.best}, patience: {self.num_bad_epochs}/{self.patience}!\"\n",
    "                )\n",
    "\n",
    "        if self.in_cooldown:\n",
    "            self.cooldown_counter -= 1\n",
    "            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n",
    "\n",
    "        if self.num_bad_epochs > self.patience or self.last_epoch >= self.limit:\n",
    "            if self.snapshotter.module:\n",
    "              print(f\"Restoring best snapshot: best_score: {self.best} @ epoch {self.best_epoch}\")\n",
    "              self.snapshotter.restore()\n",
    "\n",
    "            self._reduce_lr(epoch)\n",
    "            self.cooldown_counter = self.cooldown\n",
    "            self.num_bad_epochs = 0\n",
    "\n",
    "        self._last_lr = [group[\"lr\"] for group in self.optimizer.param_groups]\n",
    "\n",
    "    def _reduce_lr(self, epoch):\n",
    "        self.era += 1\n",
    "        if self.era >= len(self.patience_schedule):\n",
    "            if self.end_callback:\n",
    "                self.end_callback()\n",
    "            return\n",
    "        else:\n",
    "            self.patience = self.patience_schedule[self.era]\n",
    "            self.limit = self.limit_schedule[self.era]\n",
    "            if self.next_era_callback:\n",
    "                self.next_era_callback()\n",
    "\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            old_lr = float(param_group[\"lr\"])\n",
    "            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n",
    "            if old_lr - new_lr > self.eps:\n",
    "                param_group[\"lr\"] = new_lr\n",
    "                if self.verbose:\n",
    "                    print(\"Epoch {:5d}: reducing learning rate\" \" of group {} to {:.4e}.\".format(epoch, i, new_lr))\n",
    "\n",
    "        if self.era < len(self.factor_schedule):\n",
    "            self.factor = self.factor_schedule[self.era]\n",
    "\n",
    "    @property\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n",
    "    def is_better(self, a, best):\n",
    "        if self.mode == \"min\" and self.threshold_mode == \"rel\":\n",
    "            rel_epsilon = 1.0 - self.threshold\n",
    "            return a < best * rel_epsilon\n",
    "\n",
    "        elif self.mode == \"min\" and self.threshold_mode == \"abs\":\n",
    "            return a < best - self.threshold\n",
    "\n",
    "        elif self.mode == \"max\" and self.threshold_mode == \"rel\":\n",
    "            rel_epsilon = self.threshold + 1.0\n",
    "            return a > best * rel_epsilon\n",
    "\n",
    "        else:  # mode == 'max' and epsilon_mode == 'abs':\n",
    "            return a > best + self.threshold\n",
    "\n",
    "    def _init_is_better(self, mode, threshold, threshold_mode):\n",
    "        if mode not in {\"min\", \"max\"}:\n",
    "            raise ValueError(\"mode \" + mode + \" is unknown!\")\n",
    "        if threshold_mode not in {\"rel\", \"abs\"}:\n",
    "            raise ValueError(\"threshold mode \" + threshold_mode + \" is unknown!\")\n",
    "\n",
    "        if mode == \"min\":\n",
    "            self.mode_worse = inf\n",
    "        else:  # mode == 'max':\n",
    "            self.mode_worse = -inf\n",
    "\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {key: value for key, value in self.__dict__.items() if key != \"optimizer\"}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.__dict__.update(state_dict)\n",
    "        self._init_is_better(mode=self.mode, threshold=self.threshold, threshold_mode=self.threshold_mode)\n",
    "\n",
    "\n",
    "def lr_step_after_epoch(trainer, scheduler):\n",
    "    if isinstance(scheduler, ReduceLROnPlateauWithScheduleWrapper):\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def lr_step(engine):\n",
    "            scheduler.step(engine)\n",
    "\n",
    "    elif isinstance(scheduler, ReduceLROnPlateauWithSchedule):\n",
    "        warnings.warn(\n",
    "            \"Use ReduceLROnPlateauWrapper instead of torch.optim.lr_scheduler.ReduceLROnPlateau!\", DeprecationWarning\n",
    "        )\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def lr_step(engine):\n",
    "            scheduler.step(engine.state.output.loss)\n",
    "\n",
    "    elif isinstance(scheduler, torch.optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "\n",
    "        @trainer.on(Events.GET_BATCH_COMPLETED)\n",
    "        def lr_step(engine):\n",
    "            scheduler.step(engine.state.epoch + engine.state.iteration / engine.state.epoch_length)\n",
    "\n",
    "    else:\n",
    "\n",
    "        @trainer.on(Events.EPOCH_COMPLETED)\n",
    "        def lr_step(engine):\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "class ReduceLROnPlateauWithScheduleWrapper(ReduceLROnPlateauWithSchedule):\n",
    "    output_transform: Optional[Callable]\n",
    "    metrics_transform: Optional[Callable]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        *,\n",
    "        output_transform: Optional[Callable] = None,\n",
    "        metrics_transform: Optional[Callable] = None,\n",
    "        factor_schedule: [float],\n",
    "        patience_schedule: [int],\n",
    "        limit_schedule: [int],\n",
    "        next_era_callback: Callable,\n",
    "        end_callback: Callable,\n",
    "        mode=\"min\",\n",
    "        threshold=1e-4,\n",
    "        threshold_mode=\"rel\",\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        eps=1e-8,\n",
    "        verbose=False,\n",
    "        module: torch.nn.Module = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            optimizer,\n",
    "            factor_schedule=factor_schedule,\n",
    "            patience_schedule=patience_schedule,\n",
    "            limit_schedule=limit_schedule,\n",
    "            end_callback=end_callback,\n",
    "            next_era_callback=next_era_callback,\n",
    "            mode=mode,\n",
    "            threshold=threshold,\n",
    "            threshold_mode=threshold_mode,\n",
    "            cooldown=cooldown,\n",
    "            min_lr=min_lr,\n",
    "            eps=eps,\n",
    "            verbose=verbose,\n",
    "            module=module\n",
    "        )\n",
    "        self.metrics_transform = metrics_transform\n",
    "        self.output_transform = output_transform\n",
    "        assert self.metrics_transform or self.output_transform\n",
    "        assert not self.metrics_transform or not self.output_transform\n",
    "\n",
    "    # noinspection PyMethodOverriding\n",
    "    def step(self, engine: Engine):\n",
    "        if self.output_transform:\n",
    "            super().step(self.output_transform(engine.state.output))\n",
    "        elif self.metrics_transform:\n",
    "            super().step(self.metrics_transform(engine.state.metrics))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
