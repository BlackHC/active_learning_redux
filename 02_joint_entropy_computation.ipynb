{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp joint_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Entropies\n",
    "> Computing joint entropies exactly or by using importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module helps compute joint entropies for dependent categorical variables given via a density $p((y_i)_i|w))$ in the Bayesian setting. We compute the density $p((y_i)_i)$ by marginalizing over $w$.\n",
    "\n",
    "Two cases are implemented:\n",
    "\n",
    "* exact joint entropies (which works for up 5 to joint variables depending on memory and # of classes);\n",
    "* estimated joint entropies using importance sampling of configurations.\n",
    "\n",
    "Note: \"exact\" based on the given draws of $w$. They are still an approximation because we do not integrate over $w$ but use Monte-Carlo samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of inference samples `K`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from toma import toma\n",
    "from blackhc.progress_bar import create_progress_bar\n",
    "\n",
    "\n",
    "def compute_conditional_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    log_probs_N_K_C = log_probs_N_K_C.to(torch.double)\n",
    "\n",
    "    pbar = create_progress_bar(\n",
    "        N, tqdm_args=dict(desc=\"Conditional Entropy\", leave=False)\n",
    "    )\n",
    "    pbar.start()\n",
    "\n",
    "    @toma.execute.chunked(log_probs_N_K_C, 65536)\n",
    "    def compute(log_probs_n_K_C, start: int, end: int):\n",
    "        nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)\n",
    "        nats_n_K_C[torch.isnan(nats_n_K_C)] = 0.0\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_K_C, dim=(1, 2)) / K)\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.finish()\n",
    "\n",
    "    return entropies_N\n",
    "\n",
    "\n",
    "def compute_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:\n",
    "    N, K, C = log_probs_N_K_C.shape\n",
    "\n",
    "    entropies_N = torch.empty(N, dtype=torch.double)\n",
    "\n",
    "    log_probs_N_K_C = log_probs_N_K_C.to(torch.double)\n",
    "\n",
    "    pbar = create_progress_bar(N, tqdm_args=dict(desc=\"Entropy\", leave=False))\n",
    "    pbar.start()\n",
    "\n",
    "    @toma.execute.chunked(log_probs_N_K_C, 65536)\n",
    "    def compute(log_probs_n_K_C, start: int, end: int):\n",
    "        mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)\n",
    "        nats_n_C = mean_log_probs_n_C * torch.exp(mean_log_probs_n_C)\n",
    "        nats_n_C[torch.isnan(nats_n_C)] = 0.0\n",
    "\n",
    "        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1))\n",
    "        pbar.update(end - start)\n",
    "\n",
    "    pbar.finish()\n",
    "\n",
    "    return entropies_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run tests, we need a few sampled distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-a4f3ef56f743>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.stack(list(map(torch.as_tensor, l)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mixture_prob_dist(p1, p2, m):\n",
    "    return (1.0 - m) * np.asarray(p1) + m * np.asarray(p2)\n",
    "\n",
    "\n",
    "p1 = [0.1, 0.2, 0.2, 0.5]\n",
    "p2 = [0.5, 0.2, 0.1, 0.2]\n",
    "y1_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "p1 = [0.1, 0.6, 0.2, 0.1]\n",
    "p2 = [0.0, 0.5, 0.5, 0.0]\n",
    "y2_ws = [get_mixture_prob_dist(p1, p2, m) for m in np.linspace(0, 1, K)]\n",
    "\n",
    "\n",
    "def nested_to_tensor(*l):\n",
    "    return torch.stack(list(map(torch.as_tensor, l)))\n",
    "\n",
    "\n",
    "ys_ws = nested_to_tensor(y1_ws, y2_ws, y1_ws, y2_ws, y1_ws, y2_ws, y1_ws, y2_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "p = [0.25, 0.25, 0.25, 0.25]\n",
    "yu_ws = [p for m in range(K)]\n",
    "yus_ws = nested_to_tensor(yu_ws, yu_ws, yu_ws, yu_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JointEntropy interface\n",
    "\n",
    "Before we look at any implementations, we want to define an interface that we want to support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "class JointEntropy:\n",
    "    \"\"\"Random variables (all with the same # of categories $C$) can be added via `JointEntropy.add_variables`.\n",
    "\n",
    "    `JointEntropy.compute` computes the joint entropy.\n",
    "\n",
    "    `JointEntropy.compute_batch` computes the joint entropy of the added variables with each of the variables in the provided batch probabilities in turn.\"\"\"\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Computes the entropy of this joint entropy.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor) -> \"JointEntropy\":\n",
    "        \"\"\"Expands the joint entropy to include more terms.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_batch(\n",
    "        self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def lazy_compute_batch(\n",
    "        self,\n",
    "        log_probs_B_K_C: torch.Tensor,\n",
    "        previous_entropies_B: torch.Tensor,\n",
    "        top_k: int,\n",
    "        output_entropies_B=None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the joint entropy of the added variables together with the batch (one by one) using a lazy-greedy approach.\n",
    "\n",
    "        For this previous_entropies_B has to be sorted descending. We only support a top-k version here.\n",
    "\n",
    "        The idea is that once the number of updated scores > largest not-updated score is larger than k, the top-k can\n",
    "        only come from the updated scores.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.add_variables\" class=\"doc_header\"><code>JointEntropy.add_variables</code><a href=\"__main__.py#L15\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.compute\" class=\"doc_header\"><code>JointEntropy.compute</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"JointEntropy.compute_batch\" class=\"doc_header\"><code>JointEntropy.compute_batch</code><a href=\"__main__.py#L19\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>JointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(JointEntropy.add_variables)\n",
    "show_doc(JointEntropy.compute)\n",
    "show_doc(JointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact Joint Entropies\n",
    "\n",
    "To compute exact joint entropies, we have to compute all possible configurations of the $y_i$ and evaluate_old $p(y_1, \\dots, y_n)$ by averaging over $p(y_1, \\dots, y_n|w)$.\n",
    "\n",
    "The number of samples $M=C^N$, where $N$ is the number of variables in the joint and $C$ is the number of classes.\n",
    "\n",
    "For this, we provide a class `ExactJointEntropy` that takes $K$ and starts with no variables in the joint.\n",
    "\n",
    "### In the Paper\n",
    "![Version in the paper](batchbald_exact_joint_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class ExactJointEntropy(JointEntropy):\n",
    "    joint_probs_M_K: torch.Tensor\n",
    "\n",
    "    def __init__(self, joint_probs_M_K: torch.Tensor):\n",
    "        self.joint_probs_M_K = joint_probs_M_K\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(K: int, device=None, dtype=None) -> \"ExactJointEntropy\":\n",
    "        return ExactJointEntropy(torch.ones((1, K), device=device, dtype=dtype))\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        probs_M = torch.mean(self.joint_probs_M_K, dim=1, keepdim=False)\n",
    "        nats_M = -torch.log(probs_M) * probs_M\n",
    "        nats_M[torch.isnan(nats_M)] = 0.0\n",
    "        del probs_M\n",
    "        entropy = torch.sum(nats_M)\n",
    "        del nats_M\n",
    "        return entropy\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor) -> \"ExactJointEntropy\":\n",
    "        assert self.joint_probs_M_K.shape[1] == log_probs_N_K_C.shape[1]\n",
    "\n",
    "        N, K, C = log_probs_N_K_C.shape\n",
    "        joint_probs_K_M_1 = self.joint_probs_M_K.t()[:, :, None]\n",
    "\n",
    "        probs_N_K_C = log_probs_N_K_C.exp()\n",
    "\n",
    "        # Using lots of memory.\n",
    "        for i in range(N):\n",
    "            probs_i__K_1_C = probs_N_K_C[i][:, None, :].to(\n",
    "                joint_probs_K_M_1, non_blocking=True\n",
    "            )\n",
    "            joint_probs_K_M_C = joint_probs_K_M_1 * probs_i__K_1_C\n",
    "            joint_probs_K_M_1 = joint_probs_K_M_C.reshape((K, -1, 1))\n",
    "\n",
    "        self.joint_probs_M_K = joint_probs_K_M_1.squeeze(2).t()\n",
    "        return self\n",
    "\n",
    "    def compute_batch(self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None):\n",
    "        return self.lazy_compute_batch(\n",
    "            log_probs_B_K_C=log_probs_B_K_C,\n",
    "            previous_entropies_B=None,\n",
    "            top_k=None,\n",
    "            output_entropies_B=output_entropies_B,\n",
    "        )\n",
    "\n",
    "    def lazy_compute_batch(\n",
    "        self,\n",
    "        log_probs_B_K_C: torch.Tensor,\n",
    "        previous_entropies_B: Optional[torch.Tensor],\n",
    "        top_k: Optional[int],\n",
    "        output_entropies_B=None,\n",
    "    ):\n",
    "        assert self.joint_probs_M_K.shape[1] == log_probs_B_K_C.shape[1]\n",
    "\n",
    "        B, K, C = log_probs_B_K_C.shape\n",
    "        M = self.joint_probs_M_K.shape[0]\n",
    "\n",
    "        if output_entropies_B is None:\n",
    "            output_entropies_B = torch.empty(\n",
    "                B, dtype=log_probs_B_K_C.dtype, device=log_probs_B_K_C.device\n",
    "            )\n",
    "\n",
    "        pbar = create_progress_bar(\n",
    "            B, tqdm_args=dict(desc=\"ExactJointEntropy.compute_batch\", leave=False)\n",
    "        )\n",
    "        pbar.start()\n",
    "\n",
    "        copy_old_scores = None\n",
    "\n",
    "        @toma.execute.chunked(log_probs_B_K_C, initial_step=1024, dimension=0)\n",
    "        def chunked_joint_entropy(\n",
    "            chunked_log_probs_b_K_C: torch.Tensor, start: int, end: int\n",
    "        ):\n",
    "            nonlocal copy_old_scores\n",
    "\n",
    "            if start == 0:\n",
    "                pbar.reset()\n",
    "                copy_old_scores = False\n",
    "\n",
    "            if not copy_old_scores:\n",
    "                chunked_probs_b_K_C = chunked_log_probs_b_K_C.exp()\n",
    "                b = chunked_probs_b_K_C.shape[0]\n",
    "\n",
    "                probs_b_M_C = torch.empty(\n",
    "                    (b, M, C),\n",
    "                    dtype=self.joint_probs_M_K.dtype,\n",
    "                    device=self.joint_probs_M_K.device,\n",
    "                )\n",
    "                for i in range(b):\n",
    "                    torch.matmul(\n",
    "                        self.joint_probs_M_K,\n",
    "                        chunked_probs_b_K_C[i].to(\n",
    "                            self.joint_probs_M_K, non_blocking=True\n",
    "                        ),\n",
    "                        out=probs_b_M_C[i],\n",
    "                    )\n",
    "                probs_b_M_C /= K\n",
    "\n",
    "                nats_b_M_C = -torch.log(probs_b_M_C) * probs_b_M_C\n",
    "                nats_b_M_C[torch.isnan(nats_b_M_C)] = 0.0\n",
    "\n",
    "                output_entropies_B[start:end].copy_(\n",
    "                    torch.sum(nats_b_M_C, dim=(1, 2)), non_blocking=True\n",
    "                )\n",
    "\n",
    "                # Early out (if there is anything left to do)?\n",
    "                if end < B and previous_entropies_B is not None:\n",
    "                    if (\n",
    "                        torch.sum(output_entropies_B[:end] > previous_entropies_B[end])\n",
    "                        >= top_k\n",
    "                    ):\n",
    "                        copy_old_scores = True\n",
    "            else:\n",
    "                # Just copy old scores\n",
    "                output_entropies_B[start:end].copy_(\n",
    "                    previous_entropies_B[start:end], non_blocking=True\n",
    "                )\n",
    "\n",
    "            pbar.update(end - start)\n",
    "\n",
    "        pbar.finish()\n",
    "\n",
    "        return output_entropies_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.empty\" class=\"doc_header\"><code>ExactJointEntropy.empty</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.empty</code>(**`K`**:`int`, **`device`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.add_variables\" class=\"doc_header\"><code>ExactJointEntropy.add_variables</code><a href=\"__main__.py#L23\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.compute\" class=\"doc_header\"><code>ExactJointEntropy.compute</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ExactJointEntropy.compute_batch\" class=\"doc_header\"><code>ExactJointEntropy.compute_batch</code><a href=\"__main__.py#L40\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ExactJointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ExactJointEntropy.empty)\n",
    "show_doc(ExactJointEntropy.add_variables)\n",
    "show_doc(ExactJointEntropy.compute)\n",
    "show_doc(ExactJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479, dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log()).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6479)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.float)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log()).compute()\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExactJointEntropy.compute_batch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.float)\n",
    "entropies = joint_entropy.add_variables(ys_ws[:4].log()).compute_batch(ys_ws.log())\n",
    "assert np.allclose(entropies, [5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362])\n",
    "entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5452, dtype=torch.float64) 5.545177444479562\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "joint_entropy = ExactJointEntropy.empty(K, dtype=torch.double)\n",
    "entropies = joint_entropy.add_variables(yus_ws.log()).compute()\n",
    "print(entropies, np.log(4) * 4)\n",
    "assert np.isclose(entropies, 5.5452, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Joint Entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute approximate joint entropies, we have to sample possible configurations of the $y_i$ from $p(y_1, \\dots, y_n|w)$ stratified by $p(w)$ and evaluate_old $p(y_1, \\dots, y_n)$ by averaging over $p(y_1, \\dots, y_n|w)$.\n",
    "\n",
    "The number of samples is $M$, so we use $\\frac{M}{K}$ samples per $w$.\n",
    "\n",
    "For this, we provide a class `SampledJointEntropy` that takes $K$ and $M$, and implements the 'JointEntropy' interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample, we need a few helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "def batch_multi_choices(probs_b_C, M: int):\n",
    "    \"\"\"\n",
    "    probs_b_C: Ni... x C\n",
    "\n",
    "    Returns:\n",
    "        choices: Ni... x M\n",
    "    \"\"\"\n",
    "    probs_B_C = probs_b_C.reshape((-1, probs_b_C.shape[-1]))\n",
    "\n",
    "    # samples: Ni... x draw_per_xx\n",
    "    choices = torch.multinomial(probs_B_C, num_samples=M, replacement=True)\n",
    "\n",
    "    choices_b_M = choices.reshape(list(probs_b_C.shape[:-1]) + [M])\n",
    "    return choices_b_M\n",
    "\n",
    "\n",
    "def gather_expand(data, dim, index):\n",
    "    if gather_expand.DEBUG_CHECKS:\n",
    "        assert len(data.shape) == len(index.shape)\n",
    "        assert all(dr == ir or 1 in (dr, ir) for dr, ir in zip(data.shape, index.shape))\n",
    "\n",
    "    max_shape = [max(dr, ir) for dr, ir in zip(data.shape, index.shape)]\n",
    "    new_data_shape = list(max_shape)\n",
    "    new_data_shape[dim] = data.shape[dim]\n",
    "\n",
    "    new_index_shape = list(max_shape)\n",
    "    new_index_shape[dim] = index.shape[dim]\n",
    "\n",
    "    data = data.expand(new_data_shape)\n",
    "    index = index.expand(new_index_shape)\n",
    "\n",
    "    result = torch.gather(data, dim, index)\n",
    "    return result\n",
    "\n",
    "\n",
    "gather_expand.DEBUG_CHECKS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the Paper\n",
    "![Version in the paper](batchbald_importance_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class SampledJointEntropy(JointEntropy):\n",
    "    \"\"\"Random variables (all with the same # of categories $C$) can be added via `SampledJointEntropy.add_variables`.\n",
    "\n",
    "    `SampledJointEntropy.compute` computes the joint entropy.\n",
    "\n",
    "    `SampledJointEntropy.compute_batch` computes the joint entropy of the added variables with each of the variables in the provided batch probabilities in turn.\"\"\"\n",
    "\n",
    "    sampled_joint_probs_M_K: torch.Tensor\n",
    "\n",
    "    def __init__(self, sampled_joint_probs_M_K: torch.Tensor):\n",
    "        self.sampled_joint_probs_M_K = sampled_joint_probs_M_K\n",
    "\n",
    "    @staticmethod\n",
    "    def empty(K: int, device=None, dtype=None) -> \"SampledJointEntropy\":\n",
    "        return SampledJointEntropy(torch.ones((1, K), device=device, dtype=dtype))\n",
    "\n",
    "    @staticmethod\n",
    "    def sample(probs_N_K_C: torch.Tensor, M: int) -> \"SampledJointEntropy\":\n",
    "        K = probs_N_K_C.shape[1]\n",
    "\n",
    "        # S: num of samples per w\n",
    "        S = M // K\n",
    "\n",
    "        choices_N_K_S = batch_multi_choices(probs_N_K_C, S).long()\n",
    "\n",
    "        expanded_choices_N_1_K_S = choices_N_K_S[:, None, :, :]\n",
    "        expanded_probs_N_K_1_C = probs_N_K_C[:, :, None, :]\n",
    "\n",
    "        probs_N_K_K_S = gather_expand(\n",
    "            expanded_probs_N_K_1_C, dim=-1, index=expanded_choices_N_1_K_S\n",
    "        )\n",
    "        assert expanded_probs_N_K_1_C.data_ptr() != probs_N_K_K_S.data_ptr()\n",
    "\n",
    "        # exp sum log seems necessary to avoid 0s?\n",
    "        probs_K_K_S = torch.sum(probs_N_K_K_S.log_(), dim=0, keepdim=False).exp_()\n",
    "        del probs_N_K_K_S\n",
    "        samples_K_M = probs_K_K_S.reshape((K, -1))\n",
    "\n",
    "        samples_M_K = samples_K_M.t()\n",
    "        return SampledJointEntropy(samples_M_K)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        sampled_joint_probs_M = torch.mean(\n",
    "            self.sampled_joint_probs_M_K, dim=1, keepdim=False\n",
    "        )\n",
    "        nats_M = -sampled_joint_probs_M.log_()\n",
    "        nats_M[torch.isnan(nats_M)] = 0.0\n",
    "        entropy = torch.mean(nats_M)\n",
    "        return entropy\n",
    "\n",
    "    def add_variables(\n",
    "        self, log_probs_N_K_C: torch.Tensor, M2: int\n",
    "    ) -> \"SampledJointEntropy\":\n",
    "        assert self.sampled_joint_probs_M_K.shape[1] == log_probs_N_K_C.shape[1]\n",
    "        K = log_probs_N_K_C.shape[1]\n",
    "\n",
    "        sample_K_M1_1 = self.sampled_joint_probs_M_K.t()[:, :, None]\n",
    "\n",
    "        new_sample_M2_K = self.sample(log_probs_N_K_C.exp(), M2).sampled_joint_probs_M_K\n",
    "        new_sample_K_1_M2 = new_sample_M2_K.t()[:, None, :]\n",
    "\n",
    "        merged_sample_K_M1_M2 = sample_K_M1_1 * new_sample_K_1_M2\n",
    "        merged_sample_K_M = merged_sample_K_M1_M2.reshape((K, -1))\n",
    "\n",
    "        self.sampled_joint_probs_M_K = merged_sample_K_M.t()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compute_batch(self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None):\n",
    "        return self.lazy_compute_batch(\n",
    "            log_probs_B_K_C=log_probs_B_K_C,\n",
    "            previous_entropies_B=None,\n",
    "            top_k=None,\n",
    "            output_entropies_B=output_entropies_B,\n",
    "        )\n",
    "\n",
    "    def lazy_compute_batch(\n",
    "        self,\n",
    "        log_probs_B_K_C: torch.Tensor,\n",
    "        previous_entropies_B: Optional[torch.Tensor],\n",
    "        top_k: Optional[int],\n",
    "        output_entropies_B=None,\n",
    "    ):\n",
    "        assert self.sampled_joint_probs_M_K.shape[1] == log_probs_B_K_C.shape[1]\n",
    "\n",
    "        B, K, C = log_probs_B_K_C.shape\n",
    "        M = self.sampled_joint_probs_M_K.shape[0]\n",
    "\n",
    "        if output_entropies_B is None:\n",
    "            output_entropies_B = torch.empty(\n",
    "                B, dtype=log_probs_B_K_C.dtype, device=log_probs_B_K_C.device\n",
    "            )\n",
    "\n",
    "        pbar = create_progress_bar(\n",
    "            B, tqdm_args=dict(desc=\"SampledJointEntropy.compute_batch\", leave=False)\n",
    "        )\n",
    "        pbar.start()\n",
    "\n",
    "        copy_old_scores = None\n",
    "\n",
    "        @toma.execute.chunked(log_probs_B_K_C, initial_step=1024, dimension=0)\n",
    "        def chunked_joint_entropy(\n",
    "            chunked_log_probs_b_K_C: torch.Tensor, start: int, end: int\n",
    "        ):\n",
    "            nonlocal copy_old_scores\n",
    "\n",
    "            if start == 0:\n",
    "                pbar.reset()\n",
    "                copy_old_scores = False\n",
    "\n",
    "            if not copy_old_scores:\n",
    "                b = chunked_log_probs_b_K_C.shape[0]\n",
    "\n",
    "                probs_b_M_C = torch.empty(\n",
    "                    (b, M, C),\n",
    "                    dtype=self.sampled_joint_probs_M_K.dtype,\n",
    "                    device=self.sampled_joint_probs_M_K.device,\n",
    "                )\n",
    "                for i in range(b):\n",
    "                    torch.matmul(\n",
    "                        self.sampled_joint_probs_M_K,\n",
    "                        chunked_log_probs_b_K_C[i]\n",
    "                        .to(self.sampled_joint_probs_M_K, non_blocking=True)\n",
    "                        .exp(),\n",
    "                        out=probs_b_M_C[i],\n",
    "                    )\n",
    "                probs_b_M_C /= K\n",
    "\n",
    "                q_1_M_1 = self.sampled_joint_probs_M_K.mean(dim=1, keepdim=True)[None]\n",
    "\n",
    "                output_entropies_B[start:end].copy_(\n",
    "                    torch.sum(\n",
    "                        -torch.log(probs_b_M_C) * probs_b_M_C / q_1_M_1, dim=(1, 2)\n",
    "                    )\n",
    "                    / M,\n",
    "                    non_blocking=True,\n",
    "                )\n",
    "\n",
    "                # Early out (if there is anything left to do)?\n",
    "                if end < B and previous_entropies_B is not None:\n",
    "                    if (\n",
    "                        torch.sum(output_entropies_B[:end] > previous_entropies_B[end])\n",
    "                        >= top_k\n",
    "                    ):\n",
    "                        copy_old_scores = True\n",
    "            else:\n",
    "                # Just copy old scores\n",
    "                output_entropies_B[start:end].copy_(\n",
    "                    previous_entropies_B[start:end], non_blocking=True\n",
    "                )\n",
    "\n",
    "            pbar.update(end - start)\n",
    "\n",
    "        pbar.finish()\n",
    "\n",
    "        return output_entropies_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.empty\" class=\"doc_header\"><code>SampledJointEntropy.empty</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.empty</code>(**`K`**:`int`, **`device`**=*`None`*, **`dtype`**=*`None`*)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.sample\" class=\"doc_header\"><code>SampledJointEntropy.sample</code><a href=\"__main__.py#L20\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.sample</code>(**`probs_N_K_C`**:`Tensor`, **`M`**:`int`)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.compute\" class=\"doc_header\"><code>SampledJointEntropy.compute</code><a href=\"__main__.py#L43\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.compute</code>()\n",
       "\n",
       "Computes the entropy of this joint entropy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.add_variables\" class=\"doc_header\"><code>SampledJointEntropy.add_variables</code><a href=\"__main__.py#L50\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`, **`M2`**:`int`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SampledJointEntropy.compute_batch\" class=\"doc_header\"><code>SampledJointEntropy.compute_batch</code><a href=\"__main__.py#L66\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SampledJointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SampledJointEntropy.empty)\n",
    "show_doc(SampledJointEntropy.sample)\n",
    "show_doc(SampledJointEntropy.compute)\n",
    "show_doc(SampledJointEntropy.add_variables)\n",
    "show_doc(SampledJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6510, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log(), 100000).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6476, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log(), 100000).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SampledJointEntropy.compute_batch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.9729, 5.6356, 5.9729, 5.6356, 5.9729, 5.6356, 5.9729, 5.6356],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = SampledJointEntropy.empty(K, dtype=torch.float)\n",
    "entropies = joint_entropy.add_variables(ys_ws[:4].log(), 1000000).compute_batch(ys_ws.log())\n",
    "\n",
    "print(entropies)\n",
    "assert np.allclose(entropies, [5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362, 5.9735, 5.6362], atol=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamically chooses JointEntropy Method\n",
    "\n",
    "Finally, we want to be able to dynamically pick either class depending on the maximum number of samples we want. (And also resample if necessary as we add variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class DynamicJointEntropy(JointEntropy):\n",
    "    inner: JointEntropy\n",
    "    log_probs_max_N_K_C: torch.Tensor\n",
    "    N: int\n",
    "    M: int\n",
    "\n",
    "    def __init__(self, M: int, max_N: int, K: int, C: int, dtype=None, device=None):\n",
    "        self.M = M\n",
    "        self.N = 0\n",
    "        self.max_N = max_N\n",
    "\n",
    "        self.inner = ExactJointEntropy.empty(K, dtype=dtype, device=device)\n",
    "        self.log_probs_max_N_K_C = torch.empty(\n",
    "            (max_N, K, C), dtype=dtype, device=device\n",
    "        )\n",
    "\n",
    "    def add_variables(self, log_probs_N_K_C: torch.Tensor) -> \"DynamicJointEntropy\":\n",
    "        C = self.log_probs_max_N_K_C.shape[2]\n",
    "        add_N = log_probs_N_K_C.shape[0]\n",
    "\n",
    "        assert self.log_probs_max_N_K_C.shape[0] >= self.N + add_N\n",
    "        assert self.log_probs_max_N_K_C.shape[2] == C\n",
    "\n",
    "        self.log_probs_max_N_K_C[self.N : self.N + add_N] = log_probs_N_K_C\n",
    "        self.N += add_N\n",
    "\n",
    "        num_exact_samples = C ** self.N\n",
    "        if num_exact_samples > self.M:\n",
    "            self.inner = SampledJointEntropy.sample(\n",
    "                self.log_probs_max_N_K_C[: self.N].exp(), self.M\n",
    "            )\n",
    "        else:\n",
    "            self.inner.add_variables(log_probs_N_K_C)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.inner.compute()\n",
    "\n",
    "    def compute_batch(\n",
    "        self, log_probs_B_K_C: torch.Tensor, output_entropies_B=None\n",
    "    ) -> torch.Tensor:\n",
    "        return self.inner.compute_batch(log_probs_B_K_C, output_entropies_B)\n",
    "\n",
    "    def lazy_compute_batch(\n",
    "        self,\n",
    "        log_probs_B_K_C: torch.Tensor,\n",
    "        previous_entropies_B: Optional[torch.Tensor],\n",
    "        top_k: Optional[int],\n",
    "        output_entropies_B=None,\n",
    "    ):\n",
    "        return self.inner.lazy_compute_batch(\n",
    "            log_probs_B_K_C=log_probs_B_K_C,\n",
    "            previous_entropies_B=previous_entropies_B,\n",
    "            top_k=top_k,\n",
    "            output_entropies_B=output_entropies_B,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DynamicJointEntropy.add_variables\" class=\"doc_header\"><code>DynamicJointEntropy.add_variables</code><a href=\"__main__.py#L18\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DynamicJointEntropy.add_variables</code>(**`log_probs_N_K_C`**:`Tensor`)\n",
       "\n",
       "Expands the joint entropy to include more terms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DynamicJointEntropy.compute_batch\" class=\"doc_header\"><code>DynamicJointEntropy.compute_batch</code><a href=\"__main__.py#L39\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DynamicJointEntropy.compute_batch</code>(**`log_probs_B_K_C`**:`Tensor`, **`output_entropies_B`**=*`None`*)\n",
       "\n",
       "Computes the joint entropy of the added variables together with the batch (one by one)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DynamicJointEntropy.add_variables)\n",
    "show_doc(DynamicJointEntropy.compute_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6479, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "joint_entropy = DynamicJointEntropy(256, 8, K, 4, dtype=torch.double)\n",
    "entropy = joint_entropy.add_variables(ys_ws[:4].log()).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 4.6479, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(joint_entropy.inner) == ExactJointEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.1174, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "entropy = joint_entropy.add_variables(ys_ws[4:].log()).compute()\n",
    "\n",
    "print(entropy)\n",
    "assert np.isclose(entropy, 9.2756, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(joint_entropy.inner) == SampledJointEntropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
