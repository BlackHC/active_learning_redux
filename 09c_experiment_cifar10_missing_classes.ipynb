{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment CIFAR-10\n",
    "> Can we get better by training on our assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment_cifar10_missing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import dataclasses\n",
    "import traceback\n",
    "\n",
    "from blackhc.project import is_run_from_ipython\n",
    "from blackhc.project.experiment import embedded_experiments\n",
    "\n",
    "from batchbald_redux import acquisition_functions, baseline_acquisition_functions\n",
    "from batchbald_redux.experiment_data import (\n",
    "    ImbalancedTestDistributionExperimentDataConfig,\n",
    ")\n",
    "from batchbald_redux.unified_experiment import UnifiedExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "# (ID: air, automobile, ship and truck, OOD: bird, cat, deer, dog, frog and horse)\n",
    "# ood_classes={2, 3, 4, 5, 6, 7}\n",
    "\n",
    "configs = [\n",
    "    UnifiedExperiment(\n",
    "        experiment_data_config=ImbalancedTestDistributionExperimentDataConfig(\n",
    "            dataset_name=\"CIFAR-10\",\n",
    "            repetitions=1,\n",
    "            initial_training_set_size=100,\n",
    "            validation_set_size=4000,\n",
    "            validation_split_random_state=0,\n",
    "            evaluation_set_size=4000,\n",
    "            add_dataset_noise=False,\n",
    "            minority_classes={2, 3, 4, 5, 6, 7},\n",
    "            minority_class_percentage=0.0,\n",
    "        ),\n",
    "        seed=seed + 4658,\n",
    "        acquisition_function=acquisition_function,\n",
    "        acquisition_size=acquisition_size,\n",
    "        num_pool_samples=num_pool_samples,\n",
    "        max_training_set=11000,\n",
    "    )\n",
    "    for seed in range(3)\n",
    "    for acquisition_function in [\n",
    "        acquisition_functions.BALD,\n",
    "        acquisition_functions.EPIG,\n",
    "        acquisition_functions.EvalBALD,\n",
    "        baseline_acquisition_functions.BADGE,\n",
    "        acquisition_functions.Random,\n",
    "    ]\n",
    "    for acquisition_size in [800]\n",
    "    for num_pool_samples in [50]\n",
    "]\n",
    "\n",
    "if not is_run_from_ipython() and __name__ == \"__main__\":\n",
    "    for job_id, store in embedded_experiments(__file__, len(configs)):\n",
    "        config = configs[job_id]\n",
    "        config.seed += job_id\n",
    "        print(config)\n",
    "        store[\"config\"] = dataclasses.asdict(config)\n",
    "        store[\"log\"] = {}\n",
    "\n",
    "        try:\n",
    "            config.run(store=store)\n",
    "        except Exception:\n",
    "            store[\"exception\"] = traceback.format_exc()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4658,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4658,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.EPIG\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4658,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4658,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4658,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4659,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4659,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.EPIG\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4659,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4659,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4659,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4660,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4660,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.EPIG\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4660,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.EvalBALD\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4660,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.baseline_acquisition_functions.BADGE\n",
      "    ),\n",
      "    batchbald_redux.unified_experiment.UnifiedExperiment(\n",
      "        seed=4660,\n",
      "        experiment_data_config=batchbald_redux.experiment_data.ImbalancedTestDistributionExperimentDataConfig(\n",
      "            dataset_name='CIFAR-10',\n",
      "            repetitions=1,\n",
      "            initial_training_set_size=100,\n",
      "            validation_set_size=4000,\n",
      "            validation_split_random_state=0,\n",
      "            evaluation_set_size=4000,\n",
      "            add_dataset_noise=False,\n",
      "            minority_classes={2, 3, 4, 5, 6, 7},\n",
      "            minority_class_percentage=0.0\n",
      "        ),\n",
      "        acquisition_size=800,\n",
      "        max_training_set=11000,\n",
      "        num_pool_samples=50,\n",
      "        # class\n",
      "        acquisition_function=batchbald_redux.acquisition_functions.Random\n",
      "    )\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import prettyprinter\n",
    "\n",
    "prettyprinter.install_extras({\"dataclasses\"})\n",
    "prettyprinter.pprint(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/home/blackhc/anaconda3/envs/active_learning/lib/python3.8/site-packages/sklearn/utils/__init__.py:1102: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Evaluation Set Class Counts: [1000, 1000, 0, 0, 0, 0, 0, 0, 1000, 1000]\n",
      "Creating: EvalBALD(\n",
      "\tacquisition_size=800,\n",
      "\tnum_pool_samples=5\n",
      ")\n",
      "Creating: Cifar10ModelTrainer(\n",
      "\tdevice=cuda,\n",
      "\tnum_training_samples=1,\n",
      "\tnum_validation_samples=20,\n",
      "\tmax_training_epochs=1\n",
      ")\n",
      "Creating: TrainSelfDistillationEvalModel(\n",
      "\tnum_pool_samples=5\n",
      ")\n",
      "Training set size 400:\n",
      "Cosine Annealing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4336d84c28bd48c18364e3f5651a492e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/39]   3%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 metrics: {'accuracy': 0.311875, 'crossentropy': 1.9677299022674561}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.27975, 'crossentropy': tensor(1.8762)}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/22000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Annealing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f2d86a93c846fc92cd1a5aa8bec208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/39]   3%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 metrics: {'accuracy': 0.27625, 'crossentropy': 1.8955667781829835}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/208000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_predictions_labels:   0%|          | 0/208000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/41600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/41600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Conditional Entropy:   0%|          | 0/41600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Entropy:   0%|          | 0/41600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CandidateBatch(scores=[0.2676178812980652, 0.23119616508483887, 0.23033781349658966, 0.22923961281776428, 0.2230968475341797, 0.2180115133523941, 0.2167917937040329, 0.21534428000450134, 0.21259991079568863, 0.21064308285713196, 0.20960113406181335, 0.20879486948251724, 0.20701898634433746, 0.205959752202034, 0.2042892873287201, 0.20302912592887878, 0.20228202641010284, 0.19985906779766083, 0.19904935359954834, 0.19785308837890625, 0.19713661074638367, 0.1945118010044098, 0.19304795563220978, 0.1916656792163849, 0.18700842559337616, 0.18552203476428986, 0.18199604749679565, 0.17927297949790955, 0.1781848520040512, 0.1778956800699234, 0.17572203278541565, 0.1750098019838333, 0.17402583360671997, 0.17384342849254608, 0.1737957000732422, 0.17330487072467804, 0.17237897217273712, 0.17020533978939056, 0.16935309767723083, 0.16803430020809174, 0.16779014468193054, 0.16771574318408966, 0.16648700833320618, 0.16619199514389038, 0.1647893786430359, 0.16403162479400635, 0.16396352648735046, 0.16299176216125488, 0.16253885626792908, 0.16142183542251587, 0.15968728065490723, 0.1596248745918274, 0.15866558253765106, 0.15808521211147308, 0.15499450266361237, 0.15446168184280396, 0.15419511497020721, 0.1527969092130661, 0.1525518298149109, 0.1525508165359497, 0.15142905712127686, 0.15075448155403137, 0.14741788804531097, 0.14619997143745422, 0.14546795189380646, 0.14530324935913086, 0.14516416192054749, 0.1444806158542633, 0.14390966296195984, 0.14345669746398926, 0.14269345998764038, 0.142677903175354, 0.14264482259750366, 0.14245983958244324, 0.14078128337860107, 0.14077162742614746, 0.1401931345462799, 0.13979938626289368, 0.13970938324928284, 0.13852262496948242, 0.13828778266906738, 0.1382812261581421, 0.13777747750282288, 0.13774734735488892, 0.13692507147789001, 0.1365632563829422, 0.13639405369758606, 0.13638582825660706, 0.13636255264282227, 0.1354394555091858, 0.13499119877815247, 0.13419249653816223, 0.13362859189510345, 0.13276386260986328, 0.13194218277931213, 0.13188624382019043, 0.13154903054237366, 0.13104099035263062, 0.13022246956825256, 0.12978217005729675, 0.12943440675735474, 0.1280892789363861, 0.12766247987747192, 0.12691199779510498, 0.1268393099308014, 0.12633129954338074, 0.12623149156570435, 0.125825434923172, 0.1256020963191986, 0.12556296586990356, 0.12547802925109863, 0.12521466612815857, 0.12469649314880371, 0.12411358952522278, 0.12338486313819885, 0.12253537774085999, 0.12212544679641724, 0.12187451124191284, 0.12141567468643188, 0.1213422417640686, 0.12073731422424316, 0.12070517241954803, 0.12063097953796387, 0.12059950828552246, 0.12056252360343933, 0.12042409181594849, 0.11990690231323242, 0.11895594000816345, 0.11883750557899475, 0.11799469590187073, 0.11784350872039795, 0.11688840389251709, 0.11624719202518463, 0.11614888906478882, 0.11602908372879028, 0.11580424010753632, 0.11532434821128845, 0.11528807878494263, 0.11516696214675903, 0.11495080590248108, 0.11491793394088745, 0.11464005708694458, 0.11402568221092224, 0.1138123869895935, 0.11362722516059875, 0.11335396766662598, 0.11329787969589233, 0.11318841576576233, 0.1128244698047638, 0.11278605461120605, 0.1124822199344635, 0.1123085618019104, 0.11213865876197815, 0.11202022433280945, 0.11201021075248718, 0.11175116896629333, 0.1111818253993988, 0.11101499199867249, 0.11075571179389954, 0.1107182502746582, 0.11053693294525146, 0.11050105094909668, 0.1098586916923523, 0.10949325561523438, 0.10930618643760681, 0.10913661122322083, 0.10905566811561584, 0.10875797271728516, 0.10865390300750732, 0.10848096013069153, 0.10828003287315369, 0.10751202702522278, 0.10641855001449585, 0.10616499185562134, 0.10616356134414673, 0.1058473289012909, 0.10581767559051514, 0.10580462217330933, 0.10578116774559021, 0.10539206862449646, 0.10531550645828247, 0.10526913404464722, 0.10511976480484009, 0.10494878888130188, 0.10477063059806824, 0.10444086790084839, 0.10422837734222412, 0.10418611764907837, 0.10360497236251831, 0.10334724187850952, 0.10298073291778564, 0.10296997427940369, 0.1029517650604248, 0.102826327085495, 0.10264045000076294, 0.1024726927280426, 0.10230612754821777, 0.102211594581604, 0.1021849513053894, 0.1018190085887909, 0.10178729891777039, 0.10158702731132507, 0.10155802965164185, 0.10142871737480164, 0.10132059454917908, 0.10105568170547485, 0.10090722143650055, 0.10089665651321411, 0.10081180930137634, 0.10076937079429626, 0.10047844052314758, 0.10022151470184326, 0.09998565912246704, 0.0999847948551178, 0.09980767965316772, 0.09972330927848816, 0.09966281056404114, 0.09965524822473526, 0.09945440292358398, 0.09940019249916077, 0.0992608368396759, 0.09921109676361084, 0.09900644421577454, 0.09891733527183533, 0.09854578971862793, 0.09841680526733398, 0.09828732907772064, 0.09820574522018433, 0.09819769859313965, 0.0981971025466919, 0.09810400009155273, 0.09801328182220459, 0.0976766049861908, 0.0976744294166565, 0.09751445055007935, 0.0973581075668335, 0.09733766317367554, 0.09732994437217712, 0.09723863005638123, 0.09709656238555908, 0.09678617119789124, 0.09658563137054443, 0.09583035111427307, 0.09561046957969666, 0.095542311668396, 0.09507250785827637, 0.09498381614685059, 0.09494557976722717, 0.09491696953773499, 0.09484410285949707, 0.09477421641349792, 0.09464332461357117, 0.09455186128616333, 0.09450623393058777, 0.09448495507240295, 0.09433850646018982, 0.09406134486198425, 0.0938701331615448, 0.09380799531936646, 0.0938023030757904, 0.09354174137115479, 0.09326663613319397, 0.09298175573348999, 0.09288769960403442, 0.09288668632507324, 0.09281399846076965, 0.0927325189113617, 0.09242799878120422, 0.09231173992156982, 0.09219995141029358, 0.09216085076332092, 0.09179067611694336, 0.09165263175964355, 0.09164106845855713, 0.09135225415229797, 0.09127455949783325, 0.09115684032440186, 0.09111529588699341, 0.09068074822425842, 0.09047573804855347, 0.09031426906585693, 0.09030497074127197, 0.08985987305641174, 0.08975961804389954, 0.08970518410205841, 0.08942949771881104, 0.08937814831733704, 0.08937785029411316, 0.08935385942459106, 0.08923333883285522, 0.08912935853004456, 0.08896517753601074, 0.08890986442565918, 0.08889424800872803, 0.08835816383361816, 0.08818787336349487, 0.08791971206665039, 0.08739674091339111, 0.08718803524971008, 0.0870894342660904, 0.08697900176048279, 0.08692413568496704, 0.08686640858650208, 0.08661317825317383, 0.086326003074646, 0.08622816205024719, 0.08616283535957336, 0.0861472487449646, 0.08609172701835632, 0.08606791496276855, 0.085683673620224, 0.08552476763725281, 0.08530858159065247, 0.08527877926826477, 0.08517703413963318, 0.08515724539756775, 0.0850471556186676, 0.08502852916717529, 0.08495432138442993, 0.08483844995498657, 0.08471456170082092, 0.08471450209617615, 0.08471265435218811, 0.08458641171455383, 0.0840824544429779, 0.0840296745300293, 0.0838613510131836, 0.08385014533996582, 0.083624929189682, 0.0831640362739563, 0.08313769102096558, 0.08310845494270325, 0.08290192484855652, 0.08282622694969177, 0.08280733227729797, 0.08277881145477295, 0.08268162608146667, 0.0826764702796936, 0.0826021134853363, 0.08247062563896179, 0.08239805698394775, 0.08239281177520752, 0.08226066827774048, 0.08220541477203369, 0.08209630846977234, 0.08179444074630737, 0.08157795667648315, 0.08144035935401917, 0.08118075132369995, 0.08112189173698425, 0.08102422952651978, 0.08089756965637207, 0.0807487964630127, 0.08060774207115173, 0.08044132590293884, 0.08028611540794373, 0.08020186424255371, 0.08018723130226135, 0.08007475733757019, 0.07986539602279663, 0.07983875274658203, 0.07982125878334045, 0.07972386479377747, 0.07972018420696259, 0.07960563898086548, 0.07955259084701538, 0.07943159341812134, 0.07914739847183228, 0.07887798547744751, 0.07887497544288635, 0.07880619168281555, 0.07878419756889343, 0.07875937223434448, 0.07864433526992798, 0.07861092686653137, 0.07849845290184021, 0.07835268974304199, 0.07818597555160522, 0.07792162895202637, 0.0776817798614502, 0.07765144109725952, 0.07764941453933716, 0.07743984460830688, 0.07743203639984131, 0.07732594013214111, 0.07731792330741882, 0.07724261283874512, 0.0771537721157074, 0.07706373929977417, 0.07699081301689148, 0.0765826404094696, 0.07653495669364929, 0.07648348808288574, 0.07620874047279358, 0.07618507742881775, 0.07607436180114746, 0.07605928182601929, 0.0760088562965393, 0.07551005482673645, 0.07538256049156189, 0.07535681128501892, 0.07527327537536621, 0.07522910833358765, 0.07509714365005493, 0.07506394386291504, 0.0749768614768982, 0.07491990923881531, 0.07482075691223145, 0.07475772500038147, 0.07463979721069336, 0.07463580369949341, 0.0746213048696518, 0.07455617189407349, 0.07448336482048035, 0.07432055473327637, 0.07429367303848267, 0.07428774237632751, 0.07423964142799377, 0.07423397898674011, 0.07421594858169556, 0.0741814374923706, 0.07404673099517822, 0.07399749755859375, 0.07397928833961487, 0.07391270995140076, 0.07390066981315613, 0.07390010356903076, 0.07383829355239868, 0.07379287481307983, 0.0735132098197937, 0.07346606254577637, 0.07345619797706604, 0.07338452339172363, 0.07330507040023804, 0.0732976496219635, 0.07309991121292114, 0.07309067249298096, 0.07307040691375732, 0.0730014443397522, 0.07298335433006287, 0.07274484634399414, 0.07274413108825684, 0.07265534996986389, 0.07261911034584045, 0.07260632514953613, 0.07240039110183716, 0.07230275869369507, 0.07217726111412048, 0.07206103205680847, 0.07204502820968628, 0.07195782661437988, 0.07187512516975403, 0.071868896484375, 0.07183146476745605, 0.07168534398078918, 0.0716092586517334, 0.07147777080535889, 0.07146459817886353, 0.07140982151031494, 0.07136094570159912, 0.07127645611763, 0.0711519718170166, 0.07107499241828918, 0.07099682092666626, 0.07093587517738342, 0.07084733247756958, 0.07078075408935547, 0.07077732682228088, 0.07072776556015015, 0.07052969932556152, 0.07050031423568726, 0.07046526670455933, 0.07018142938613892, 0.07007914781570435, 0.07006514072418213, 0.0700228214263916, 0.0698758065700531, 0.06987059116363525, 0.06981846690177917, 0.06967690587043762, 0.06958144903182983, 0.06949058175086975, 0.06946736574172974, 0.0693771243095398, 0.0693739652633667, 0.06934678554534912, 0.0693008303642273, 0.06929385662078857, 0.06911259889602661, 0.0690462589263916, 0.0688914954662323, 0.06865689158439636, 0.06859511137008667, 0.06858018040657043, 0.06852734088897705, 0.06831759214401245, 0.0683087706565857, 0.06821936368942261, 0.06821584701538086, 0.06811553239822388, 0.06798732280731201, 0.06780388951301575, 0.06778812408447266, 0.06758150458335876, 0.06741881370544434, 0.06741556525230408, 0.06732165813446045, 0.06696921586990356, 0.06683903932571411, 0.06682223081588745, 0.06679868698120117, 0.06675669550895691, 0.06652653217315674, 0.066470205783844, 0.06631475687026978, 0.06630915403366089, 0.06611707806587219, 0.06603384017944336, 0.06584835052490234, 0.06578254699707031, 0.06576675176620483, 0.06575673818588257, 0.06565135717391968, 0.06562268733978271, 0.06556516885757446, 0.0655127763748169, 0.06549268960952759, 0.06540441513061523, 0.06538605690002441, 0.06536376476287842, 0.06533730030059814, 0.06529122591018677, 0.0652381181716919, 0.0651717483997345, 0.06515145301818848, 0.06512704491615295, 0.0650847852230072, 0.0649753212928772, 0.06496962904930115, 0.06493332982063293, 0.06490141153335571, 0.06460464000701904, 0.06460380554199219, 0.06448090076446533, 0.06446811556816101, 0.0643959641456604, 0.06428647041320801, 0.0642322301864624, 0.06411093473434448, 0.06375715136528015, 0.0637388825416565, 0.06373852491378784, 0.06373679637908936, 0.06364330649375916, 0.0636354386806488, 0.0635388195514679, 0.0635032057762146, 0.06346270442008972, 0.0634201169013977, 0.06326162815093994, 0.06306499242782593, 0.0630388855934143, 0.06299808621406555, 0.06294238567352295, 0.06275784969329834, 0.06262719631195068, 0.06244993209838867, 0.06219959259033203, 0.0621945858001709, 0.06201791763305664, 0.061730265617370605, 0.061724305152893066, 0.06160545349121094, 0.06155216693878174, 0.061438024044036865, 0.06142532825469971, 0.06138831377029419, 0.061359941959381104, 0.06135964393615723, 0.06133571267127991, 0.061274945735931396, 0.061041414737701416, 0.061003535985946655, 0.06100285053253174, 0.060939013957977295, 0.06091427803039551, 0.06076440215110779, 0.0607529878616333, 0.06075260043144226, 0.060319334268569946, 0.060227036476135254, 0.06018894910812378, 0.06017693877220154, 0.06011909246444702, 0.06004801392555237, 0.0599483847618103, 0.05990713834762573, 0.05990159511566162, 0.05980950593948364, 0.05980762839317322, 0.05978330969810486, 0.05975717306137085, 0.0596289336681366, 0.05956244468688965, 0.059417724609375, 0.059396207332611084, 0.05933922529220581, 0.05913981795310974, 0.05908513069152832, 0.059077948331832886, 0.05894315242767334, 0.05891972780227661, 0.058873116970062256, 0.0588604211807251, 0.058839499950408936, 0.05876019597053528, 0.058725178241729736, 0.05870109796524048, 0.05842769145965576, 0.058338701725006104, 0.05831405520439148, 0.05817282199859619, 0.05816155672073364, 0.05813169479370117, 0.05804699659347534, 0.05779993534088135, 0.05777084827423096, 0.05775150656700134, 0.05770927667617798, 0.05768704414367676, 0.057678282260894775, 0.05757558345794678, 0.05754733085632324, 0.05748093128204346, 0.05741316080093384, 0.05739462375640869, 0.05703020095825195, 0.056911349296569824, 0.056812018156051636, 0.0567626953125, 0.056659698486328125, 0.05652552843093872, 0.05646991729736328, 0.056454092264175415, 0.0564538836479187, 0.05639803409576416, 0.056385576725006104, 0.05636173486709595, 0.05622148513793945, 0.056198328733444214, 0.05619776248931885, 0.05619525909423828, 0.056177377700805664, 0.05609765648841858, 0.05608999729156494, 0.05601143836975098, 0.055935055017471313, 0.05590119957923889, 0.05581849813461304, 0.05579918622970581, 0.055694520473480225, 0.05564337968826294, 0.05549347400665283, 0.05548316240310669, 0.05537915229797363, 0.05532020330429077, 0.05525773763656616, 0.05519843101501465, 0.0551905632019043, 0.05514061450958252, 0.05506342649459839, 0.055054306983947754, 0.0548783540725708, 0.05469703674316406, 0.05465191602706909, 0.054454803466796875, 0.05438709259033203, 0.054339587688446045, 0.05430179834365845, 0.05429565906524658, 0.05427175760269165, 0.05414104461669922, 0.0541379451751709, 0.05406367778778076, 0.054055094718933105, 0.0539664626121521, 0.05393499135971069, 0.05386322736740112, 0.05385187268257141, 0.05383068323135376, 0.05379372835159302, 0.053745001554489136, 0.05371594429016113, 0.053678810596466064, 0.053626298904418945, 0.053535521030426025, 0.05345892906188965, 0.05342763662338257, 0.05332183837890625, 0.053285956382751465, 0.053116798400878906, 0.05309075117111206, 0.053062498569488525, 0.05305337905883789, 0.05303555727005005, 0.05301636457443237, 0.052958548069000244, 0.052905142307281494, 0.0529019832611084, 0.052824467420578, 0.05277925729751587, 0.05277344584465027, 0.0527118444442749, 0.05265706777572632, 0.05264467000961304, 0.052515387535095215, 0.05238887667655945, 0.05234062671661377, 0.05229216814041138, 0.052247196435928345, 0.05222254991531372, 0.052120208740234375, 0.05211794376373291, 0.05211007595062256, 0.052043795585632324, 0.05199313163757324, 0.051987290382385254, 0.05195337533950806, 0.05191412568092346, 0.05189955234527588, 0.05184221267700195, 0.05182492733001709, 0.05176973342895508, 0.051728904247283936, 0.051622867584228516, 0.05153051018714905, 0.05151480436325073, 0.05151337385177612, 0.051454633474349976, 0.05138951539993286, 0.05137914419174194, 0.05129784345626831, 0.05125826597213745, 0.05120283365249634, 0.051142096519470215, 0.0511019229888916, 0.05097976326942444, 0.05097544193267822, 0.05097442865371704, 0.050893962383270264, 0.050844043493270874, 0.050752460956573486, 0.050690531730651855, 0.05067664384841919, 0.050662994384765625, 0.05059206485748291, 0.050516724586486816, 0.050483763217926025, 0.05045410990715027, 0.050412774085998535, 0.05039787292480469, 0.05030137300491333, 0.05029618740081787, 0.05028331279754639, 0.050273895263671875, 0.05023205280303955, 0.04998809099197388, 0.049930572509765625, 0.04978817701339722, 0.04967385530471802, 0.04958862066268921, 0.04952695965766907, 0.04951035976409912, 0.04944002628326416, 0.04939675331115723, 0.049386411905288696, 0.04936385154724121, 0.0493432879447937, 0.049268126487731934, 0.049240827560424805, 0.04923826456069946, 0.04921555519104004, 0.0491909384727478, 0.049112558364868164, 0.04905831813812256, 0.049050986766815186, 0.049047112464904785, 0.04903364181518555, 0.048990607261657715, 0.04898715019226074, 0.048980772495269775, 0.048959046602249146, 0.04893636703491211, 0.04876351356506348, 0.04875257611274719, 0.04870957136154175, 0.04869401454925537, 0.04867821931838989, 0.04858386516571045, 0.04857057332992554], indices=[2019, 9839, 26859, 25416, 21722, 3940, 15226, 9084, 39763, 38220, 18285, 36364, 15730, 18286, 5665, 14135, 8367, 1534, 10689, 23553, 33383, 16321, 1588, 39781, 40006, 31518, 25317, 26246, 38453, 22865, 36501, 4427, 29015, 29213, 16521, 8030, 28757, 33343, 17521, 36591, 37971, 10350, 39843, 17095, 22948, 8260, 38131, 9481, 36546, 38175, 11151, 39029, 34276, 29547, 2404, 9709, 26578, 25683, 23570, 8208, 30000, 26631, 13866, 16792, 6478, 18263, 19923, 4570, 3473, 7142, 39489, 30793, 22870, 785, 28667, 8060, 38102, 21266, 38198, 38475, 27143, 24070, 20588, 8346, 19790, 19606, 17011, 22059, 37238, 20668, 18408, 34380, 36863, 7265, 1948, 22490, 33109, 29404, 41416, 37085, 19475, 18024, 24355, 14394, 40886, 35213, 16702, 10840, 30904, 28673, 41347, 14533, 39505, 13047, 19984, 2746, 36372, 10518, 10985, 28215, 27236, 18741, 32887, 5187, 12305, 38704, 7916, 39141, 35179, 1367, 14195, 23420, 34757, 19639, 21076, 6790, 25821, 33414, 41390, 27638, 33613, 40251, 31750, 26245, 16925, 25051, 4065, 14984, 38087, 28033, 34972, 5145, 24307, 9647, 14484, 33379, 968, 26810, 1098, 35249, 12182, 18111, 6442, 13187, 7302, 17484, 7534, 7540, 37586, 5512, 13197, 34639, 1628, 9432, 23658, 31985, 35160, 32338, 38040, 24729, 25239, 5636, 29990, 15009, 18323, 31514, 15626, 3534, 24844, 5398, 18938, 40254, 39084, 15644, 13014, 5405, 40424, 17189, 11054, 39905, 32023, 10800, 944, 11905, 12145, 22824, 37328, 7758, 36884, 5514, 6265, 30639, 10909, 387, 3485, 40959, 3599, 3453, 11573, 221, 27115, 41073, 12988, 27011, 33995, 23359, 28617, 23866, 8887, 3155, 36351, 37720, 15311, 29846, 9488, 16781, 8045, 39323, 16350, 35945, 16475, 30125, 15911, 13332, 40014, 21466, 14574, 28703, 7928, 33240, 31707, 6838, 19321, 11193, 20917, 16408, 3174, 16603, 24474, 18052, 35109, 9326, 23383, 28593, 22070, 39470, 25799, 23178, 40618, 38374, 18291, 23740, 25038, 12300, 37308, 28338, 38943, 563, 15953, 23983, 37678, 38121, 11555, 35856, 19438, 15619, 41100, 2205, 23709, 2470, 7887, 37269, 21988, 33152, 37363, 6251, 37110, 1075, 3986, 32825, 35647, 5529, 7403, 8182, 26771, 34885, 31948, 27416, 36407, 10944, 38390, 25549, 7638, 1872, 3140, 24868, 7051, 6081, 24164, 39473, 12707, 12446, 34771, 21433, 26817, 710, 5519, 27811, 9368, 40552, 774, 10541, 16910, 11353, 11989, 9345, 7007, 27380, 25068, 37376, 26576, 39565, 10795, 30958, 3581, 20970, 15697, 17064, 8229, 26298, 22103, 753, 33478, 1244, 20495, 654, 25141, 5177, 16472, 4878, 22541, 18562, 33898, 34201, 33846, 32490, 10459, 35034, 18482, 27405, 25684, 59, 39111, 14494, 39398, 17707, 33798, 7503, 15517, 29906, 29202, 10211, 20925, 17778, 38091, 402, 22540, 16470, 35293, 8099, 15546, 35478, 11117, 35597, 5591, 4748, 30119, 30006, 37365, 18241, 23, 3442, 4305, 39005, 12899, 15646, 16820, 26401, 34234, 7101, 38558, 21978, 1762, 31661, 21113, 1700, 138, 26716, 31976, 39603, 1790, 12767, 34498, 11842, 16812, 31388, 2334, 3191, 10892, 11145, 23438, 22443, 31504, 33311, 7206, 29209, 28102, 14039, 40853, 5948, 39034, 35503, 26533, 15252, 5789, 37351, 18995, 36435, 32632, 15068, 8144, 5327, 26403, 16115, 11584, 11517, 28716, 26707, 11624, 16745, 34298, 39056, 35627, 25112, 11189, 5899, 41375, 17382, 632, 17134, 22107, 31411, 37151, 4957, 26136, 19424, 32190, 18680, 39798, 10039, 11764, 14839, 1643, 41354, 30654, 767, 28520, 20558, 4097, 7027, 35317, 25339, 26852, 2934, 34299, 40472, 5913, 5827, 9883, 40264, 29370, 38239, 1674, 28937, 37883, 12523, 16492, 33705, 21475, 39874, 9742, 24923, 33917, 26094, 35722, 18178, 29935, 37549, 38139, 19368, 31963, 21815, 35774, 10439, 23050, 1863, 24848, 4740, 9565, 13142, 18722, 33537, 41145, 38925, 8856, 8217, 14882, 13634, 20490, 10744, 34434, 34635, 30092, 26555, 10658, 16599, 14256, 23503, 2751, 12712, 28819, 10324, 9264, 8336, 33468, 37007, 20473, 26839, 39190, 27093, 15657, 1882, 11217, 23533, 24090, 3978, 31558, 35346, 33856, 7750, 4869, 40729, 17833, 28771, 40109, 11186, 12572, 24293, 2257, 8499, 29638, 7417, 7784, 10732, 11938, 36306, 21512, 15677, 34506, 22792, 4042, 27947, 9829, 5088, 18949, 18038, 35819, 33353, 40080, 20238, 15903, 18200, 8890, 23083, 32384, 9724, 6507, 10368, 36100, 35804, 31910, 2666, 8964, 29250, 11291, 40126, 33382, 2086, 40496, 32826, 34617, 24608, 20708, 35166, 1839, 3999, 18107, 36166, 15079, 6589, 34253, 37852, 32940, 37381, 27194, 21175, 31572, 37837, 2871, 39059, 38720, 12178, 27117, 40646, 14057, 281, 12839, 24462, 30399, 1442, 1193, 19519, 41503, 11502, 12868, 37583, 24026, 41192, 19993, 16996, 26671, 19763, 38343, 12641, 3154, 29551, 11770, 23135, 8185, 1457, 21454, 15900, 36830, 40013, 1081, 26848, 15588, 28836, 26958, 713, 30223, 40456, 16231, 38513, 33888, 25739, 25439, 35149, 17489, 11629, 34392, 21657, 10940, 23064, 25458, 32611, 2173, 17713, 9136, 7860, 19979, 1958, 5901, 14376, 23356, 26742, 38817, 37280, 13145, 34472, 22291, 36344, 16401, 9108, 10067, 6992, 10922, 34892, 33102, 28275, 31145, 4336, 24071, 10118, 38519, 26680, 24731, 36970, 7292, 41263, 37449, 5066, 40921, 32503, 7849, 26227, 38562, 34354, 9978, 38701, 30386, 6174, 35848, 16840, 12612, 29566, 35872, 34808, 13515, 2031, 25255, 9592, 34988, 13248, 542, 12116, 19968, 26027, 9252, 26066, 2242, 19300, 11812, 30020, 31057, 25763, 26854, 4402, 26032, 12479, 29970, 29594, 21784, 25927, 39965, 19410, 13140, 357, 9145, 6737, 35949, 10597, 27782, 16738, 16873, 9248, 11508, 7288, 33759, 37792, 34172, 22631, 16246, 17796, 12744, 23210, 34461, 32934, 3964, 11308])\n",
      "[('id', 2215), ('id', 10819), ('id', 29704), ('id', 28100), ('id', 24010), ('id', 4349), ('id', 16795), ('id', 9996), ('id', 43950), ('id', 42235), ('id', 20175), ('id', 40178), ('id', 17346), ('id', 20176), ('id', 6246), ('id', 15572), ('id', 9206), ('id', 1684), ('id', 11756), ('id', 26033), ('id', 36909), ('id', 17988), ('id', 1741), ('id', 43968), ('id', 44215), ('id', 34838), ('id', 27991), ('id', 29030), ('id', 42496), ('id', 25277), ('id', 40331), ('id', 4883), ('id', 32086), ('id', 32298), ('id', 18208), ('id', 8832), ('id', 31805), ('id', 36866), ('id', 19315), ('id', 40429), ('id', 41958), ('id', 11390), ('id', 44039), ('id', 18847), ('id', 25367), ('id', 9085), ('id', 42138), ('id', 10426), ('id', 40380), ('id', 42186), ('id', 12274), ('id', 43146), ('id', 37899), ('id', 32663), ('id', 2632), ('id', 10678), ('id', 29394), ('id', 28401), ('id', 26052), ('id', 9029), ('id', 33160), ('id', 29451), ('id', 15278), ('id', 18512), ('id', 7135), ('id', 20150), ('id', 22001), ('id', 5040), ('id', 3819), ('id', 7866), ('id', 43656), ('id', 34030), ('id', 25283), ('id', 865), ('id', 31703), ('id', 8865), ('id', 42106), ('id', 23498), ('id', 42210), ('id', 42521), ('id', 30014), ('id', 26598), ('id', 22746), ('id', 9184), ('id', 21854), ('id', 21650), ('id', 18760), ('id', 24386), ('id', 41155), ('id', 22835), ('id', 20312), ('id', 38017), ('id', 40736), ('id', 8001), ('id', 2135), ('id', 24864), ('id', 36598), ('id', 32510), ('id', 45797), ('id', 40984), ('id', 21500), ('id', 19881), ('id', 26906), ('id', 15864), ('id', 45201), ('id', 38908), ('id', 18407), ('id', 11925), ('id', 34152), ('id', 31709), ('id', 45719), ('id', 16024), ('id', 43673), ('id', 14363), ('id', 22071), ('id', 3004), ('id', 40189), ('id', 11568), ('id', 12088), ('id', 31199), ('id', 30122), ('id', 20673), ('id', 36350), ('id', 5721), ('id', 13541), ('id', 42782), ('id', 8704), ('id', 43268), ('id', 38873), ('id', 1508), ('id', 15635), ('id', 25890), ('id', 38424), ('id', 21690), ('id', 23284), ('id', 7476), ('id', 28553), ('id', 36942), ('id', 45769), ('id', 30561), ('id', 37162), ('id', 44494), ('id', 35094), ('id', 29029), ('id', 18661), ('id', 27690), ('id', 4490), ('id', 16526), ('id', 42091), ('id', 30994), ('id', 38652), ('id', 5673), ('id', 26856), ('id', 10609), ('id', 15964), ('id', 36905), ('id', 1064), ('id', 29653), ('id', 1206), ('id', 38950), ('id', 13398), ('id', 19979), ('id', 7099), ('id', 14517), ('id', 8043), ('id', 19276), ('id', 8293), ('id', 8300), ('id', 41535), ('id', 6081), ('id', 14528), ('id', 38300), ('id', 1783), ('id', 10372), ('id', 26146), ('id', 35356), ('id', 38853), ('id', 35739), ('id', 42035), ('id', 27331), ('id', 27907), ('id', 6214), ('id', 33150), ('id', 16552), ('id', 20215), ('id', 34834), ('id', 17235), ('id', 3889), ('id', 27460), ('id', 5957), ('id', 20894), ('id', 44497), ('id', 43204), ('id', 17254), ('id', 14325), ('id', 5966), ('id', 44685), ('id', 18950), ('id', 12163), ('id', 44105), ('id', 35396), ('id', 11873), ('id', 1040), ('id', 13091), ('id', 13355), ('id', 25232), ('id', 41246), ('id', 8531), ('id', 40760), ('id', 6083), ('id', 6902), ('id', 33858), ('id', 12004), ('id', 434), ('id', 3836), ('id', 45285), ('id', 3965), ('id', 3796), ('id', 12729), ('id', 250), ('id', 29983), ('id', 45417), ('id', 14298), ('id', 29866), ('id', 37586), ('id', 25823), ('id', 31646), ('id', 26379), ('id', 9779), ('id', 3456), ('id', 40165), ('id', 41683), ('id', 16890), ('id', 32988), ('id', 10434), ('id', 18494), ('id', 8848), ('id', 43468), ('id', 18019), ('id', 39708), ('id', 18157), ('id', 33297), ('id', 17543), ('id', 14685), ('id', 44223), ('id', 23720), ('id', 16068), ('id', 31742), ('id', 8718), ('id', 36748), ('id', 35041), ('id', 7526), ('id', 21330), ('id', 12318), ('id', 23111), ('id', 18085), ('id', 3475), ('id', 18292), ('id', 27041), ('id', 19913), ('id', 38799), ('id', 10257), ('id', 25849), ('id', 31621), ('id', 24399), ('id', 43632), ('id', 28531), ('id', 25623), ('id', 44899), ('id', 42405), ('id', 20181), ('id', 26241), ('id', 27673), ('id', 13535), ('id', 41226), ('id', 31332), ('id', 43047), ('id', 627), ('id', 17591), ('id', 26506), ('id', 41636), ('id', 42128), ('id', 12711), ('id', 39612), ('id', 21458), ('id', 17227), ('id', 45446), ('id', 2417), ('id', 26199), ('id', 2704), ('id', 8674), ('id', 41186), ('id', 24307), ('id', 36648), ('id', 41283), ('id', 6886), ('id', 41012), ('id', 1181), ('id', 4399), ('id', 36276), ('id', 39387), ('id', 6099), ('id', 8149), ('id', 9001), ('id', 29613), ('id', 38560), ('id', 35315), ('id', 30318), ('id', 40228), ('id', 12042), ('id', 42424), ('id', 28252), ('id', 8403), ('id', 2055), ('id', 3440), ('id', 27487), ('id', 7763), ('id', 6702), ('id', 26700), ('id', 43635), ('id', 13988), ('id', 13697), ('id', 38438), ('id', 23682), ('id', 29660), ('id', 782), ('id', 6088), ('id', 30746), ('id', 10304), ('id', 44825), ('id', 854), ('id', 11594), ('id', 18644), ('id', 12497), ('id', 13183), ('id', 10276), ('id', 7712), ('id', 30281), ('id', 27707), ('id', 41297), ('id', 29392), ('id', 43737), ('id', 11868), ('id', 34213), ('id', 3944), ('id', 23168), ('id', 17311), ('id', 18816), ('id', 9054), ('id', 29086), ('id', 24435), ('id', 831), ('id', 37011), ('id', 1372), ('id', 22644), ('id', 721), ('id', 27795), ('id', 5709), ('id', 18154), ('id', 5383), ('id', 24921), ('id', 20475), ('id', 37478), ('id', 37814), ('id', 37418), ('id', 35904), ('id', 11503), ('id', 38717), ('id', 20388), ('id', 30307), ('id', 28402), ('id', 61), ('id', 43235), ('id', 15978), ('id', 43553), ('id', 19527), ('id', 37368), ('id', 8260), ('id', 17117), ('id', 33055), ('id', 32284), ('id', 11229), ('id', 23120), ('id', 19608), ('id', 42095), ('id', 450), ('id', 24920), ('id', 18152), ('id', 38996), ('id', 8907), ('id', 17150), ('id', 39201), ('id', 12235), ('id', 39333), ('id', 6167), ('id', 5242), ('id', 33291), ('id', 33166), ('id', 41285), ('id', 20127), ('id', 23), ('id', 3785), ('id', 4754), ('id', 43119), ('id', 14195), ('id', 17256), ('id', 18544), ('id', 29198), ('id', 37855), ('id', 7819), ('id', 42616), ('id', 24296), ('id', 1932), ('id', 34992), ('id', 23325), ('id', 1860), ('id', 152), ('id', 29549), ('id', 35345), ('id', 43777), ('id', 1962), ('id', 14053), ('id', 38145), ('id', 13025), ('id', 18534), ('id', 34692), ('id', 2559), ('id', 3494), ('id', 11986), ('id', 12267), ('id', 25909), ('id', 24811), ('id', 34824), ('id', 36829), ('id', 7935), ('id', 32293), ('id', 31073), ('id', 15465), ('id', 45164), ('id', 6558), ('id', 43151), ('id', 39228), ('id', 29347), ('id', 16823), ('id', 6384), ('id', 41270), ('id', 20956), ('id', 40258), ('id', 36065), ('id', 16618), ('id', 8957), ('id', 5878), ('id', 29200), ('id', 17762), ('id', 12741), ('id', 12672), ('id', 31758), ('id', 29539), ('id', 12786), ('id', 18455), ('id', 37924), ('id', 43174), ('id', 39364), ('id', 27761), ('id', 12314), ('id', 6503), ('id', 45749), ('id', 19162), ('id', 698), ('id', 18890), ('id', 24439), ('id', 34716), ('id', 41062), ('id', 5469), ('id', 28908), ('id', 21443), ('id', 35580), ('id', 20602), ('id', 43985), ('id', 11041), ('id', 12940), ('id', 16363), ('id', 1799), ('id', 45726), ('id', 33877), ('id', 846), ('id', 31540), ('id', 22713), ('id', 4526), ('id', 7734), ('id', 39023), ('id', 28016), ('id', 29697), ('id', 3219), ('id', 37926), ('id', 44737), ('id', 6517), ('id', 6424), ('id', 10867), ('id', 44509), ('id', 32472), ('id', 42258), ('id', 1831), ('id', 32001), ('id', 41860), ('id', 13780), ('id', 18177), ('id', 37262), ('id', 23730), ('id', 44072), ('id', 10713), ('id', 27546), ('id', 37500), ('id', 28864), ('id', 39467), ('id', 20052), ('id', 33087), ('id', 41491), ('id', 42146), ('id', 21381), ('id', 35331), ('id', 24110), ('id', 39523), ('id', 11482), ('id', 25487), ('id', 2046), ('id', 27465), ('id', 5231), ('id', 10518), ('id', 14465), ('id', 20654), ('id', 37077), ('id', 45494), ('id', 43028), ('id', 9747), ('id', 9039), ('id', 16413), ('id', 15020), ('id', 22639), ('id', 11814), ('id', 38075), ('id', 38296), ('id', 33262), ('id', 29371), ('id', 11722), ('id', 18288), ('id', 15708), ('id', 25980), ('id', 3009), ('id', 13993), ('id', 31871), ('id', 11356), ('id', 10187), ('id', 9172), ('id', 37000), ('id', 40895), ('id', 22622), ('id', 29682), ('id', 43319), ('id', 29959), ('id', 17267), ('id', 2065), ('id', 12343), ('id', 26010), ('id', 26620), ('id', 4391), ('id', 34879), ('id', 39056), ('id', 37429), ('id', 8523), ('id', 5373), ('id', 45020), ('id', 19671), ('id', 31819), ('id', 44329), ('id', 12311), ('id', 13833), ('id', 26842), ('id', 2474), ('id', 9344), ('id', 32762), ('id', 8165), ('id', 8563), ('id', 11801), ('id', 13126), ('id', 40116), ('id', 23769), ('id', 17288), ('id', 38153), ('id', 25195), ('id', 4463), ('id', 30898), ('id', 10808), ('id', 5611), ('id', 20905), ('id', 19898), ('id', 39572), ('id', 36877), ('id', 44297), ('id', 22361), ('id', 17534), ('id', 20078), ('id', 9782), ('id', 25523), ('id', 35787), ('id', 10694), ('id', 7168), ('id', 11409), ('id', 39878), ('id', 39557), ('id', 35273), ('id', 2919), ('id', 9863), ('id', 32337), ('id', 12425), ('id', 44352), ('id', 36908), ('id', 2284), ('id', 44764), ('id', 36277), ('id', 38277), ('id', 27194), ('id', 22879), ('id', 38860), ('id', 2018), ('id', 4412), ('id', 19974), ('id', 39958), ('id', 16632), ('id', 7255), ('id', 37875), ('id', 41827), ('id', 36411), ('id', 41302), ('id', 30075), ('id', 23396), ('id', 34894), ('id', 41811), ('id', 3149), ('id', 43177), ('id', 42804), ('id', 13393), ('id', 29985), ('id', 44930), ('id', 15486), ('id', 316), ('id', 14131), ('id', 27026), ('id', 33596), ('id', 1587), ('id', 1316), ('id', 21557), ('id', 45893), ('id', 12656), ('id', 14162), ('id', 41532), ('id', 26552), ('id', 45545), ('id', 22082), ('id', 18743), ('id', 29497), ('id', 21826), ('id', 42370), ('id', 13911), ('id', 3455), ('id', 32667), ('id', 12946), ('id', 25577), ('id', 9004), ('id', 1604), ('id', 23707), ('id', 17531), ('id', 40698), ('id', 44222), ('id', 1187), ('id', 29692), ('id', 17195), ('id', 31890), ('id', 29809), ('id', 785), ('id', 33407), ('id', 44721), ('id', 17887), ('id', 42562), ('id', 37467), ('id', 28463), ('id', 28125), ('id', 38840), ('id', 19281), ('id', 12793), ('id', 38029), ('id', 23935), ('id', 12038), ('id', 25504), ('id', 28148), ('id', 36042), ('id', 2384), ('id', 19534), ('id', 10050), ('id', 8644), ('id', 22066), ('id', 2146), ('id', 6505), ('id', 15842), ('id', 25820), ('id', 29578), ('id', 42909), ('id', 41197), ('id', 14468), ('id', 38117), ('id', 24641), ('id', 40157), ('id', 18078), ('id', 10021), ('id', 11075), ('id', 7697), ('id', 12020), ('id', 38567), ('id', 36591), ('id', 31264), ('id', 34427), ('id', 4788), ('id', 26599), ('id', 11129), ('id', 42568), ('id', 29510), ('id', 27333), ('id', 40856), ('id', 8033), ('id', 45624), ('id', 41377), ('id', 5589), ('id', 45241), ('id', 35917), ('id', 8631), ('id', 29008), ('id', 42620), ('id', 37988), ('id', 10976), ('id', 42778), ('id', 33583), ('id', 6805), ('id', 39604), ('id', 18567), ('id', 13878), ('id', 32682), ('id', 39629), ('id', 38479), ('id', 14885), ('id', 2227), ('id', 27924), ('id', 10550), ('id', 38669), ('id', 14587), ('id', 606), ('id', 13322), ('id', 22054), ('id', 28790), ('id', 10173), ('id', 28833), ('id', 2458), ('id', 21303), ('id', 12990), ('id', 33182), ('id', 34327), ('id', 28491), ('id', 29699), ('id', 4857), ('id', 28795), ('id', 13733), ('id', 33128), ('id', 32715), ('id', 24076), ('id', 28676), ('id', 44174), ('id', 21429), ('id', 14463), ('id', 404), ('id', 10061), ('id', 7419), ('id', 39712), ('id', 11656), ('id', 30715), ('id', 18445), ('id', 18606), ('id', 10169), ('id', 12662), ('id', 8028), ('id', 37320), ('id', 41761), ('id', 37779), ('id', 25020), ('id', 17902), ('id', 19628), ('id', 14026), ('id', 25659), ('id', 38104), ('id', 36405), ('id', 4376), ('id', 12445)]\n",
      "Acquiring (label, score)s: 0 (0.2676), 0 (0.2312), 0 (0.2303), 0 (0.2292), 0 (0.2231), 4 (0.218), 0 (0.2168), 0 (0.2153), 0 (0.2126), 0 (0.2106), 0 (0.2096), 0 (0.2088), 8 (0.207), 2 (0.206), 0 (0.2043), 2 (0.203), 0 (0.2023), 0 (0.1999), 0 (0.199), 0 (0.1979), 0 (0.1971), 0 (0.1945), 2 (0.193), 2 (0.1917), 2 (0.187), 2 (0.1855), 0 (0.182), 2 (0.1793), 0 (0.1782), 0 (0.1779), 2 (0.1757), 2 (0.175), 0 (0.174), 2 (0.1738), 0 (0.1738), 4 (0.1733), 0 (0.1724), 2 (0.1702), 0 (0.1694), 0 (0.168), 2 (0.1678), 0 (0.1677), 0 (0.1665), 0 (0.1662), 0 (0.1648), 0 (0.164), 0 (0.164), 8 (0.163), 0 (0.1625), 0 (0.1614), 0 (0.1597), 2 (0.1596), 4 (0.1587), 0 (0.1581), 0 (0.155), 0 (0.1545), 0 (0.1542), 0 (0.1528), 4 (0.1526), 8 (0.1526), 0 (0.1514), 8 (0.1508), 0 (0.1474), 0 (0.1462), 8 (0.1455), 0 (0.1453), 8 (0.1452), 0 (0.1445), 0 (0.1439), 0 (0.1435), 4 (0.1427), 4 (0.1427), 0 (0.1426), 2 (0.1425), 0 (0.1408), 0 (0.1408), 0 (0.1402), 2 (0.1398), 0 (0.1397), 4 (0.1385), 1 (0.1383), 2 (0.1383), 2 (0.1378), 8 (0.1377), 4 (0.1369), 0 (0.1366), 0 (0.1364), 2 (0.1364), 2 (0.1364), 2 (0.1354), 4 (0.135), 2 (0.1342), 2 (0.1336), 0 (0.1328), 0 (0.1319), 8 (0.1319), 0 (0.1315), 0 (0.131), 8 (0.1302), 8 (0.1298), 0 (0.1294), 0 (0.1281), 0 (0.1277), 8 (0.1269), 0 (0.1268), 9 (0.1263), 2 (0.1262), 2 (0.1258), 8 (0.1256), 2 (0.1256), 0 (0.1255), 0 (0.1252), 0 (0.1247), 0 (0.1241), 0 (0.1234), 1 (0.1225), 0 (0.1221), 0 (0.1219), 2 (0.1214), 8 (0.1213), 0 (0.1207), 0 (0.1207), 0 (0.1206), 0 (0.1206), 0 (0.1206), 4 (0.1204), 2 (0.1199), 8 (0.119), 0 (0.1188), 0 (0.118), 2 (0.1178), 0 (0.1169), 2 (0.1162), 0 (0.1161), 0 (0.116), 2 (0.1158), 8 (0.1153), 2 (0.1153), 8 (0.1152), 0 (0.115), 0 (0.1149), 0 (0.1146), 8 (0.114), 2 (0.1138), 0 (0.1136), 0 (0.1134), 0 (0.1133), 0 (0.1132), 2 (0.1128), 4 (0.1128), 0 (0.1125), 8 (0.1123), 4 (0.1121), 0 (0.112), 0 (0.112), 8 (0.1118), 0 (0.1112), 0 (0.111), 0 (0.1108), 2 (0.1107), 2 (0.1105), 0 (0.1105), 0 (0.1099), 0 (0.1095), 8 (0.1093), 8 (0.1091), 0 (0.1091), 0 (0.1088), 2 (0.1087), 2 (0.1085), 0 (0.1083), 2 (0.1075), 0 (0.1064), 0 (0.1062), 0 (0.1062), 2 (0.1058), 8 (0.1058), 8 (0.1058), 0 (0.1058), 8 (0.1054), 2 (0.1053), 0 (0.1053), 0 (0.1051), 8 (0.1049), 1 (0.1048), 2 (0.1044), 0 (0.1042), 1 (0.1042), 2 (0.1036), 4 (0.1033), 8 (0.103), 8 (0.103), 4 (0.103), 0 (0.1028), 0 (0.1026), 8 (0.1025), 8 (0.1023), 0 (0.1022), 8 (0.1022), 2 (0.1018), 8 (0.1018), 0 (0.1016), 2 (0.1016), 0 (0.1014), 2 (0.1013), 0 (0.1011), 0 (0.1009), 0 (0.1009), 0 (0.1008), 2 (0.1008), 8 (0.1005), 0 (0.1002), 0 (0.09999), 0 (0.09998), 2 (0.09981), 0 (0.09972), 2 (0.09966), 0 (0.09966), 0 (0.09945), 0 (0.0994), 8 (0.09926), 8 (0.09921), 0 (0.09901), 0 (0.09892), 0 (0.09855), 1 (0.09842), 0 (0.09829), 0 (0.09821), 8 (0.0982), 1 (0.0982), 0 (0.0981), 2 (0.09801), 0 (0.09768), 0 (0.09767), 2 (0.09751), 8 (0.09736), 0 (0.09734), 0 (0.09733), 0 (0.09724), 8 (0.0971), 8 (0.09679), 0 (0.09659), 0 (0.09583), 8 (0.09561), 0 (0.09554), 0 (0.09507), 0 (0.09498), 2 (0.09495), 8 (0.09492), 8 (0.09484), 2 (0.09477), 0 (0.09464), 8 (0.09455), 1 (0.09451), 0 (0.09448), 8 (0.09434), 0 (0.09406), 0 (0.09387), 8 (0.09381), 8 (0.0938), 2 (0.09354), 0 (0.09327), 2 (0.09298), 0 (0.09289), 0 (0.09289), 8 (0.09281), 8 (0.09273), 8 (0.09243), 0 (0.09231), 8 (0.0922), 0 (0.09216), 0 (0.09179), 4 (0.09165), 0 (0.09164), 8 (0.09135), 2 (0.09127), 8 (0.09116), 0 (0.09112), 2 (0.09068), 2 (0.09048), 0 (0.09031), 0 (0.0903), 0 (0.08986), 0 (0.08976), 0 (0.08971), 8 (0.08943), 0 (0.08938), 0 (0.08938), 0 (0.08935), 8 (0.08923), 8 (0.08913), 8 (0.08897), 4 (0.08891), 2 (0.08889), 8 (0.08836), 0 (0.08819), 2 (0.08792), 8 (0.0874), 0 (0.08719), 0 (0.08709), 8 (0.08698), 2 (0.08692), 8 (0.08687), 8 (0.08661), 0 (0.08633), 2 (0.08623), 0 (0.08616), 1 (0.08615), 0 (0.08609), 8 (0.08607), 0 (0.08568), 0 (0.08552), 8 (0.08531), 0 (0.08528), 0 (0.08518), 8 (0.08516), 8 (0.08505), 0 (0.08503), 2 (0.08495), 8 (0.08484), 8 (0.08471), 0 (0.08471), 8 (0.08471), 0 (0.08459), 0 (0.08408), 8 (0.08403), 0 (0.08386), 2 (0.08385), 0 (0.08362), 8 (0.08316), 0 (0.08314), 0 (0.08311), 0 (0.0829), 0 (0.08283), 8 (0.08281), 9 (0.08278), 8 (0.08268), 0 (0.08268), 8 (0.0826), 8 (0.08247), 0 (0.0824), 2 (0.08239), 0 (0.08226), 8 (0.08221), 0 (0.0821), 1 (0.08179), 0 (0.08158), 4 (0.08144), 8 (0.08118), 8 (0.08112), 0 (0.08102), 2 (0.0809), 8 (0.08075), 2 (0.08061), 8 (0.08044), 9 (0.08029), 0 (0.0802), 0 (0.08019), 0 (0.08007), 0 (0.07987), 2 (0.07984), 0 (0.07982), 0 (0.07972), 0 (0.07972), 0 (0.07961), 0 (0.07955), 0 (0.07943), 0 (0.07915), 0 (0.07888), 2 (0.07887), 4 (0.07881), 8 (0.07878), 0 (0.07876), 2 (0.07864), 8 (0.07861), 2 (0.0785), 0 (0.07835), 0 (0.07819), 2 (0.07792), 0 (0.07768), 0 (0.07765), 0 (0.07765), 8 (0.07744), 8 (0.07743), 8 (0.07733), 0 (0.07732), 0 (0.07724), 1 (0.07715), 9 (0.07706), 2 (0.07699), 0 (0.07658), 0 (0.07653), 2 (0.07648), 0 (0.07621), 8 (0.07619), 0 (0.07607), 0 (0.07606), 0 (0.07601), 2 (0.07551), 0 (0.07538), 8 (0.07536), 0 (0.07527), 0 (0.07523), 0 (0.0751), 8 (0.07506), 0 (0.07498), 8 (0.07492), 8 (0.07482), 2 (0.07476), 0 (0.07464), 0 (0.07464), 2 (0.07462), 8 (0.07456), 8 (0.07448), 0 (0.07432), 0 (0.07429), 4 (0.07429), 0 (0.07424), 8 (0.07423), 8 (0.07422), 8 (0.07418), 0 (0.07405), 0 (0.074), 4 (0.07398), 0 (0.07391), 3 (0.0739), 0 (0.0739), 0 (0.07384), 9 (0.07379), 8 (0.07351), 0 (0.07347), 0 (0.07346), 8 (0.07338), 0 (0.07331), 0 (0.0733), 0 (0.0731), 2 (0.07309), 2 (0.07307), 8 (0.073), 8 (0.07298), 0 (0.07274), 9 (0.07274), 2 (0.07266), 2 (0.07262), 0 (0.07261), 0 (0.0724), 2 (0.0723), 2 (0.07218), 0 (0.07206), 8 (0.07205), 8 (0.07196), 1 (0.07188), 8 (0.07187), 2 (0.07183), 8 (0.07169), 0 (0.07161), 2 (0.07148), 0 (0.07146), 8 (0.07141), 8 (0.07136), 8 (0.07128), 0 (0.07115), 0 (0.07107), 4 (0.071), 7 (0.07094), 0 (0.07085), 2 (0.07078), 0 (0.07078), 2 (0.07073), 8 (0.07053), 8 (0.0705), 0 (0.07047), 0 (0.07018), 0 (0.07008), 8 (0.07007), 8 (0.07002), 0 (0.06988), 9 (0.06987), 2 (0.06982), 0 (0.06968), 3 (0.06958), 0 (0.06949), 8 (0.06947), 2 (0.06938), 0 (0.06937), 9 (0.06935), 1 (0.0693), 9 (0.06929), 8 (0.06911), 5 (0.06905), 0 (0.06889), 0 (0.06866), 0 (0.0686), 8 (0.06858), 0 (0.06853), 0 (0.06832), 8 (0.06831), 0 (0.06822), 0 (0.06822), 8 (0.06812), 0 (0.06799), 0 (0.0678), 8 (0.06779), 2 (0.06758), 8 (0.06742), 0 (0.06742), 0 (0.06732), 8 (0.06697), 8 (0.06684), 4 (0.06682), 0 (0.0668), 0 (0.06676), 2 (0.06653), 2 (0.06647), 8 (0.06631), 0 (0.06631), 0 (0.06612), 2 (0.06603), 8 (0.06585), 8 (0.06578), 5 (0.06577), 8 (0.06576), 0 (0.06565), 0 (0.06562), 0 (0.06557), 4 (0.06551), 0 (0.06549), 0 (0.0654), 0 (0.06539), 2 (0.06536), 2 (0.06534), 0 (0.06529), 0 (0.06524), 8 (0.06517), 6 (0.06515), 0 (0.06513), 1 (0.06508), 2 (0.06498), 2 (0.06497), 5 (0.06493), 0 (0.0649), 0 (0.0646), 8 (0.0646), 0 (0.06448), 8 (0.06447), 0 (0.0644), 2 (0.06429), 0 (0.06423), 0 (0.06411), 0 (0.06376), 8 (0.06374), 0 (0.06374), 8 (0.06374), 0 (0.06364), 2 (0.06364), 0 (0.06354), 2 (0.0635), 2 (0.06346), 1 (0.06342), 8 (0.06326), 2 (0.06306), 2 (0.06304), 8 (0.063), 1 (0.06294), 3 (0.06276), 8 (0.06263), 2 (0.06245), 4 (0.0622), 0 (0.06219), 1 (0.06202), 0 (0.06173), 0 (0.06172), 4 (0.06161), 0 (0.06155), 0 (0.06144), 0 (0.06143), 9 (0.06139), 9 (0.06136), 0 (0.06136), 8 (0.06134), 4 (0.06127), 8 (0.06104), 0 (0.061), 8 (0.061), 8 (0.06094), 8 (0.06091), 0 (0.06076), 2 (0.06075), 2 (0.06075), 0 (0.06032), 8 (0.06023), 8 (0.06019), 4 (0.06018), 0 (0.06012), 2 (0.06005), 8 (0.05995), 0 (0.05991), 5 (0.0599), 2 (0.05981), 0 (0.05981), 8 (0.05978), 0 (0.05976), 0 (0.05963), 8 (0.05956), 4 (0.05942), 1 (0.0594), 8 (0.05934), 2 (0.05914), 0 (0.05909), 0 (0.05908), 0 (0.05894), 0 (0.05892), 0 (0.05887), 2 (0.05886), 0 (0.05884), 0 (0.05876), 0 (0.05873), 0 (0.0587), 8 (0.05843), 8 (0.05834), 8 (0.05831), 0 (0.05817), 0 (0.05816), 0 (0.05813), 0 (0.05805), 2 (0.0578), 0 (0.05777), 8 (0.05775), 0 (0.05771), 8 (0.05769), 8 (0.05768), 0 (0.05758), 0 (0.05755), 8 (0.05748), 4 (0.05741), 2 (0.05739), 2 (0.05703), 8 (0.05691), 0 (0.05681), 8 (0.05676), 0 (0.05666), 0 (0.05653), 0 (0.05647), 8 (0.05645), 2 (0.05645), 8 (0.0564), 8 (0.05639), 0 (0.05636), 0 (0.05622), 8 (0.0562), 8 (0.0562), 2 (0.0562), 2 (0.05618), 9 (0.0561), 0 (0.05609), 0 (0.05601), 6 (0.05594), 8 (0.0559), 0 (0.05582), 0 (0.0558), 8 (0.05569), 8 (0.05564), 0 (0.05549), 2 (0.05548), 0 (0.05538), 3 (0.05532), 0 (0.05526), 0 (0.0552), 8 (0.05519), 0 (0.05514), 0 (0.05506), 8 (0.05505), 8 (0.05488), 8 (0.0547), 2 (0.05465), 2 (0.05445), 0 (0.05439), 2 (0.05434), 8 (0.0543), 0 (0.0543), 1 (0.05427), 0 (0.05414), 2 (0.05414), 0 (0.05406), 0 (0.05406), 0 (0.05397), 2 (0.05393), 2 (0.05386), 0 (0.05385), 0 (0.05383), 0 (0.05379), 0 (0.05375), 3 (0.05372), 8 (0.05368), 8 (0.05363), 8 (0.05354), 8 (0.05346), 2 (0.05343), 8 (0.05332), 2 (0.05329), 0 (0.05312), 0 (0.05309), 2 (0.05306), 4 (0.05305), 8 (0.05304), 0 (0.05302), 8 (0.05296), 0 (0.05291), 0 (0.0529), 8 (0.05282), 1 (0.05278), 0 (0.05277), 1 (0.05271), 8 (0.05266), 0 (0.05264), 1 (0.05252), 0 (0.05239), 2 (0.05234), 2 (0.05229), 0 (0.05225), 0 (0.05222), 0 (0.05212), 2 (0.05212), 2 (0.05211), 4 (0.05204), 8 (0.05199), 0 (0.05199), 0 (0.05195), 9 (0.05191), 8 (0.0519), 0 (0.05184), 0 (0.05182), 0 (0.05177), 3 (0.05173), 0 (0.05162), 8 (0.05153), 0 (0.05151), 8 (0.05151), 0 (0.05145), 0 (0.05139), 0 (0.05138), 0 (0.0513), 8 (0.05126), 0 (0.0512), 0 (0.05114), 2 (0.0511), 0 (0.05098), 9 (0.05098), 7 (0.05097), 8 (0.05089), 0 (0.05084), 0 (0.05075), 8 (0.05069), 2 (0.05068), 8 (0.05066), 0 (0.05059), 2 (0.05052), 0 (0.05048), 2 (0.05045), 9 (0.05041), 8 (0.0504), 0 (0.0503), 8 (0.0503), 0 (0.05028), 0 (0.05027), 8 (0.05023), 8 (0.04999), 0 (0.04993), 0 (0.04979), 3 (0.04967), 8 (0.04959), 2 (0.04953), 4 (0.04951), 8 (0.04944), 8 (0.0494), 8 (0.04939), 0 (0.04936), 6 (0.04934), 0 (0.04927), 0 (0.04924), 8 (0.04924), 4 (0.04922), 8 (0.04919), 0 (0.04911), 8 (0.04906), 3 (0.04905), 2 (0.04905), 0 (0.04903), 2 (0.04899), 0 (0.04899), 8 (0.04898), 0 (0.04896), 8 (0.04894), 0 (0.04876), 2 (0.04875), 4 (0.04871), 0 (0.04869), 0 (0.04868), 2 (0.04858), 8 (0.04857)\n",
      "Training set size 1200:\n",
      "Cosine Annealing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592a33b58cbe4a84be9a31a10636930b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "100%|##########| 1/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b0a3642a1d542e282bf366d75936050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/39]   3%|2          [00:00<?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-debfac4d65fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_training_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_pool_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/unified_experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mactive_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/unified_experiment.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, log)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             trained_model = model_trainer.get_trained(\n\u001b[0m\u001b[1;32m    103\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mtrain_augmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_augmentations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/resnet_models.py\u001b[0m in \u001b[0;36mget_trained\u001b[0;34m(self, train_loader, train_augmentations, validation_loader, log, loss, validation_loss)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cosine Annealing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             train_with_cosine_annealing(\n\u001b[0m\u001b[1;32m    403\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/black_box_model_training.py\u001b[0m in \u001b[0;36mtrain_with_cosine_annealing\u001b[0;34m(model, training_samples, validation_samples, train_loader, validation_loader, max_epochs, device, training_log, loss, validation_loss, optimizer, train_augmentations)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# Kick everything off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;31m# Return the optimizer in case we want to continue training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                 \u001b[0;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/ignite/engine/__init__.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/kornia/augmentation/augmentation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, params, return_transform)\u001b[0m\n\u001b[1;32m   1135\u001b[0m             \u001b[0minput_pad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecrop_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_pad\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;31m# Update the actual input size for inverse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/kornia/augmentation/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, params, return_transform)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_transform_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mori_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeepdim\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/kornia/augmentation/base.py\u001b[0m in \u001b[0;36mapply_func\u001b[0;34m(self, in_tensor, in_transform, params, return_transform)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_apply\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_apply\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtrans_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/kornia/augmentation/augmentation.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(self, input, params, transform)\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m                 \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "store = {}\n",
    "\n",
    "config=configs[2]\n",
    "\n",
    "config.max_training_set = 2000\n",
    "config.experiment_data_config.initial_training_set_size = 400  # int(50000/10*4-1000)\n",
    "config.max_training_epochs = 1\n",
    "config.num_pool_samples = 5\n",
    "config.run(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
