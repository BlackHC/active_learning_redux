{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An experiment template\n",
    "> Modularity might not be the solution, but it's all we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from ignite.contrib.engines.common import (\n",
    "    add_early_stopping_by_val_score,\n",
    "    setup_common_training_handlers,\n",
    ")\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events, create_supervised_evaluator, create_supervised_trainer\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from batchbald_redux.active_learning import (\n",
    "    ActiveLearningData,\n",
    "    RandomFixedLengthSampler,\n",
    "    get_base_indices,\n",
    ")\n",
    "from batchbald_redux.batchbald import get_bald_batch\n",
    "from batchbald_redux.black_box_model_training import evaluate, get_predictions, train\n",
    "from batchbald_redux.consistent_mc_dropout import (\n",
    "    GeometricMeanPrediction,\n",
    "    SamplerModel,\n",
    "    geometric_mean_loss,\n",
    "    multi_sample_loss,\n",
    ")\n",
    "from batchbald_redux.example_models import BayesianMNISTCNN\n",
    "from batchbald_redux.fast_mnist import FastMNIST\n",
    "from batchbald_redux.repeated_mnist import create_repeated_MNIST_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    acquisition_size: int = 10\n",
    "    max_training_set: int = 300\n",
    "    num_pool_samples: int = 20\n",
    "    num_eval_samples: int = 4\n",
    "    num_training_samples: int = 1\n",
    "    num_patience_epochs: int = 3\n",
    "    max_training_epochs: int = 10\n",
    "    device = \"cuda\"\n",
    "    validation_set_size: int = 1024\n",
    "    initial_set_size: int = 20\n",
    "    samples_per_epoch: int = 32768\n",
    "\n",
    "    def load_dataset(self) -> (ActiveLearningData, Dataset, Dataset):\n",
    "        train_dataset, test_dataset = create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False)\n",
    "        active_learning_data = ActiveLearningData(train_dataset)\n",
    "\n",
    "        validation_dataset = active_learning_data.extract_dataset_from_pool(self.validation_set_size)\n",
    "\n",
    "        return active_learning_data, validation_dataset, test_dataset\n",
    "\n",
    "    def new_model(self):\n",
    "        return BayesianMNISTCNN()\n",
    "\n",
    "    def new_optimizer(self, model):\n",
    "        return torch.optim.Adam(model.parameters(), weight_decay=5e-4)\n",
    "\n",
    "    def get_candidate_batch(self, model, pool_loader):\n",
    "        # Evaluate pool set\n",
    "        bald_model = SamplerModel(model, self.num_pool_samples)\n",
    "        pool_log_probs_N_K_C = get_predictions(model=bald_model, loader=pool_loader, device=self.device)\n",
    "\n",
    "        # Evaluate BALD scores\n",
    "        candidate_batch = get_bald_batch(\n",
    "            pool_log_probs_N_K_C, batch_size=self.acquisition_size, dtype=torch.double, device=self.device\n",
    "        )\n",
    "        return candidate_batch\n",
    "\n",
    "    def run(self, results):\n",
    "        results[\"hparams\"] = dataclasses.asdict(self)\n",
    "\n",
    "        # Active Learning setup\n",
    "        active_learning_data, validation_dataset, test_dataset = self.load_dataset()\n",
    "\n",
    "        initial_training_set_indices = active_learning_data.get_random_pool_indices(self.initial_set_size)\n",
    "        active_learning_data.acquire(initial_training_set_indices)\n",
    "\n",
    "        results[\"initial_training_set_indices\"] = initial_training_set_indices\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            active_learning_data.training_dataset,\n",
    "            batch_size=64,\n",
    "            sampler=RandomFixedLengthSampler(active_learning_data.training_dataset, self.samples_per_epoch),\n",
    "        )\n",
    "        pool_loader = torch.utils.data.DataLoader(active_learning_data.pool_dataset, batch_size=64, drop_last=False)\n",
    "\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, drop_last=False)\n",
    "\n",
    "        results[\"active_learning_steps\"] = []\n",
    "        active_learning_steps = results[\"active_learning_steps\"]\n",
    "\n",
    "        # Active Training Loop\n",
    "        while True:\n",
    "            training_set_size = len(active_learning_data.training_dataset)\n",
    "            print(f\"Training set size {training_set_size}:\")\n",
    "\n",
    "            iteration_log = dict(training_log=[], evalution_metrics=None, acquisition=None)\n",
    "\n",
    "            model = self.new_model()\n",
    "            optimizer = self.new_optimizer(model)\n",
    "            train(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                training_samples=self.num_training_samples,\n",
    "                validation_samples=self.num_eval_samples,\n",
    "                train_loader=train_loader,\n",
    "                validation_loader=validation_loader,\n",
    "                patience=self.num_patience_epochs,\n",
    "                max_epochs=self.max_training_epochs,\n",
    "                device=self.device,\n",
    "                epochs_log=iteration_log[\"training_log\"],\n",
    "            )\n",
    "\n",
    "            evaluation_metrics = evaluate(\n",
    "                model=model, num_samples=self.num_eval_samples, loader=test_loader, device=self.device\n",
    "            )\n",
    "            iteration_log[\"evalution_metrics\"] = evaluation_metrics\n",
    "            print(f\"Perf after training {evaluation_metrics}\")\n",
    "\n",
    "            if training_set_size >= self.max_training_set:\n",
    "                print(\"Done.\")\n",
    "                break\n",
    "\n",
    "            candidate_batch = self.get_candidate_batch(model, pool_loader)\n",
    "\n",
    "            candidate_global_indices = get_base_indices(active_learning_data.pool_dataset, candidate_batch.indices)\n",
    "            candidate_labels = [\n",
    "                active_learning_data.dataset.targets[index].item() for index in candidate_global_indices\n",
    "            ]\n",
    "\n",
    "            iteration_log[\"acquisition\"] = dict(\n",
    "                indices=candidate_global_indices, labels=candidate_labels, scores=candidate_batch.scores\n",
    "            )\n",
    "\n",
    "            active_learning_data.acquire(candidate_batch.indices)\n",
    "\n",
    "            ls = \", \".join(f\"{label} ({score:.4})\" for label, score in zip(candidate_labels, candidate_batch.scores))\n",
    "            print(f\"Acquiring (label, score)s: {ls}\")\n",
    "\n",
    "            active_learning_steps.append(iteration_log)\n",
    "\n",
    "        return active_learning_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = Experiment(max_training_epochs=1, max_training_set=150)\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
