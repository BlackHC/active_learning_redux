{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n",
    "> Can we get better by training on our assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "import blackhc.project.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and functions were are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended /home/blackhc/PycharmProjects/bald-ical/src to paths\n",
      "Switched to directory /home/blackhc/PycharmProjects/bald-ical\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n"
     ]
    }
   ],
   "source": [
    "# exports\n",
    "\n",
    "import dataclasses\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "import blackhc.project.script\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from blackhc.project.experiment import embedded_experiments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from batchbald_redux.active_learning import (\n",
    "    ActiveLearningData,\n",
    "    RandomFixedLengthSampler,\n",
    "    get_balanced_sample_indices,\n",
    "    get_base_indices,\n",
    ")\n",
    "from batchbald_redux.batchbald import (\n",
    "    CandidateBatch,\n",
    "    get_bald_batch,\n",
    "    get_batchbald_batch,\n",
    "    get_batchbaldical_batch,\n",
    "    get_random_bald_batch,\n",
    "    get_thompson_bald_batch,\n",
    ")\n",
    "from batchbald_redux.black_box_model_training import evaluate, get_predictions, train\n",
    "from batchbald_redux.consistent_mc_dropout import SamplerModel\n",
    "from batchbald_redux.example_models import BayesianMNISTCNN\n",
    "from batchbald_redux.repeated_mnist import create_repeated_MNIST_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class AcquisitionFunction(Enum):\n",
    "    random = \"Random\"\n",
    "    bald = \"BALD\"\n",
    "    batchbald = \"BatchBALD\"\n",
    "    randombald = \"RandomBALD\"\n",
    "    thompsonbald = \"ThompsonBALD\"\n",
    "    batchbaldical = \"BatchBALD-ICAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "class PredictionDataset(torch.utils.data.Dataset):\n",
    "    dataset: torch.utils.data.Dataset\n",
    "    predictions: torch.Tensor\n",
    "\n",
    "    def __init__(self, dataset, predictions):\n",
    "        assert len(dataset) == predictions.shape[0], f\"{len(dataset)} == {predictions.shape[0]}\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.predictions = predictions\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.dataset[index]\n",
    "        p = self.predictions[index]\n",
    "        return x, p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "# From the BatchBALD Repo\n",
    "\n",
    "mnist_initial_samples = [\n",
    "    38043,\n",
    "    40091,\n",
    "    17418,\n",
    "    2094,\n",
    "    39879,\n",
    "    3133,\n",
    "    5011,\n",
    "    40683,\n",
    "    54379,\n",
    "    24287,\n",
    "    9849,\n",
    "    59305,\n",
    "    39508,\n",
    "    39356,\n",
    "    8758,\n",
    "    52579,\n",
    "    13655,\n",
    "    7636,\n",
    "    21562,\n",
    "    41329,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    seed: int = 1337\n",
    "    acquisition_size: int = 5\n",
    "    max_training_set: int = 300\n",
    "    num_pool_samples: int = 20\n",
    "    num_eval_samples: int = 4\n",
    "    num_training_samples: int = 1\n",
    "    num_patience_epochs: int = 3\n",
    "    max_training_epochs: int = 10\n",
    "    device = \"cuda\"\n",
    "    validation_set_size: int = 4096\n",
    "    initial_set_size: int = 20\n",
    "    samples_per_epoch: int = 4096 * 6\n",
    "    repeated_mnist_repetitions: int = 2\n",
    "    add_dataset_noise: bool = True\n",
    "    acquisition_function: AcquisitionFunction = AcquisitionFunction.bald\n",
    "\n",
    "    def load_dataset(self) -> (ActiveLearningData, Dataset, Dataset):\n",
    "        train_dataset, test_dataset = create_repeated_MNIST_dataset(\n",
    "            num_repetitions=self.repeated_mnist_repetitions, add_noise=self.add_dataset_noise\n",
    "        )\n",
    "        active_learning_data = ActiveLearningData(train_dataset)\n",
    "\n",
    "        validation_dataset = active_learning_data.extract_dataset_from_pool(self.validation_set_size)\n",
    "\n",
    "        return active_learning_data, validation_dataset, test_dataset\n",
    "\n",
    "    def new_model(self):\n",
    "        return BayesianMNISTCNN()\n",
    "\n",
    "    def new_optimizer(self, model):\n",
    "        return torch.optim.Adam(model.parameters(), weight_decay=5e-4)\n",
    "\n",
    "    def get_random_candidate_batch(self, num_pool_samples):\n",
    "        indices = np.random.choice(num_pool_samples, size=self.acquisition_size, replace=False)\n",
    "        return CandidateBatch([0.0] * self.acquisition_size, indices)\n",
    "\n",
    "    def get_thompson_bald_candidate_batch(self, model, pool_loader):\n",
    "        # Evaluate pool set\n",
    "        log_probs_N_K_C = get_predictions(\n",
    "            model=model, num_samples=self.num_pool_samples, num_classes=10, loader=pool_loader, device=self.device\n",
    "        )\n",
    "\n",
    "        # Evaluate BALD scores\n",
    "        candidate_batch = get_thompson_bald_batch(\n",
    "            log_probs_N_K_C,\n",
    "            batch_size=self.acquisition_size,\n",
    "            dtype=torch.double,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return candidate_batch\n",
    "\n",
    "    def get_random_bald_candidate_batch(self, model, pool_loader):\n",
    "        # Evaluate pool set\n",
    "        log_probs_N_K_C = get_predictions(\n",
    "            model=model, num_samples=self.num_pool_samples, num_classes=10, loader=pool_loader, device=self.device\n",
    "        )\n",
    "\n",
    "        # Evaluate BALD scores\n",
    "        candidate_batch = get_random_bald_batch(\n",
    "            log_probs_N_K_C,\n",
    "            batch_size=self.acquisition_size,\n",
    "            dtype=torch.double,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return candidate_batch\n",
    "\n",
    "    def get_bald_candidate_batch(self, model, pool_loader):\n",
    "        # Evaluate pool set\n",
    "        log_probs_N_K_C = get_predictions(\n",
    "            model=model, num_samples=self.num_pool_samples, num_classes=10, loader=pool_loader, device=self.device\n",
    "        )\n",
    "\n",
    "        # Evaluate BALD scores\n",
    "        candidate_batch = get_bald_batch(\n",
    "            log_probs_N_K_C,\n",
    "            batch_size=self.acquisition_size,\n",
    "            dtype=torch.double,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return candidate_batch\n",
    "\n",
    "    def get_batchbald_ical_candidate_batch(self, model, pool_model, pool_loader):\n",
    "        # Evaluate pool set\n",
    "        normal_log_probs_N_K_C = get_predictions(\n",
    "            model=model, num_samples=self.num_pool_samples, num_classes=10, loader=pool_loader, device=self.device\n",
    "        )\n",
    "\n",
    "        pool_log_probs_N_K_C = get_predictions(\n",
    "            model=pool_model, num_samples=self.num_pool_samples, num_classes=10, loader=pool_loader, device=self.device\n",
    "        )\n",
    "\n",
    "        # Evaluate BALD scores\n",
    "        candidate_batch = get_batchbaldical_batch(\n",
    "            normal_log_probs_N_K_C,\n",
    "            pool_log_probs_N_K_C,\n",
    "            batch_size=self.acquisition_size,\n",
    "            num_samples=100000,\n",
    "            dtype=torch.double,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return candidate_batch\n",
    "\n",
    "    def get_batchbald_candidate_batch(self, model, pool_loader):\n",
    "        # Evaluate pool set\n",
    "        log_probs_N_K_C = get_predictions(\n",
    "            model=model, num_samples=self.num_pool_samples, num_classes=10, loader=pool_loader, device=self.device\n",
    "        )\n",
    "\n",
    "        # Evaluate BALD scores\n",
    "        candidate_batch = get_batchbald_batch(\n",
    "            log_probs_N_K_C,\n",
    "            batch_size=self.acquisition_size,\n",
    "            num_samples=100000,\n",
    "            dtype=torch.double,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return candidate_batch\n",
    "\n",
    "    def train_pool_model(\n",
    "        self, *, model, train_pool_dataset, train_pool_loader, validation_loader, num_epochs, training_log\n",
    "    ):\n",
    "        log_probs_N_C = (\n",
    "            get_predictions(\n",
    "                model=model,\n",
    "                num_samples=self.num_eval_samples,\n",
    "                num_classes=10,\n",
    "                loader=train_pool_loader,\n",
    "                device=self.device,\n",
    "            )\n",
    "            .mean(dim=1)\n",
    "            .cpu()\n",
    "        )\n",
    "\n",
    "        train_pool_prediction_dataset = PredictionDataset(train_pool_dataset, log_probs_N_C)\n",
    "        train_pool_prediction_loader = torch.utils.data.DataLoader(\n",
    "            train_pool_prediction_dataset, batch_size=64, drop_last=True, shuffle=True\n",
    "        )\n",
    "\n",
    "        pool_model = self.new_model()\n",
    "        pool_optimizer = self.new_optimizer(pool_model)\n",
    "\n",
    "        loss = torch.nn.KLDivLoss(log_target=True, reduction=\"batchmean\")\n",
    "\n",
    "        train(\n",
    "            model=pool_model,\n",
    "            optimizer=pool_optimizer,\n",
    "            loss=loss,\n",
    "            validation_loss=torch.nn.NLLLoss(),\n",
    "            training_samples=self.num_training_samples,\n",
    "            validation_samples=self.num_eval_samples,\n",
    "            train_loader=train_pool_prediction_loader,\n",
    "            validation_loader=validation_loader,\n",
    "            patience=self.num_patience_epochs,\n",
    "            max_epochs=num_epochs,\n",
    "            device=self.device,\n",
    "            training_log=training_log,\n",
    "        )\n",
    "        # print(training_log)\n",
    "\n",
    "        return pool_model\n",
    "\n",
    "    def run(self, store):\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        # Active Learning setup\n",
    "        active_learning_data, validation_dataset, test_dataset = self.load_dataset()\n",
    "\n",
    "        # initial_training_set_indices = active_learning_data.get_random_pool_indices(self.initial_set_size)\n",
    "        # initial_training_set_indices = get_balanced_sample_indices(\n",
    "        #     active_learning_data.pool_dataset, 10, self.initial_set_size // 10\n",
    "        # )\n",
    "        initial_training_set_indices = mnist_initial_samples\n",
    "        active_learning_data.acquire(initial_training_set_indices)\n",
    "\n",
    "        store[\"initial_training_set_indices\"] = initial_training_set_indices\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            active_learning_data.training_dataset,\n",
    "            batch_size=64,\n",
    "            sampler=RandomFixedLengthSampler(active_learning_data.training_dataset, self.samples_per_epoch),\n",
    "            drop_last=True,\n",
    "        )\n",
    "        pool_loader = torch.utils.data.DataLoader(active_learning_data.pool_dataset, batch_size=64, drop_last=False)\n",
    "\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64, drop_last=False)\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, drop_last=False)\n",
    "\n",
    "        store[\"active_learning_steps\"] = []\n",
    "        active_learning_steps = store[\"active_learning_steps\"]\n",
    "\n",
    "        # Active Training Loop\n",
    "        while True:\n",
    "            training_set_size = len(active_learning_data.training_dataset)\n",
    "            print(f\"Training set size {training_set_size}:\")\n",
    "\n",
    "            # iteration_log = dict(training={}, pool_training={}, evalution_metrics=None, acquisition=None)\n",
    "            active_learning_steps.append({})\n",
    "            iteration_log = active_learning_steps[-1]\n",
    "\n",
    "            iteration_log[\"training\"] = {}\n",
    "\n",
    "            model = self.new_model()\n",
    "            optimizer = self.new_optimizer(model)\n",
    "            train(\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                training_samples=self.num_training_samples,\n",
    "                validation_samples=self.num_eval_samples,\n",
    "                train_loader=train_loader,\n",
    "                validation_loader=validation_loader,\n",
    "                patience=self.num_patience_epochs,\n",
    "                max_epochs=self.max_training_epochs,\n",
    "                device=self.device,\n",
    "                training_log=iteration_log[\"training\"],\n",
    "            )\n",
    "\n",
    "            evaluation_metrics = evaluate(\n",
    "                model=model, num_samples=self.num_eval_samples, loader=test_loader, device=self.device\n",
    "            )\n",
    "            iteration_log[\"evalution_metrics\"] = evaluation_metrics\n",
    "            print(f\"Perf after training {evaluation_metrics}\")\n",
    "\n",
    "            if training_set_size >= self.max_training_set:\n",
    "                print(\"Done.\")\n",
    "                break\n",
    "\n",
    "            if self.acquisition_function == AcquisitionFunction.batchbaldical:\n",
    "                train_pool_dataset = torch.utils.data.ConcatDataset(\n",
    "                    [active_learning_data.training_dataset, active_learning_data.pool_dataset]\n",
    "                )\n",
    "                train_pool_loader = torch.utils.data.DataLoader(train_pool_dataset, batch_size=64, drop_last=False)\n",
    "\n",
    "                num_epochs = iteration_log[\"training\"][\"best_epoch\"]\n",
    "\n",
    "                iteration_log[\"pool_training\"] = {}\n",
    "                pool_model = self.train_pool_model(\n",
    "                    model=model,\n",
    "                    train_pool_dataset=train_pool_dataset,\n",
    "                    train_pool_loader=train_pool_loader,\n",
    "                    validation_loader=validation_loader,\n",
    "                    num_epochs=num_epochs,\n",
    "                    training_log=iteration_log[\"pool_training\"],\n",
    "                )\n",
    "\n",
    "                candidate_batch = self.get_batchbald_ical_candidate_batch(model, pool_model, pool_loader)\n",
    "            elif self.acquisition_function == AcquisitionFunction.bald:\n",
    "                candidate_batch = self.get_bald_candidate_batch(model, pool_loader)\n",
    "            elif self.acquisition_function == AcquisitionFunction.batchbald:\n",
    "                candidate_batch = self.get_batchbald_candidate_batch(model, pool_loader)\n",
    "            elif self.acquisition_function == AcquisitionFunction.thompsonbald:\n",
    "                candidate_batch = self.get_thompson_bald_candidate_batch(model, pool_loader)\n",
    "            elif self.acquisition_function == AcquisitionFunction.randombald:\n",
    "                candidate_batch = self.get_random_bald_candidate_batch(model, pool_loader)\n",
    "            elif self.acquisition_function == AcquisitionFunction.random:\n",
    "                candidate_batch = self.get_random_candidate_batch(len(active_learning_data.pool_dataset))\n",
    "            else:\n",
    "                raise f\"Unknown acquisition function {self.acquisition_function}!\"\n",
    "\n",
    "            candidate_global_indices = get_base_indices(active_learning_data.pool_dataset, candidate_batch.indices)\n",
    "            candidate_labels = [active_learning_data.dataset[index][1].item() for index in candidate_global_indices]\n",
    "\n",
    "            iteration_log[\"acquisition\"] = dict(\n",
    "                indices=candidate_global_indices, labels=candidate_labels, scores=candidate_batch.scores\n",
    "            )\n",
    "\n",
    "            active_learning_data.acquire(candidate_batch.indices)\n",
    "\n",
    "            ls = \", \".join(f\"{label} ({score:.4})\" for label, score in zip(candidate_labels, candidate_batch.scores))\n",
    "            print(f\"Acquiring (label, score)s: {ls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "validate pool_model on the pool set only! (and not on train and pool together!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 20:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4100b2bb66402eaa19690f105b6072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=384.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=64.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: -6.660010442137718)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=157.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perf after training {'accuracy': 0.5369, 'crossentropy': 6.5704283523559575}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='get_predictions'), FloatProgress(value=0.0, max=463616.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c242920ead49a38ad1ecbb3d4dbb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1811.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=64.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RestoringEarlyStopping: Restoring best parameters. (Score: -4.959371902048588)\n",
      "RestoringEarlyStopping: Restoring optimizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='get_predictions'), FloatProgress(value=0.0, max=2317680.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='get_predictions'), FloatProgress(value=0.0, max=2317680.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Conditional Entropy'), FloatProgress(value=0.0, max=115884.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Conditional Entropy'), FloatProgress(value=0.0, max=115884.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95215cdd48604542999b30cf4d787c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='BatchBALD'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='ExactJointEntropy.compute_batch'), FloatProgress(value=0.0, max=115884.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='ExactJointEntropy.compute_batch'), FloatProgress(value=0.0, max=115884.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='ExactJointEntropy.compute_batch'), FloatProgress(value=0.0, max=115884.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='ExactJointEntropy.compute_batch'), FloatProgress(value=0.0, max=115884.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab78a7cc668437190977687c7635d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='ExactJointEntropy.compute_batch'), FloatProgress(value=0.0, max=115884.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c28b71b6094e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c28370483a3f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 )\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mcandidate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batchbald_ical_candidate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquisition_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mAcquisitionFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbald\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0mcandidate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bald_candidate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c28370483a3f>\u001b[0m in \u001b[0;36mget_batchbald_ical_candidate_batch\u001b[0;34m(self, model, pool_model, pool_loader)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Evaluate BALD scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         candidate_batch = get_batchbaldical_batch(\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mnormal_log_probs_N_K_C\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mpool_log_probs_N_K_C\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/batchbald.py\u001b[0m in \u001b[0;36mget_batchbaldical_batch\u001b[0;34m(training_log_probs_N_K_C, pool_log_probs_N_K_C, batch_size, num_samples, dtype, device)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mpool_batchbald_scorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_to_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mtraining_batchbald_scorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_scores_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mpool_batchbald_scorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_scores_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/batchbald.py\u001b[0m in \u001b[0;36mcompute_scores\u001b[0;34m(self, out_scores_N)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_scores_N\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_joint_entropy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_probs_N_K_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_entropies_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_scores_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mout_scores_N\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditional_entropies_N\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_conditional_entropies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/joint_entropy.py\u001b[0m in \u001b[0;36mcompute_batch\u001b[0;34m(self, log_probs_B_K_C, output_entropies_B)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs_B_K_C\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_entropies_B\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;34m\"\"\"Computes the joint entropy of the added variables together with the batch (one by one).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs_B_K_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_entropies_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/bald-ical/batchbald_redux/joint_entropy.py\u001b[0m in \u001b[0;36mcompute_batch\u001b[0;34m(self, log_probs_B_K_C, output_entropies_B)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mtoma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs_B_K_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mchunked_joint_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_log_probs_b_K_C\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py\u001b[0m in \u001b[0;36mexecute_chunked\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mexecute_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                 return explicit.chunked(\n\u001b[0m\u001b[1;32m    202\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py\u001b[0m in \u001b[0;36mchunked\u001b[0;34m(func, tensor, initial_step, toma_dimension, toma_context, toma_cache_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoma_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         explicit.range(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/__init__.py\u001b[0m in \u001b[0;36mrange\u001b[0;34m(func, start, end, initial_step, toma_context, toma_cache_type, *args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mcurrent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 \u001b[0mgc_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshould_reduce_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/active_learning/lib/python3.8/site-packages/toma/torch_cuda_memory.py\u001b[0m in \u001b[0;36mgc_cuda\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgc_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"\"\"Gargage collect Torch (CUDA) memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "experiment = Experiment(\n",
    "    max_training_epochs=1, max_training_set=25, acquisition_function=AcquisitionFunction.batchbaldical\n",
    ")\n",
    "\n",
    "results = {}\n",
    "experiment.run(results)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<AcquisitionFunction.batchbald: 'BatchBALD'>,\n",
       " <AcquisitionFunction.batchbaldical: 'BatchBALD-ICAL'>,\n",
       " <AcquisitionFunction.random: 'Random'>,\n",
       " <AcquisitionFunction.randombald: 'RandomBALD'>,\n",
       " <AcquisitionFunction.thompsonbald: 'ThompsonBALD'>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(AcquisitionFunction) - {AcquisitionFunction.bald}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b53445d8f32b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedded_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# exports\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configs = (\n",
    "        [\n",
    "            Experiment(\n",
    "                seed=seed+120,\n",
    "                acquisition_function=acquisition_function,\n",
    "                acquisition_size=acquisition_size,\n",
    "                num_pool_samples=100,\n",
    "            )\n",
    "            for seed in range(5)\n",
    "            for acquisition_size in [5, 10]\n",
    "            for acquisition_function in [AcquisitionFunction.batchbald, AcquisitionFunction.batchbaldical]\n",
    "        ]\n",
    "        + [\n",
    "            Experiment(\n",
    "                seed=seed+120,\n",
    "                acquisition_function=acquisition_function,\n",
    "                acquisition_size=acquisition_size,\n",
    "                num_pool_samples=max(20, acquisition_size),\n",
    "            )\n",
    "            for seed in range(5)\n",
    "            for acquisition_size in [5, 10, 20, 50]\n",
    "            for acquisition_function in [\n",
    "                AcquisitionFunction.bald,\n",
    "                AcquisitionFunction.thompsonbald,\n",
    "                AcquisitionFunction.randombald,\n",
    "            ]\n",
    "        ]\n",
    "        + [\n",
    "            Experiment(\n",
    "                seed=seed+120,\n",
    "                acquisition_function=AcquisitionFunction.random,\n",
    "                acquisition_size=5,\n",
    "                num_pool_samples=20,\n",
    "            )\n",
    "            for seed in range(40)\n",
    "            for acquisition_size in [5]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for job_id, store in embedded_experiments(__file__, len(configs)):\n",
    "        config = configs[job_id]\n",
    "        config.seed += job_id\n",
    "        print(config)\n",
    "        store[\"config\"] = dataclasses.asdict(config)\n",
    "        store[\"log\"] = {}\n",
    "\n",
    "        try:\n",
    "            config.run(store=store)\n",
    "        except Exception:\n",
    "            store[\"exception\"] = traceback.format_exc()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:active_learning]",
   "language": "python",
   "name": "conda-env-active_learning-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
