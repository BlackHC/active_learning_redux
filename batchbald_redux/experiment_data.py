# AUTOGENERATED! DO NOT EDIT! File to edit: 08b_experiment_data.ipynb (unless otherwise specified).

__all__ = ['ExperimentData', 'ExperimentDataConfig', 'OoDDatasetConfig', 'StandardExperimentDataConfig',
           'load_standard_experiment_data', 'ImbalancedTestDistributionExperimentDataConfig',
           'load_imbalanced_experiment_data', 'OODClassesDistributionExperimentDataConfig',
           'load_ood_classes_experiment_data', 'CinicCifarShiftExperimentDataConfig',
           'load_cinic_cifar_shift_experiment_data']

# Cell

from dataclasses import dataclass
from typing import List, Optional, Set

import numpy as np
import torch
import torch.utils.data
from torch import nn
from torch.utils.data import Dataset

from .active_learning import ActiveLearningData
from .dataset_challenges import (
    AdditiveGaussianNoise,
    AliasDataset,
    NamedDataset,
    get_balanced_sample_indices,
    get_balanced_sample_indices_by_class,
    get_class_indices,
    get_class_indices_by_class,
    get_targets,
)
from .datasets import get_dataset

# Cell


@dataclass
class ExperimentData:
    active_learning: ActiveLearningData
    validation_dataset: Dataset
    evaluation_dataset: Dataset
    test_dataset: Dataset

    train_augmentations: nn.Module

    initial_training_set_indices: List[int]
    evaluation_set_indices: List[int]

    ood_dataset: Optional[NamedDataset]

    # TODO: replace this with dataset info on the targets
    ood_exposure: bool

    device: str


class ExperimentDataConfig:
    def load(self, device) -> ExperimentData:
        raise NotImplementedError()

# Cell


@dataclass
class OoDDatasetConfig:
    ood_dataset_name: str
    ood_repetitions: float
    ood_exposure: bool


@dataclass
class StandardExperimentDataConfig(ExperimentDataConfig):
    id_dataset_name: str
    id_repetitions: float

    initial_training_set_size: int

    validation_set_size: int
    validation_split_random_state: int

    evaluation_set_size: int

    add_dataset_noise: bool

    ood_dataset_config: Optional[OoDDatasetConfig]

    def load(self, device) -> ExperimentData:
        return load_standard_experiment_data(
            id_dataset_name=self.id_dataset_name,
            id_repetitions=self.id_repetitions,
            initial_training_set_size=self.initial_training_set_size,
            validation_set_size=self.validation_set_size,
            validation_split_random_state=self.validation_split_random_state,
            evaluation_set_size=self.evaluation_set_size,
            add_dataset_noise=self.add_dataset_noise,
            ood_dataset_config=self.ood_dataset_config,
            device=device,
        )


def load_standard_experiment_data(
    *,
    id_dataset_name: str,
    id_repetitions: float,
    initial_training_set_size: int,
    validation_set_size: int,
    validation_split_random_state: int,
    evaluation_set_size: int,
    add_dataset_noise: bool,
    ood_dataset_config: Optional[OoDDatasetConfig],
    device: str,
) -> ExperimentData:
    split_dataset = get_dataset(
        id_dataset_name,
        root="data",
        validation_set_size=validation_set_size,
        validation_split_random_state=validation_split_random_state,
        normalize_like_cifar10=True,
        device_hint=device,
    )

    train_dataset = split_dataset.train

    # TODO: add hook here to further process the train dataset?

    # If we reduce the train set, we need to do so before picking the initial train set.
    if id_repetitions < 1:
        train_dataset = train_dataset * id_repetitions

    targets = train_dataset.get_targets()
    num_classes = train_dataset.get_num_classes()
    initial_samples_per_class = initial_training_set_size // num_classes
    evaluation_set_samples_per_class = evaluation_set_size // num_classes
    samples_per_class = initial_samples_per_class + evaluation_set_samples_per_class

    balanced_samples_indices = get_balanced_sample_indices_by_class(
        targets=targets,
        num_classes=num_classes,
        samples_per_class=samples_per_class,
        seed=validation_split_random_state,
    )

    initial_training_set_indices = [
        idx for by_class in balanced_samples_indices.values() for idx in by_class[:initial_samples_per_class]
    ]
    evaluation_set_indices = [
        idx for by_class in balanced_samples_indices.values() for idx in by_class[initial_samples_per_class:]
    ]

    # If we over-sample the train set, we do so after picking the initial train set to avoid duplicates
    # (duplicates within the initial train set).
    if id_repetitions > 1:
        train_dataset = train_dataset * id_repetitions

    if ood_dataset_config:
        ood_exposure = ood_dataset_config.ood_exposure
        odd_split_dataset = get_dataset(
            ood_dataset_config.ood_dataset_name, root="data", normalize_like_cifar10=True, device_hint=device
        )
        assert split_dataset.device == odd_split_dataset.device, (
            f"ID dataset resides on {split_dataset.device}, while OOD dataset is on {odd_split_dataset.device};"
            'try to put both on "cpu"!'
        )
        original_ood_dataset = odd_split_dataset.train
        if ood_exposure:
            train_dataset = train_dataset.one_hot(device=split_dataset.device)
            ood_dataset = original_ood_dataset.uniform_target(
                device=split_dataset.device, num_classes=train_dataset.get_num_classes()
            )
        else:
            ood_dataset = original_ood_dataset.constant_target(
                target=torch.tensor(-1, device=split_dataset.device), num_classes=train_dataset.get_num_classes()
            )

        if ood_dataset_config.ood_repetitions != 1:
            ood_dataset = ood_dataset * ood_dataset_config.ood_repetitions

        train_dataset = train_dataset + ood_dataset
    else:
        original_ood_dataset = None
        ood_exposure = False

    if add_dataset_noise:
        train_dataset = AdditiveGaussianNoise(train_dataset, 0.1)
    else:
        if id_repetitions > 1 or (ood_dataset_config is not None and ood_dataset_config.ood_repetitions) > 1:
            raise RuntimeError("`add_dataset_noise`==False, even though repeated id or ood data!")

    active_learning_data = ActiveLearningData(train_dataset)

    active_learning_data.acquire_base_indices(initial_training_set_indices)

    evaluation_dataset = AliasDataset(
        active_learning_data.extract_dataset_from_base_indices(evaluation_set_indices),
        f"Evaluation Set ({len(evaluation_set_indices)} samples)",
    )

    return ExperimentData(
        active_learning=active_learning_data,
        validation_dataset=split_dataset.validation,
        test_dataset=split_dataset.test,
        evaluation_dataset=evaluation_dataset,
        train_augmentations=split_dataset.train_augmentations,
        initial_training_set_indices=initial_training_set_indices,
        evaluation_set_indices=evaluation_set_indices,
        ood_dataset=original_ood_dataset,
        ood_exposure=ood_exposure,
        device=split_dataset.device,
    )

# Cell


@dataclass
class ImbalancedTestDistributionExperimentDataConfig(ExperimentDataConfig):
    """Make the test set and evaluation set imbalanced"""

    dataset_name: str
    repetitions: float

    initial_training_set_size: int

    validation_set_size: int
    validation_split_random_state: int

    evaluation_set_size: int

    add_dataset_noise: bool

    minority_classes: Set[int]
    minority_class_percentage: float

    def load(self, device) -> ExperimentData:
        return load_imbalanced_experiment_data(
            dataset_name=self.dataset_name,
            repetitions=self.repetitions,
            initial_training_set_size=self.initial_training_set_size,
            validation_set_size=self.validation_set_size,
            validation_split_random_state=self.validation_split_random_state,
            evaluation_set_size=self.evaluation_set_size,
            add_dataset_noise=self.add_dataset_noise,
            minority_classes=self.minority_classes,
            minority_class_percentage=self.minority_class_percentage,
            device=device,
        )


def load_imbalanced_experiment_data(
    *,
    dataset_name: str,
    repetitions: float,
    initial_training_set_size: int,
    validation_set_size: int,
    validation_split_random_state: int,
    evaluation_set_size: int,
    add_dataset_noise: bool,
    minority_classes: Set[int],
    minority_class_percentage: float,
    device: str,
) -> ExperimentData:
    split_dataset = get_dataset(
        dataset_name,
        root="data",
        validation_set_size=validation_set_size,
        validation_split_random_state=validation_split_random_state,
        normalize_like_cifar10=True,
        device_hint=device,
    )

    train_dataset = split_dataset.train

    # If we reduce the train set, we need to do so before picking the initial train set.
    if repetitions < 1:
        train_dataset = train_dataset * repetitions

    num_classes = train_dataset.get_num_classes()
    # Keep the initial training set balanced at least.
    initial_samples_per_class = initial_training_set_size // num_classes
    weighted_num_classes = num_classes - len(minority_classes) * (1 - minority_class_percentage / 100)

    evaluation_set_samples_per_class = int(evaluation_set_size / weighted_num_classes)
    evaluation_set_class_counts = [
        int(evaluation_set_samples_per_class * minority_class_percentage / 100)
        if i in minority_classes
        else evaluation_set_samples_per_class
        for i in range(num_classes)
    ]

    print("Evaluation Set Class Counts:", evaluation_set_class_counts)

    generator = np.random.default_rng(validation_split_random_state)
    class_indices_by_class = get_class_indices_by_class(
        train_dataset.get_targets(),
        class_counts=[
            initial_samples_per_class + evaluation_set_class_count
            for evaluation_set_class_count in evaluation_set_class_counts
        ],
        generator=generator,
    )

    initial_training_set_indices = [
        idx for by_class in class_indices_by_class.values() for idx in by_class[:initial_samples_per_class]
    ]
    evaluation_set_indices = [
        idx for by_class in class_indices_by_class.values() for idx in by_class[initial_samples_per_class:]
    ]

    # If we over-sample the train set, we do so after picking the initial train set to avoid duplicates.
    if repetitions > 1:
        train_dataset = train_dataset * repetitions

    if add_dataset_noise:
        train_dataset = AdditiveGaussianNoise(train_dataset, 0.1)
    else:
        if repetitions > 1:
            raise RuntimeError("`add_dataset_noise`==False, even though repeated id!")

    active_learning_data = ActiveLearningData(train_dataset)

    active_learning_data.acquire_base_indices(initial_training_set_indices)

    evaluation_dataset = AliasDataset(
        active_learning_data.extract_dataset_from_base_indices(evaluation_set_indices),
        f"Evaluation Set ({len(evaluation_set_indices)} samples)",
    )

    test_dataset = split_dataset.test.imbalance_subsample(
        minority_classes=minority_classes,
        minority_percentage=minority_class_percentage,
        seed=validation_split_random_state,
    )
    validation_dataset = split_dataset.validation.imbalance_subsample(
        minority_classes=minority_classes,
        minority_percentage=minority_class_percentage,
        seed=validation_split_random_state,
    )

    return ExperimentData(
        active_learning=active_learning_data,
        validation_dataset=validation_dataset,
        test_dataset=test_dataset,
        evaluation_dataset=evaluation_dataset,
        train_augmentations=split_dataset.train_augmentations,
        initial_training_set_indices=initial_training_set_indices,
        evaluation_set_indices=evaluation_set_indices,
        ood_dataset=None,
        ood_exposure=False,
        device=split_dataset.device,
    )

# Cell


@dataclass
class OODClassesDistributionExperimentDataConfig(ExperimentDataConfig):
    """Make the test set and evaluation set imbalanced"""

    dataset_name: str
    repetitions: float

    initial_training_set_size: int

    validation_set_size: int
    validation_split_random_state: int

    evaluation_set_size: int

    add_dataset_noise: bool

    ood_classes: Set[int]
    ood_repetitions: float
    ood_exposure: bool

    def load(self, device) -> ExperimentData:
        return load_ood_classes_experiment_data(
            dataset_name=self.dataset_name,
            repetitions=self.repetitions,
            initial_training_set_size=self.initial_training_set_size,
            validation_set_size=self.validation_set_size,
            validation_split_random_state=self.validation_split_random_state,
            evaluation_set_size=self.evaluation_set_size,
            add_dataset_noise=self.add_dataset_noise,
            ood_classes=self.ood_classes,
            ood_exposure=self.ood_exposure,
            ood_repetitions=self.ood_repetitions,
            device=device,
        )


def load_ood_classes_experiment_data(
    *,
    dataset_name: str,
    repetitions: float,
    initial_training_set_size: int,
    validation_set_size: int,
    validation_split_random_state: int,
    evaluation_set_size: int,
    add_dataset_noise: bool,
    ood_classes: Set[int],
    ood_exposure: bool,
    ood_repetitions: float,
    device: str,
) -> ExperimentData:
    split_dataset = get_dataset(
        dataset_name,
        root="data",
        validation_set_size=validation_set_size,
        validation_split_random_state=validation_split_random_state,
        normalize_like_cifar10=True,
        device_hint=device,
    )

    # Split off OOD dataset
    train_targets = get_targets(split_dataset.train)
    id_indices = [index for index, target in enumerate(train_targets) if int(target) not in ood_classes]
    ood_indices = [index for index, target in enumerate(train_targets) if int(target) in ood_classes]

    original_ood_dataset = NamedDataset(
        split_dataset.train.subset(ood_indices), f"{split_dataset.train}[target in {ood_classes}]"
    )
    id_dataset = AliasDataset(
        split_dataset.train.subset(id_indices), f"{split_dataset.train}[target not in {ood_classes}]"
    )

    # If we reduce the train set, we need to do so before picking the initial train set.
    if repetitions < 1:
        id_dataset = id_dataset * repetitions

    num_classes = split_dataset.train.get_num_classes()
    num_id_classes = num_classes - len(ood_classes)
    assert num_id_classes > 0

    # Keep the initial training set balanced at least.
    initial_samples_per_class = initial_training_set_size // num_id_classes
    evaluation_set_samples_per_class = evaluation_set_size // num_id_classes

    class_counts = [
        0 if i in ood_classes else initial_samples_per_class + evaluation_set_samples_per_class
        for i in range(num_classes)
    ]

    print("Initial Samples + Evaluation Set Class Counts:", class_counts)

    generator = np.random.default_rng(validation_split_random_state)
    class_indices_by_class = get_class_indices_by_class(get_targets(id_dataset), class_counts=class_counts, generator=generator)

    initial_training_set_indices = [
        idx for by_class in class_indices_by_class.values() for idx in by_class[:initial_samples_per_class]
    ]
    evaluation_set_indices = [
        idx for by_class in class_indices_by_class.values() for idx in by_class[initial_samples_per_class:]
    ]

    # If we over-sample the train set, we do so after picking the initial train set to avoid duplicates.
    if repetitions > 1:
        id_dataset = id_dataset * repetitions

    if ood_exposure:
        id_dataset = id_dataset.one_hot(device=split_dataset.device)
        ood_dataset = original_ood_dataset.uniform_target(device=split_dataset.device, num_classes=num_classes)
    else:
        ood_dataset = original_ood_dataset.constant_target(
            target=torch.tensor(-1, device=split_dataset.device), num_classes=num_classes
        )

    if ood_repetitions != 1:
        ood_dataset = ood_dataset * ood_repetitions

    train_dataset = id_dataset + ood_dataset

    if add_dataset_noise:
        train_dataset = AdditiveGaussianNoise(train_dataset, 0.1)
    else:
        if repetitions > 1:
            raise RuntimeError("`add_dataset_noise`==False, even though repeated id!")

    active_learning_data = ActiveLearningData(train_dataset)

    active_learning_data.acquire_base_indices(initial_training_set_indices)

    evaluation_dataset = AliasDataset(
        active_learning_data.extract_dataset_from_base_indices(evaluation_set_indices),
        f"Evaluation Set ({len(evaluation_set_indices)} samples)",
    )

    test_dataset = split_dataset.test.imbalance_subsample(
        minority_classes=ood_classes, minority_percentage=0, seed=validation_split_random_state
    )
    validation_dataset = split_dataset.validation.imbalance_subsample(
        minority_classes=ood_classes, minority_percentage=0, seed=validation_split_random_state
    )

    return ExperimentData(
        active_learning=active_learning_data,
        validation_dataset=validation_dataset,
        test_dataset=test_dataset,
        evaluation_dataset=evaluation_dataset,
        train_augmentations=split_dataset.train_augmentations,
        initial_training_set_indices=initial_training_set_indices,
        evaluation_set_indices=evaluation_set_indices,
        ood_dataset=original_ood_dataset,
        ood_exposure=ood_exposure,
        device=split_dataset.device,
    )

# Cell


@dataclass
class CinicCifarShiftExperimentDataConfig(ExperimentDataConfig):
    """CINIC-10 as train set, CIFAR-10 as test/eval set."""

    train_imagenet_only: bool

    initial_training_set_size: int

    validation_set_size: int
    validation_split_random_state: int

    evaluation_set_size: int

    def load(self, device) -> ExperimentData:
        return load_cinic_cifar_shift_experiment_data(
            train_imagenet_only=self.train_imagenet_only,
            initial_training_set_size=self.initial_training_set_size,
            validation_set_size=self.validation_set_size,
            validation_split_random_state=self.validation_split_random_state,
            evaluation_set_size=self.evaluation_set_size,
            device=device,
        )


def load_cinic_cifar_shift_experiment_data(
    *,
    train_imagenet_only: bool,
    initial_training_set_size: int,
    validation_set_size: int,
    validation_split_random_state: int,
    evaluation_set_size: int,
    device: str,
) -> ExperimentData:
    split_imagenet_cinic10_dataset = get_dataset(
        name="IMAGENET-CINIC-10",
        root="data",
        validation_set_size=0,
        validation_split_random_state=validation_split_random_state,
        normalize_like_cifar10=True,
        device_hint=device,
    )

    split_cifar10_dataset = get_dataset(
        name="CIFAR-10",
        root="data",
        validation_set_size=validation_set_size,
        validation_split_random_state=validation_split_random_state,
        normalize_like_cifar10=True,
        device_hint=device,
    )

    assert split_imagenet_cinic10_dataset.device == split_cifar10_dataset.device

    train_dataset = split_imagenet_cinic10_dataset.train
    validation_dataset = split_cifar10_dataset.validation
    test_dataset = split_cifar10_dataset.test

    num_classes = split_cifar10_dataset.train.get_num_classes()
    evaluation_set_samples_per_class = evaluation_set_size // num_classes
    balanced_evaluation_indices = get_balanced_sample_indices(
        targets=split_cifar10_dataset.train.get_targets(),
        num_classes=num_classes,
        samples_per_class=evaluation_set_samples_per_class,
        seed=validation_split_random_state,
    )

    evaluation_dataset, cifar10_train_dataset = split_cifar10_dataset.train.split(balanced_evaluation_indices)

    # If we add CIFAR-10 back, exclude the evaluation samples.
    if not train_imagenet_only:
        train_dataset += cifar10_train_dataset

    initial_samples_per_class = initial_training_set_size // num_classes
    balanced_initial_indices = get_balanced_sample_indices(
        targets=train_dataset.get_targets(),
        num_classes=num_classes,
        samples_per_class=initial_samples_per_class,
        seed=validation_split_random_state,
    )

    active_learning_data = ActiveLearningData(train_dataset)
    active_learning_data.acquire_base_indices(balanced_initial_indices)

    return ExperimentData(
        active_learning=active_learning_data,
        validation_dataset=validation_dataset,
        test_dataset=test_dataset,
        evaluation_dataset=evaluation_dataset,
        train_augmentations=split_imagenet_cinic10_dataset.train_augmentations,
        initial_training_set_indices=balanced_initial_indices,
        evaluation_set_indices=balanced_evaluation_indices,
        ood_dataset=None,
        ood_exposure=False,
        device=split_imagenet_cinic10_dataset.device,
    )