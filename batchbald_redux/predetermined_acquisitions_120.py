# AUTOGENERATED! DO NOT EDIT! File to edit: U_predetermined_acquisitions_120.ipynb (unless otherwise specified).

__all__ = ['additional_initial_acquisitions', 'predetermind_acquisition_base_indices', 'ActiveLearner',
           'UnifiedExperiment', 'experiment']

# Cell

import dataclasses
import traceback
from dataclasses import dataclass
from typing import Optional, Type, Union

import torch
import torch.utils.data
from blackhc.project import is_run_from_ipython
from blackhc.project.experiment import embedded_experiment

from batchbald_redux import acquisition_functions, baseline_acquisition_functions
from .acquisition_functions import (
    CandidateBatchComputer,
    EvalDatasetBatchComputer,
    EvalModelBatchComputer,
)
from .batchbald import compute_entropy, get_bald_scores
from .black_box_model_training import evaluate
from .dataset_challenges import get_base_dataset_index, get_target
from .di import DependencyInjection
from .experiment_data import (
    ExperimentData,
    ExperimentDataConfig,
    OoDDatasetConfig,
    StandardExperimentDataConfig,
)
from .models import MnistModelTrainer
from .resnet_models import Cifar10ModelTrainer
from .train_eval_model import (
    TrainEvalModel,
    TrainSelfDistillationEvalModel,
)
from .trained_model import BayesianEnsembleModelTrainer, ModelTrainer

# Cell

additional_initial_acquisitions = [
    26919,
    43627,
    1666,
    354,
    23669,
    48412,
    48486,
    18284,
    51745,
    8120,
    41099,
    11397,
    17942,
    38275,
    9674,
    7069,
    2810,
    35239,
    1279,
    11383,
    2271,
    921,
    15619,
    32386,
    17830,
    1385,
    20850,
    16780,
    15765,
    6786,
    18938,
    46468,
    54880,
    14885,
    15543,
    13091,
    39530,
    9241,
    21243,
    48253,
    42363,
    31951,
    6689,
    20219,
    17178,
    26621,
    27534,
    3889,
    48169,
    38735,
    31664,
    20215,
    554,
    23576,
    46590,
    16769,
    11169,
    27305,
    10657,
    13150,
    935,
    50655,
    55379,
    36245,
    42634,
    34842,
    1785,
    52851,
    47218,
    29309,
    50342,
    20286,
    33050,
    35791,
    50338,
    42327,
    48649,
    49161,
    18021,
    37139,
    15876,
    21433,
    47625,
    29242,
    7308,
    29771,
    30687,
    9840,
    5501,
    26821,
    10240,
    54942,
    15126,
    2032,
    26545,
    31021,
    50273,
    45507,
    41945,
    45728,
]

predetermind_acquisition_base_indices = [
    39492,
    22860,
    8309,
    1584,
    30947,
    27798,
    49452,
    48971,
    54905,
    2742,
    21660,
    32371,
    4311,
    20634,
    50230,
    1885,
    4079,
    19361,
    2886,
    48405,
    17698,
    30394,
    805,
    49092,
    25854,
    30736,
    871,
    48347,
    51650,
    27713,
    11253,
    21777,
    19195,
    11800,
    22268,
    27723,
    43088,
    53816,
    2086,
    16125,
    16620,
    21942,
    22726,
    14641,
    48998,
    20892,
    53574,
    38467,
    42071,
    44011,
    34860,
    15146,
    12674,
    2990,
    24740,
    2760,
    32952,
    26176,
    6547,
    41979,
    52877,
    37500,
    1251,
    35731,
    54035,
    24622,
    50138,
    19706,
    35440,
    54586,
    10641,
    41349,
    43654,
    47639,
    50196,
    39898,
    18035,
    38260,
    12219,
    23881,
    16296,
    55545,
    26873,
    43739,
    21868,
    33908,
    21086,
    15485,
    3767,
    31372,
    53402,
    3729,
    28189,
    20061,
    34193,
    10498,
    23905,
    6358,
    14781,
    34520,
    37468,
    29386,
    13444,
    12408,
    37104,
    4590,
    5323,
    577,
    53472,
    21362,
    23853,
    28510,
    54071,
    4843,
    48017,
    44900,
    35034,
    51362,
    11031,
    9873,
    29603,
    20111,
    5094,
    52435,
    29050,
    26298,
    1284,
    10490,
    32036,
    14807,
    19792,
    20534,
    21430,
    49163,
    51884,
    55396,
    37561,
    31063,
    47366,
    44381,
    11263,
    50555,
    25533,
    18661,
    22764,
    30708,
    7645,
    1624,
    29680,
    21821,
    31566,
    22029,
    20734,
    1983,
    48719,
    26089,
    33084,
    17852,
    19936,
    21637,
    33360,
    9793,
    39691,
    25514,
    22504,
    30414,
    55095,
    42780,
    12486,
    46210,
    53877,
    27138,
    17841,
    49253,
    44947,
    9044,
    43754,
    35652,
    218,
    11413,
    9457,
    37284,
    12929,
    1439,
    40966,
    24559,
    53588,
    26152,
    26194,
    23020,
    27978,
    41692,
    24677,
    6440,
    37637,
    37251,
    14031,
    2054,
    1271,
    4901,
    48304,
    52315,
    483,
    33990,
    42188,
    49437,
    52413,
    13903,
    27831,
    4441,
    18310,
    53548,
    41017,
    52106,
    12777,
    14309,
    27089,
    26053,
    12028,
    54900,
    5075,
    24127,
    25301,
    51541,
    45006,
    38036,
    6800,
    39982,
    15891,
    51050,
    22334,
    47286,
    42210,
    8842,
    10727,
    34357,
    13116,
    14069,
    31558,
    13205,
    35513,
    26167,
    41741,
    40856,
    229,
    43583,
    31311,
    4571,
    45969,
    45102,
    10675,
    38303,
    43028,
    28001,
    50360,
    6716,
    4051,
    11285,
    36242,
    26289,
    35151,
    21680,
    15880,
    2191,
    37175,
    16911,
    53487,
    31381,
    32042,
    20243,
    18767,
    45299,
    51128,
    49286,
    15074,
    16560,
    2652,
    17181,
    31671,
    6087,
    53992,
    14076,
    18204,
    19160,
    48812,
    691,
    41131,
    55873,
    45176,
    7524,
    12986,
    32575,
    47759,
    6899,
    45044,
    19981,
    27515,
    3861,
    42400,
    53742,
]

# Cell


@dataclass
class ActiveLearner:
    acquisition_size: int

    num_validation_samples: int
    num_pool_samples: int

    train_eval_model: TrainEvalModel
    model_trainer: ModelTrainer
    data: ExperimentData

    disable_training_augmentations: bool

    device: Optional

    def __call__(self, log):
        log["seed"] = torch.seed()

        # Active Learning setup
        data = self.data

        train_augmentations = data.train_augmentations if not self.disable_training_augmentations else None

        model_trainer = self.model_trainer
        train_eval_model = self.train_eval_model

        train_loader = model_trainer.get_train_dataloader(data.active_learning.training_dataset)
        pool_loader = model_trainer.get_evaluation_dataloader(data.active_learning.pool_dataset)
        validation_loader = model_trainer.get_evaluation_dataloader(data.validation_dataset)
        test_loader = model_trainer.get_evaluation_dataloader(data.test_dataset)

        log["active_learning_steps"] = []
        active_learning_steps = log["active_learning_steps"]

        data.active_learning.acquire_base_indices(additional_initial_acquisitions)

        # Active Training Loop
        for base_index in predetermind_acquisition_base_indices:
            training_set_size = len(data.active_learning.training_dataset)
            print(f"Training set size {training_set_size}:")

            # iteration_log = dict(training={}, pool_training={}, evaluation_metrics=None, acquisition=None)
            active_learning_steps.append({})
            iteration_log = active_learning_steps[-1]

            iteration_log["training"] = {}

            # TODO: this is a hack! :(
            if data.ood_dataset is None:
                loss = validation_loss = torch.nn.NLLLoss()
            elif data.ood_exposure:
                loss = torch.nn.KLDivLoss(log_target=False, reduction="batchmean")
                validation_loss = torch.nn.NLLLoss()
            else:
                loss = validation_loss = torch.nn.NLLLoss()

            trained_model = model_trainer.get_trained(
                train_loader=train_loader,
                train_augmentations=train_augmentations,
                validation_loader=validation_loader,
                log=iteration_log["training"],
                loss=loss,
                validation_loss=validation_loss,
            )

            evaluation_metrics = evaluate(
                model=trained_model,
                num_samples=self.num_validation_samples,
                loader=test_loader,
                device=self.device,
                storage_device="cpu",
            )
            iteration_log["evaluation_metrics"] = evaluation_metrics
            print(f"Perf after training {evaluation_metrics}")

            iteration_log["acquisition"] = dict(indices=[base_index])
            acquired_label = get_target(data.active_learning.base_dataset, base_index)

            data.active_learning.acquire_base_indices([base_index])

            print(f"Acquiring base index {base_index} {acquired_label}")


@dataclass
class UnifiedExperiment:
    seed: int

    experiment_data_config: ExperimentDataConfig

    acquisition_size: int = 5

    max_training_epochs: int = 300

    num_pool_samples: int = 100
    num_validation_samples: int = 20
    num_training_samples: int = 1

    device: str = "cuda"
    acquisition_function: Union[Type[CandidateBatchComputer], Type[EvalModelBatchComputer]] = acquisition_functions.BALD
    train_eval_model: Type[TrainEvalModel] = TrainSelfDistillationEvalModel
    model_trainer_factory: Type[ModelTrainer] = Cifar10ModelTrainer
    ensemble_size: int = 1

    temperature: float = 0.0
    epig_bootstrap_type: acquisition_functions.BootstrapType = acquisition_functions.BootstrapType.NO_BOOTSTRAP
    epig_bootstrap_factor: float = 1.0
    epig_dtype: torch.dtype = torch.double
    disable_training_augmentations: bool = False
    cache_explicit_eval_model: bool = False

    def load_experiment_data(self) -> ExperimentData:
        print(self.experiment_data_config)
        return self.experiment_data_config.load(self.device)

    # Simple Dependency Injection
    def create_train_eval_model(self) -> TrainEvalModel:
        di = DependencyInjection(vars(self))
        return di.create_dataclass_type(self.train_eval_model)

    def create_model_trainer(self) -> ModelTrainer:
        di = DependencyInjection(vars(self))
        return di.create_dataclass_type(self.model_trainer_factory)

    def run(self, store):
        torch.manual_seed(self.seed)

        # Active Learning setup
        data = self.load_experiment_data()
        store["dataset_info"] = dict(training=repr(data.active_learning.base_dataset), test=repr(data.test_dataset))
        store["initial_training_set_indices"] = data.initial_training_set_indices
        store["evaluation_set_indices"] = data.evaluation_set_indices

        model_trainer = self.create_model_trainer()
        if self.ensemble_size > 1:
            model_trainer = BayesianEnsembleModelTrainer(model_trainer=model_trainer, ensemble_size=self.ensemble_size)
        train_eval_model = self.create_train_eval_model()

        active_learner = ActiveLearner(
            acquisition_size=self.acquisition_size,
            num_validation_samples=self.num_validation_samples,
            num_pool_samples=self.num_pool_samples,
            disable_training_augmentations=self.disable_training_augmentations,
            train_eval_model=train_eval_model,
            model_trainer=model_trainer,
            data=data,
            device=self.device,
        )

        active_learner(store)

# Cell

# MNIST experiment (ood_exposure=False)

experiment = UnifiedExperiment(
    experiment_data_config=StandardExperimentDataConfig(
        id_dataset_name="MNIST",
        id_repetitions=1,
        initial_training_set_size=20,
        validation_set_size=4096,
        validation_split_random_state=0,
        evaluation_set_size=0,
        add_dataset_noise=False,
        ood_dataset_config=None,
    ),
    seed=1,
    max_training_epochs=120,
    model_trainer_factory=MnistModelTrainer,
    num_pool_samples=100,
    ensemble_size=2,
    device="cuda",
)

if not is_run_from_ipython() and __name__ == "__main__":
    store = embedded_experiment(__file__)
    experiment.seed += job_id
    print(experiment)
    store["config"] = dataclasses.asdict(experiment)
    store["log"] = {}

    try:
        experiment.run(store=store)
    except Exception:
        store["exception"] = traceback.format_exc()
        raise