# AUTOGENERATED! DO NOT EDIT! File to edit: 05b_dataset_challenges.ipynb (unless otherwise specified).

__all__ = ['NamedDataset', 'OverridenTargetDataset', 'CorruptedLabelsDataset', 'RandomLabelsDataset',
           'get_class_indices', 'get_balanced_sample_indices', 'ImbalancedDataset', 'ImbalancedClassSplitDataset',
           'OneHotDataset', 'RepeatedDataset', 'SubsetDataset', 'ConstantTargetDataset', 'UniformTargetDataset',
           'AdditiveGaussianNoise', 'dataset_to_tensors', 'get_dataset_state_dict', 'ImportedDataset', 'save_dataset',
           'load_dataset', 'get_base_dataset', 'get_base_index', 'get_target', 'get_targets',
           'create_repeated_MNIST_dataset', 'create_MNIST_dataset']

# Cell

import bisect
from typing import Optional, Union, List

import numpy as np
import torch
import torch.utils.data as data
import torchvision.datasets

from .fast_mnist import FastMNIST

# Cell


def _wrap_alias(dataset: data.Dataset):
    if isinstance(dataset, NamedDataset):
        return repr(dataset)
    return f"({dataset.alias})"


# TODO: add get_num_classes()

class _AliasDataset(data.Dataset):
    """
    A dataset with an easier to understand alias.

    And convenience operators.
    """

    dataset: data.Dataset
    alias: str

    def __init__(self, dataset: data.Dataset, alias: str):
        self.dataset = dataset
        self.alias = alias

    def __getitem__(self, idx):
        return self.dataset[idx]

    def __len__(self):
        return len(self.dataset)

    def __repr__(self):
        return self.alias

    def __add__(self, other):
        return _AliasDataset(data.ConcatDataset([self, other]), f"{_wrap_alias(self)} + {_wrap_alias(other)}")

    def __mul__(self, factor):
        if int(factor) == factor:
            return RepeatedDataset(self, num_repeats=factor)

        return SubsetDataset(self, factor=factor, seed=0)

    def __rmul__(self, factor):
        if int(factor) == factor:
            return RepeatedDataset(self, num_repeats=factor)

        return SubsetDataset(self, factor=factor, seed=0)


class NamedDataset(_AliasDataset):
    def __init__(self, dataset: data.Dataset, name: str):
        super().__init__(dataset, repr(name))

# Cell


class OverridenTargetDataset(_AliasDataset):
    reverse_indices: dict
    new_targets: list

    def __init__(self, dataset: data.Dataset, *, indices: list, new_targets: list):
        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}
        self.new_targets = new_targets

        super().__init__(dataset, f"{dataset} | override_targets{dict(indices=indices, new_targets=new_targets)}")

    def __getitem__(self, idx):
        data, label = self.dataset[idx]

        if idx not in self.reverse_indices:
            return data, label

        ridx = self.reverse_indices[idx]
        new_y = self.new_targets[ridx]
        return data, new_y


class CorruptedLabelsDataset(_AliasDataset):
    options: dict
    implementation: OverridenTargetDataset

    def __init__(
        self, dataset: data.Dataset, *, size_corrupted: Union[float, int], num_classes: int, seed: int, device=None
    ):
        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)

        super().__init__(dataset, f"{dataset} | corrupt_labels{options}")
        self.options = options

        generator = np.random.default_rng(seed)

        N = len(dataset)

        if size_corrupted > 1:
            num_corrupted = size_corrupted
        else:
            num_corrupted = int(N * size_corrupted)

        indices = generator.choice(N, size=num_corrupted, replace=False)
        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)

        self.implementation = OverridenTargetDataset(
            dataset, indices=indices, new_targets=torch.as_tensor(new_targets, device=device)
        )

    def __getitem__(self, idx):
        return self.implementation[idx]

    def __len__(self):
        return len(self.implementation)


class RandomLabelsDataset(_AliasDataset):
    options: dict
    new_labels: list

    def __init__(self, dataset: data.Dataset, *, num_classes: int, seed: int, device=None):
        options = dict(num_classes=num_classes, seed=seed)

        super().__init__(dataset, f"{dataset} | randomize_labels{options}")
        self.options = options

        generator = np.random.default_rng(seed)
        N = len(dataset)

        self.new_labels = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.new_labels[idx]

# Cell


def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator):
    class_counts = list(class_counts)

    subset_indices = []

    remaining_samples = sum(class_counts)

    indices = generator.permutation(len(dataset))
    for index in indices:
        _, y = dataset[index]

        if class_counts[y] > 0:
            subset_indices.append(index)
            class_counts[y] -= 1
            remaining_samples -= 1

            if remaining_samples <= 0:
                break

    return subset_indices


def get_balanced_sample_indices(dataset: data.Dataset, *, num_classes, samples_per_class, seed: int) -> List[int]:
    class_counts = [samples_per_class] * num_classes
    generator = np.random.default_rng(seed)

    return get_class_indices(dataset, class_counts=class_counts, generator=generator)

# Cell


class ImbalancedDataset(_AliasDataset):
    options: dict
    indices: list

    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):
        options = dict(class_counts=class_counts, seed=seed)
        super().__init__(dataset, f"ImbalancedDataset(dataset={dataset}, {options})")
        self.options = options

        generator = np.random.default_rng(seed)
        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)


class ImbalancedClassSplitDataset(_AliasDataset):
    options: dict
    indices: list

    def __init__(
        self, dataset: data.Dataset, *, num_classes: int, majority_percentage: int, minority_percentage: int, seed: int
    ):
        assert (num_classes % 2) == 0

        super().__init__(dataset, None)

        num_samples_per_class = len(dataset) // num_classes
        num_samples_majority = num_samples_per_class * majority_percentage // 100
        num_samples_minority = num_samples_per_class * minority_percentage // 100

        generator = np.random.default_rng(seed)

        class_counts = [num_samples_majority] * (num_classes // 2) + [num_samples_minority] * (num_classes // 2)
        class_counts = generator.permuted(class_counts)

        self.options = dict(
            num_classes=num_classes, majority_percentage=majority_percentage, seed=seed, class_counts=class_counts
        )
        self.alias = f"ImbalancedDataset(dataset={self.dataset}, {self.options})"

        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)

# Cell


# Convert label dataset to one hot
class OneHotDataset(_AliasDataset):
    options: dict
    targets: list

    def __init__(self, dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):
        options = dict(num_classes=num_classes)

        super().__init__(dataset, f"{dataset} | one_hot_targets{options}")
        self.options = options

        N = len(dataset)
        targets = torch.zeros(len(dataset), num_classes, dtype=dtype, device=device)
        # TODO: use get_targets() here, which will require a refactoring
        for i, (_, label) in enumerate(dataset):
            targets[i, label] = 1.0

        self.targets = targets

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.targets[idx]


class RepeatedDataset(_AliasDataset):
    def __init__(self, dataset: data.Dataset, *, num_repeats: int):
        self.num_repeats = num_repeats

        super().__init__(dataset, f"{dataset}x{num_repeats}")

    def __getitem__(self, idx):
        if idx > len(self):
            return self.dataset[idx]

        return self.dataset[idx % len(self.dataset)]

    def __len__(self):
        return len(self.dataset) * self.num_repeats


class SubsetDataset(_AliasDataset):
    options: dict
    indices: list

    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):
        options = dict(size=size, factor=factor, seed=seed)
        self.options = options

        generator = np.random.default_rng(seed)

        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)
        if size is not None:
            subset_size = size
            if seed == 0:
                alias = f"{dataset}[:{size}]"
            else:
                alias = f"{dataset}[:{size};seed={seed}]"
        elif factor is not None:
            subset_size = int(len(dataset) * factor)
            if seed == 0:
                alias = f"{dataset}~x{factor}"
            else:
                alias = f"{dataset}~x{factor} (seed={seed})"

        self.indices = generator.choice(len(dataset), size=subset_size, replace=subset_size > len(dataset))

        super().__init__(dataset, alias)

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)


class ConstantTargetDataset(_AliasDataset):
    target: object

    def __init__(self, dataset: data.Dataset, target: object):
        super().__init__(dataset, f"{dataset} | constant_target{target}")
        self.target = target

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.target


def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):
    target = torch.ones(num_classes, dtype=dtype, device=device) / num_classes
    result = ConstantTargetDataset(dataset, target)
    result.options = dict(num_classes=num_classes)
    result.alias = f"{dataset} | uniform_targets{result.options}"
    return result

# Cell


class AdditiveGaussianNoise(_AliasDataset):
    noise: torch.Tensor
    options: dict

    def __init__(self, dataset: data.Dataset, sigma: float):
        sample = dataset[0][0]
        self.noise = torch.randn(len(dataset), *sample.shape, device=sample.device)
        self.options = dict(sigma=sigma)

        super().__init__(dataset, f"{dataset} + 𝓝(0;σ={sigma})")

    def __getitem__(self, idx):
        sample, target = self.dataset[idx]
        return sample + self.noise[idx], target

# Cell


def dataset_to_tensors(dataset):
    samples = []
    targets = []

    for sample, target in dataset:
        samples.append(sample.to(device="cpu", non_blocking=True))
        targets.append(target.to(device="cpu", non_blocking=True))

    samples = torch.stack(samples)
    targets = torch.stack(targets)

    return samples, targets


def get_dataset_state_dict(dataset):
    dataset_alias = repr(dataset)

    samples, targets = dataset_to_tensors(dataset)

    state_dict = dict(alias=dataset_alias, samples=samples, targets=targets)

    return state_dict


class ImportedDataset(_AliasDataset):
    def __init__(self, state_dict, device=None):
        tensor_dataset = data.TensorDataset(state_dict["samples"], state_dict["targets"])
        super().__init__(tensor_dataset, state_dict["alias"])


def save_dataset(dataset: data.Dataset, f, **kwargs):
    torch.save(get_dataset_state_dict(dataset), f, **kwargs)


def load_dataset(f, map_location=None, **kwargs):
    state_dict = torch.load(f, map_location=map_location, **kwargs)
    dataset = ImportedDataset(state_dict)
    return dataset

# Cell


def get_base_dataset(dataset, index):
    if isinstance(dataset, NamedDataset):
        return dataset
    elif isinstance(dataset, data.ConcatDataset):
        if idx < 0:
            if -idx > len(self):
                raise ValueError("absolute value of index should not exceed dataset length")
            idx = len(self) + idx
        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        return get_base_dataset(dataset.datasets[dataset_idx], sample_idx)
    elif isinstance(dataset, ImbalancedDataset):
        return get_base_dataset(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, ImbalancedClassSplitDataset):
        return get_base_dataset(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, SubsetDataset):
        return get_base_dataset(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, data.Subset):
        return get_base_dataset(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, RepeatedDataset):
        return get_base_dataset(dataset.dataset, index % len(dataset.dataset))
    elif isinstance(dataset, _AliasDataset):
        return get_base_dataset(dataset.dataset, index)
    return dataset


def get_base_index(dataset, index):
    if isinstance(dataset, data.ConcatDataset):
        if idx < 0:
            if -idx > len(self):
                raise ValueError("absolute value of index should not exceed dataset length")
            idx = len(self) + idx
        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        return get_base_index(dataset.datasets[dataset_idx], sample_idx)
    elif isinstance(dataset, ImbalancedDataset):
        return get_base_index(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, ImbalancedClassSplitDataset):
        return get_base_index(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, SubsetDataset):
        return get_base_index(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, data.Subset):
        return get_base_index(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, RepeatedDataset):
        return get_base_index(dataset.dataset, index % len(dataset.dataset))
    elif isinstance(dataset, _AliasDataset):
        return get_base_index(dataset.dataset, index)
    elif isinstance(dataset, data.TensorDataset):
        return index
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return index
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return index

    raise NotImplementedError(f"Unrecognized dataset {dataset}!")


def get_target(dataset, index):
    if isinstance(dataset, data.ConcatDataset):
        if idx < 0:
            if -idx > len(self):
                raise ValueError("absolute value of index should not exceed dataset length")
            idx = len(self) + idx
        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        return get_target(dataset.datasets[dataset_idx], sample_idx)
    elif isinstance(dataset, CorruptedLabelsDataset):
        return get_target(dataset.implementation, index)
    elif isinstance(dataset, OverridenTargetDataset):
        if index not in dataset.reverse_indices:
            return get_target(dataset.dataset, index)

        ridx = dataset.reverse_indices[index]
        new_y = dataset.new_targets[ridx]
        return new_y
    elif isinstance(dataset, RandomLabelsDataset):
        return dataset.new_labels[index]
    elif isinstance(dataset, ImbalancedDataset):
        return get_target(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, ImbalancedClassSplitDataset):
        return get_target(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, SubsetDataset):
        return get_target(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, data.Subset):
        return get_target(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, RepeatedDataset):
        return get_target(dataset.dataset, index % len(dataset.dataset))
    elif isinstance(dataset, OneHotDataset):
        return dataset.targets[index]
    elif isinstance(dataset, ConstantTargetDataset):
        return dataset.target
    elif isinstance(dataset, _AliasDataset):
        return get_target(dataset.dataset, index)
    elif isinstance(dataset, data.TensorDataset):
        return dataset.tensors[1][index]
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return dataset.targets[index]
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return dataset.targets[index]

    raise NotImplementedError(f"Unrecognized dataset {dataset}!")


def get_targets(dataset):
    if isinstance(dataset, data.ConcatDataset):
        return torch.concat([get_targets(sub_dataset) for sub_dataset in dataset.datasets], device="cpu")
    elif isinstance(dataset, CorruptedLabelsDataset):
        return get_targets(dataset.implementation)
    elif isinstance(dataset, OverridenTargetDataset):
        targets = torch.clone(get_targets(dataset.dataset))
        targets[list(dataset.reverse_indices.keys())] = torch.as_tensor(dataset.new_targets)
        return targets
    elif isinstance(dataset, RandomLabelsDataset):
        return dataset.new_labels
    elif isinstance(dataset, ImbalancedDataset):
        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]
    elif isinstance(dataset, ImbalancedClassSplitDataset):
        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]
    elif isinstance(dataset, SubsetDataset):
        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]
    elif isinstance(dataset, Subset):
        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]
    elif isinstance(dataset, RepeatedDataset):
        return get_targets(dataset.dataset).repeat(dataset.num_repeats)
    elif isinstance(dataset, OneHotDataset):
        return dataset.targets
    elif isinstance(dataset, ConstantTargetDataset):
        return dataset.target.expand(len(dataset), *dataset.target.shape)
    elif isinstance(dataset, _AliasDataset):
        return get_targets(dataset.dataset)
    elif isinstance(dataset, data.TensorDataset):
        return torch.as_tensor(dataset.tensors[1])
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return torch.as_tensor(dataset.targets)
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return torch.as_tensor(dataset.targets)

    raise NotImplementedError(f"Unrecognized dataset {dataset} with type {type(dataset)}!")

# Cell


def create_repeated_MNIST_dataset(*, device=None, num_repetitions: int = 3, add_noise: bool = True):
    # num_classes = 10, input_size = 28

    train_dataset = NamedDataset(FastMNIST("data", train=True, download=True, device=device), "FastMNIST (Train)")

    rmnist_train_dataset = train_dataset
    if num_repetitions > 1:
        rmnist_train_dataset = train_dataset * num_repetitions

    if add_noise:
        rmnist_train_dataset = AdditiveGaussianNoise(rmnist_train_dataset, 0.1)

    test_dataset = NamedDataset(FastMNIST("data", train=False, device=device), "FastMNIST (Test)")

    return rmnist_train_dataset, test_dataset


def create_MNIST_dataset(device=None):
    return create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False, device=device)