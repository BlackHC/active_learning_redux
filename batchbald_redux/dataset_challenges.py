# AUTOGENERATED! DO NOT EDIT! File to edit: 05b_dataset_challenges.ipynb (unless otherwise specified).

__all__ = ['get_base_dataset_index', 'get_num_classes', 'get_target', 'get_targets', 'NamedDataset',
           'OverridenTargetDataset', 'CorruptedLabelsDataset', 'RandomLabelsDataset', 'get_class_indices',
           'get_balanced_sample_indices', 'ImbalancedDataset', 'ImbalancedClassSplitDataset', 'OneHotDataset',
           'RepeatedDataset', 'SubsetDataset', 'ConstantTargetDataset', 'UniformTargetDataset', 'AdditiveGaussianNoise',
           'dataset_to_tensors', 'get_dataset_state_dict', 'ImportedDataset', 'save_dataset', 'load_dataset',
           'create_repeated_MNIST_dataset', 'create_MNIST_dataset']

# Cell

import bisect
from typing import List, Optional, Union

import numpy as np
import torch
import torch.utils.data as data
import torchvision.datasets

from .fast_mnist import FastMNIST

# Cell


def _wrap_alias(dataset: data.Dataset):
    if isinstance(dataset, NamedDataset):
        return repr(dataset)
    return f"({dataset.alias})"


class _AliasDataset(data.Dataset):
    """
    A dataset with an easier to understand alias.

    And convenience operators.
    """

    dataset: data.Dataset
    alias: str

    def __init__(self, dataset: data.Dataset, alias: str):
        self.dataset = dataset
        self.alias = alias

    def __getitem__(self, idx):
        return self.dataset[idx]

    def __len__(self):
        return len(self.dataset)

    def __repr__(self):
        return self.alias

    def __add__(self, other):
        assert get_num_classes(self.dataset) == get_num_classes(other)
        return _AliasDataset(data.ConcatDataset([self, other]), f"{_wrap_alias(self)} + {_wrap_alias(other)}")

    def __mul__(self, factor):
        if int(factor) == factor:
            return RepeatedDataset(self, num_repeats=factor)

        return SubsetDataset(self, factor=factor, seed=0)

    def __rmul__(self, factor):
        if int(factor) == factor:
            return RepeatedDataset(self, num_repeats=factor)

        return SubsetDataset(self, factor=factor, seed=0)

    def get_base_dataset_index(self, index):
        return get_base_dataset_index(self.dataset, index)

    def get_num_classes(self):
        return get_num_classes(self.dataset)

    def get_target(self, index):
        return get_target(self.dataset, index)

    def get_targets(self):
        return get_targets(self.dataset)

# Cell


def get_base_dataset_index(dataset: data.Dataset, index: int) -> (data.Dataset, int):
    if isinstance(dataset, data.ConcatDataset):
        if index < 0:
            if -index > len(dataset):
                raise ValueError("absolute value of index should not exceed dataset length")
            index = len(dataset) + index
        dataset_idx = bisect.bisect_right(dataset.cumulative_sizes, index)
        if dataset_idx == 0:
            sample_idx = index
        else:
            sample_idx = index - dataset.cumulative_sizes[dataset_idx - 1]
        return get_base_dataset_index(dataset.datasets[dataset_idx], sample_idx)
    elif isinstance(dataset, data.Subset):
        return get_base_dataset_index(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, data.TensorDataset):
        return dataset, index
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return dataset, index
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return dataset, index
    elif isinstance(dataset, _AliasDataset):
        return dataset.get_base_dataset_index(index)
    else:
        raise NotImplementedError(f"Unrecognized dataset {dataset}!")


def get_num_classes(dataset: data.Dataset) -> int:
    if isinstance(dataset, data.ConcatDataset):
        return get_num_classes(dataset.datasets[0])
    elif isinstance(dataset, data.Subset):
        return get_num_classes(dataset.dataset)
    elif isinstance(dataset, data.TensorDataset):
        return dataset.tensors[1].shape[1:]
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return len(dataset.classes)
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return len(dataset.classes)
    elif isinstance(dataset, _AliasDataset):
        return dataset.get_num_classes()
    else:
        raise NotImplementedError(f"Unrecognized dataset {dataset}!")


def get_target(dataset, index) -> torch.Tensor:
    if isinstance(dataset, data.ConcatDataset):
        if index < 0:
            if -index > len(dataset):
                raise ValueError("absolute value of index should not exceed dataset length")
            index = len(dataset) + index
        dataset_idx = bisect.bisect_right(dataset.cumulative_sizes, index)
        if dataset_idx == 0:
            sample_idx = index
        else:
            sample_idx = index - dataset.cumulative_sizes[dataset_idx - 1]
        return get_target(dataset.datasets[dataset_idx], sample_idx)
    elif isinstance(dataset, data.Subset):
        return get_target(dataset.dataset, dataset.indices[index])
    elif isinstance(dataset, data.TensorDataset):
        return dataset.tensors[1][index]
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return dataset.targets[index]
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return dataset.targets[index]
    elif isinstance(dataset, _AliasDataset):
        return dataset.get_target(index)

    raise NotImplementedError(f"Unrecognized dataset {dataset}!")


def get_targets(dataset):
    if isinstance(dataset, data.ConcatDataset):
        return torch.concat([get_targets(sub_dataset) for sub_dataset in dataset.datasets], device="cpu")
    elif isinstance(dataset, data.Subset):
        return get_targets(dataset.dataset)[torch.as_tensor(dataset.indices)]
    elif isinstance(dataset, data.TensorDataset):
        return torch.as_tensor(dataset.tensors[1])
    elif isinstance(dataset, torchvision.datasets.MNIST):
        return torch.as_tensor(dataset.targets)
    elif isinstance(dataset, torchvision.datasets.CIFAR10):
        return torch.as_tensor(dataset.targets)
    elif isinstance(dataset, _AliasDataset):
        return dataset.get_targets()

    raise NotImplementedError(f"Unrecognized dataset {dataset} with type {type(dataset)}!")


class _SubsetAliasDataset(_AliasDataset):
    indices: list

    def __init__(self, dataset: data.Dataset, indices: list, alias: str):
        super().__init__(dataset, alias)

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)

    def get_base_dataset_index(self, index) -> (data.Dataset, int):
        return get_base_dataset_index(self.dataset, self.indices[index])

    def get_num_classes(self) -> int:
        return get_num_classes(self.dataset)

    def get_target(self, index) -> torch.Tensor:
        return get_target(self.dataset, self.indices[index])

    def get_targets(self) -> torch.Tensor:
        return get_targets(self.dataset)[torch.as_tensor(self.indices)]


class _NewTargetsDataset(_AliasDataset):
    targets: torch.Tensor
    num_classes: int

    def __init__(self, dataset: data.Dataset, targets: torch.Tensor, num_classes: int, alias: str):
        super().__init__(dataset, alias)
        self.num_classes = num_classes
        self.targets = targets

    def __getitem__(self, index):
        data, _ = self.dataset[index]
        return data, self.targets[index]

    def get_num_classes(self):
        return self.num_classes

    def get_target(self, index):
        return self.targets[index]

    def get_targets(self):
        return self.targets


class NamedDataset(_AliasDataset):
    def __init__(self, dataset: data.Dataset, name: str):
        super().__init__(dataset, repr(name))

    def get_base_dataset_index(self, index):
        # NamedDatasets are a leaf for all purposes.
        return self, index

# Cell


class OverridenTargetDataset(_AliasDataset):
    reverse_indices: dict
    new_targets: list
    num_classes: int

    def __init__(
        self, dataset: data.Dataset, *, indices: list, new_targets: Union[list, torch.Tensor], num_classes: int = None
    ):
        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}
        self.new_targets = new_targets
        self.num_classes = num_classes

        super().__init__(
            dataset,
            f"{dataset} | override_targets{dict(indices=indices, new_targets=new_targets, num_classes=num_classes)}",
        )

    def __getitem__(self, index):
        data, target = self.dataset[index]

        if index not in self.reverse_indices:
            return data, target

        reverse_index = self.reverse_indices[index]
        new_target = self.new_targets[reverse_index]
        return data, new_target

    def get_num_classes(self):
        return self.num_classes or get_num_classes(self.dataset)

    def get_target(self, index):
        if index not in self.reverse_indices:
            return get_target(self.dataset, index)

        ridx = self.reverse_indices[index]
        new_target = self.new_targets[ridx]
        return new_target

    def get_targets(self):
        targets = torch.clone(get_targets(self.dataset))
        targets[list(self.reverse_indices.keys())] = torch.as_tensor(self.new_targets)
        return targets


class CorruptedLabelsDataset(_AliasDataset):
    options: dict
    implementation: OverridenTargetDataset

    def __init__(
        self,
        dataset: data.Dataset,
        *,
        size_corrupted: Union[float, int],
        seed: int,
        num_classes: int = None,
        device=None,
    ):
        num_classes = num_classes or get_num_classes(dataset)
        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)

        super().__init__(dataset, f"{dataset} | corrupt_labels{options}")
        self.options = options
        self.num_classes = num_classes

        N = len(dataset)

        if size_corrupted > 1:
            num_corrupted = size_corrupted
        else:
            num_corrupted = int(N * size_corrupted)

        generator = np.random.default_rng(seed)
        indices = generator.choice(N, size=num_corrupted, replace=False)
        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)

        self.implementation = OverridenTargetDataset(
            dataset, indices=indices, new_targets=torch.as_tensor(new_targets, device=device), num_classes=num_classes
        )

    def __getitem__(self, idx):
        return self.implementation[idx]

    def __len__(self):
        return len(self.implementation)

    def get_num_classes(self):
        return self.num_classes

    def get_target(self, index):
        return self.implementation.get_target(index)

    def get_targets(self):
        return self.implementation.get_targets()


class RandomLabelsDataset(_NewTargetsDataset):
    options: dict

    def __init__(self, dataset: data.Dataset, *, seed: int, num_classes: int = None, device=None):
        num_classes = num_classes or get_num_classes(dataset)
        options = dict(num_classes=num_classes, seed=seed)

        generator = np.random.default_rng(seed)
        N = len(dataset)
        targets = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)

        super().__init__(dataset, targets, num_classes, f"{dataset} | randomize_labels{options}")
        self.options = options

# Cell


def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator):
    class_counts = list(class_counts)

    subset_indices = []

    remaining_samples = sum(class_counts)

    indices = generator.permutation(len(dataset))
    for index in indices:
        _, y = dataset[index]

        if class_counts[y] > 0:
            subset_indices.append(index)
            class_counts[y] -= 1
            remaining_samples -= 1

            if remaining_samples <= 0:
                break

    return subset_indices


def get_balanced_sample_indices(dataset: data.Dataset, *, num_classes, samples_per_class, seed: int) -> List[int]:
    class_counts = [samples_per_class] * num_classes
    generator = np.random.default_rng(seed)

    return get_class_indices(dataset, class_counts=class_counts, generator=generator)

# Cell


class ImbalancedDataset(_SubsetAliasDataset):
    options: dict

    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):
        options = dict(class_counts=class_counts, seed=seed)
        generator = np.random.default_rng(seed)
        indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)

        super().__init__(dataset, indices, f"ImbalancedDataset(dataset={dataset}, {options})")
        self.options = options


class ImbalancedClassSplitDataset(_SubsetAliasDataset):
    options: dict

    def __init__(
        self, dataset: data.Dataset, *, num_classes: int, majority_percentage: int, minority_percentage: int, seed: int
    ):
        assert (num_classes % 2) == 0

        num_samples_per_class = len(dataset) // num_classes
        num_samples_majority = num_samples_per_class * majority_percentage // 100
        num_samples_minority = num_samples_per_class * minority_percentage // 100

        generator = np.random.default_rng(seed)

        class_counts = [num_samples_majority] * (num_classes // 2) + [num_samples_minority] * (num_classes // 2)
        class_counts = generator.permuted(class_counts)

        indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)

        super().__init__(dataset, indices, f"ImbalancedDataset(dataset={self.dataset}, {self.options})")

        self.options = dict(
            num_classes=num_classes, majority_percentage=majority_percentage, seed=seed, class_counts=class_counts
        )

# Cell

# Convert label dataset to one hot
class OneHotDataset(_NewTargetsDataset):
    options: dict

    def __init__(self, dataset: data.Dataset, *, dtype=None, device=None):
        num_classes = get_num_classes(dataset)
        options = dict(num_classes=num_classes)

        N = len(dataset)
        targets = torch.zeros(len(dataset), num_classes, dtype=dtype, device=device)
        # TODO: use get_targets() here, which will require a refactoring
        for i, (_, label) in enumerate(dataset):
            targets[i, label] = 1.0

        super().__init__(dataset, targets, num_classes, f"{dataset} | one_hot_targets{options}")
        self.options = options


class RepeatedDataset(_AliasDataset):
    def __init__(self, dataset: data.Dataset, *, num_repeats: int):
        self.num_repeats = num_repeats

        super().__init__(dataset, f"{dataset}x{num_repeats}")

    def __getitem__(self, idx):
        if idx > len(self):
            return self.dataset[idx]

        return self.dataset[idx % len(self.dataset)]

    def __len__(self):
        return len(self.dataset) * self.num_repeats

    def get_target(self, index):
        return get_target(self.dataset, index % len(self.dataset))

    def get_targets(self):
        return get_targets(self.dataset).repeat(self.num_repeats)


class SubsetDataset(_SubsetAliasDataset):
    options: dict

    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):
        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)
        if size is not None:
            subset_size = size
            if seed == 0:
                alias = f"{dataset}[:{size}]"
            else:
                alias = f"{dataset}[:{size};seed={seed}]"
        elif factor is not None:
            subset_size = int(len(dataset) * factor)
            if seed == 0:
                alias = f"{dataset}~x{factor}"
            else:
                alias = f"{dataset}~x{factor} (seed={seed})"

        generator = np.random.default_rng(seed)
        indices = generator.choice(len(dataset), size=subset_size, replace=subset_size > len(dataset))

        super().__init__(dataset, indices, alias)
        self.options = dict(size=size, factor=factor, seed=seed)


class ConstantTargetDataset(_AliasDataset):
    target: torch.Tensor

    def __init__(self, dataset: data.Dataset, target: torch.Tensor, num_classes=None):
        num_classes = num_classes or get_num_classes(dataset)
        super().__init__(dataset, f"{dataset} | constant_target{dict(target=target, num_classes=num_classes)}")
        self.target = target
        self.num_classes = num_classes

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.target

    def get_num_classes(self):
        return self.num_classes

    def get_target(self, index):
        return self.target

    def get_targets(self):
        return self.target.expand(len(self), *self.target.shape)


def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):
    target = torch.ones(num_classes, dtype=dtype, device=device) / num_classes
    result = ConstantTargetDataset(dataset, target)
    result.options = dict(num_classes=num_classes)
    result.alias = f"{dataset} | uniform_targets{result.options}"
    return result

# Cell


class AdditiveGaussianNoise(_AliasDataset):
    noise: torch.Tensor
    options: dict

    def __init__(self, dataset: data.Dataset, sigma: float):
        sample = dataset[0][0]
        self.noise = torch.randn(len(dataset), *sample.shape, device=sample.device)
        self.options = dict(sigma=sigma)

        super().__init__(dataset, f"{dataset} + 𝓝(0;σ={sigma})")

    def __getitem__(self, idx):
        sample, target = self.dataset[idx]
        return sample + self.noise[idx], target

# Cell


def dataset_to_tensors(dataset):
    samples = []
    targets = []

    for sample, target in dataset:
        samples.append(sample.to(device="cpu", non_blocking=True))
        targets.append(target.to(device="cpu", non_blocking=True))

    samples = torch.stack(samples)
    targets = torch.stack(targets)

    return samples, targets


def get_dataset_state_dict(dataset):
    dataset_alias = repr(dataset)

    samples, targets = dataset_to_tensors(dataset)

    state_dict = dict(alias=dataset_alias, samples=samples, targets=targets)

    return state_dict


class ImportedDataset(_AliasDataset):
    def __init__(self, state_dict):
        tensor_dataset = data.TensorDataset(state_dict["samples"], state_dict["targets"])
        super().__init__(tensor_dataset, state_dict["alias"])


def save_dataset(dataset: data.Dataset, f, **kwargs):
    torch.save(get_dataset_state_dict(dataset), f, **kwargs)


def load_dataset(f, map_location=None, **kwargs):
    state_dict = torch.load(f, map_location=map_location, **kwargs)
    dataset = ImportedDataset(state_dict)
    return dataset

# Cell


def create_repeated_MNIST_dataset(*, device=None, num_repetitions: int = 3, add_noise: bool = True):
    # num_classes = 10, input_size = 28

    train_dataset = NamedDataset(FastMNIST("data", train=True, download=True, device=device), "FastMNIST (Train)")

    rmnist_train_dataset = train_dataset
    if num_repetitions > 1:
        rmnist_train_dataset = train_dataset * num_repetitions

    if add_noise:
        rmnist_train_dataset = AdditiveGaussianNoise(rmnist_train_dataset, 0.1)

    test_dataset = NamedDataset(FastMNIST("data", train=False, device=device), "FastMNIST (Test)")

    return rmnist_train_dataset, test_dataset


def create_MNIST_dataset(device=None):
    return create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False, device=device)