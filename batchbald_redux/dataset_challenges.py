# AUTOGENERATED! DO NOT EDIT! File to edit: 05d_dataset_challenges.ipynb (unless otherwise specified).

__all__ = ['NamedDataset', 'get_base_dataset', 'OverridenTargetDataset', 'CorruptedLabelsDataset',
           'RandomLabelsDataset', 'get_class_indices', 'ImbalancedDataset', 'ImbalancedClassSplitDataset',
           'OneHotDataset', 'SubsetDataset', 'ConstantTargetDataset', 'UniformTargetDataset', 'AdditiveGaussianNoise',
           'dataset_to_tensors', 'get_dataset_state_dict', 'ImportedDataset', 'save_dataset', 'load_dataset']

# Cell

import bisect
from typing import Optional, Union

import numpy as np
import torch
import torch.utils.data as data

# Cell


class _AliasDataset(data.Dataset):
    """
    A dataset with an easier to understand alias.

    And convenience operators.
    """
    dataset: data.Dataset
    alias: str

    def __init__(self, dataset: data.Dataset, alias: str):
        self.dataset = dataset
        self.alias = alias

    def __getitem__(self, idx):
        return self.dataset[idx]

    def __len__(self):
        return len(self.dataset)

    def __repr__(self):
        return self.alias

    def __add__(self, other):
        return _AliasDataset(data.ConcatDataset([self, other]), f"{self} + {other}")

    def __mul__(self, factor):
        return SubsetDataset(self, factor=factor, seed=0)

    def __rmul__(self, factor):
        return SubsetDataset(self, factor=factor, seed=0)


class NamedDataset(_AliasDataset):
    def __init__(self, dataset: data.Dataset, name: str):
        super().__init__(dataset, name)


def get_base_dataset(dataset, index):
    if isinstance(dataset, NamedDataset):
        return dataset
    elif isinstance(dataset, data.ConcatDataset):
        if idx < 0:
            if -idx > len(self):
                raise ValueError("absolute value of index should not exceed dataset length")
            idx = len(self) + idx
        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)
        if dataset_idx == 0:
            sample_idx = idx
        else:
            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]
        return get_base_dataset(dataset.datasets[dataset_idx], sample_idx)
    elif isinstance(dataset, _AliasDataset):
        return get_base_dataset(dataset.dataset, index)
    return dataset

# Cell


class OverridenTargetDataset(_AliasDataset):
    indices_set: set
    reverse_indices: dict
    new_targets: list

    def __init__(self, dataset: data.Dataset, *, indices: list, new_targets: list):
        self.indices_set = set(indices)
        self.reverse_indices = {idx: rank for rank, idx in enumerate(indices)}
        self.new_targets = new_targets

        super().__init__(dataset, f"OverridenTargetDataset({var(self)})")

    def __getitem__(self, idx):
        data, label = self.dataset[idx]

        if idx not in self.indices_set:
            return data, label

        ridx = self.reverse_indices[idx]
        new_y = self.new_targets[ridx]
        return data, new_y


class CorruptedLabelsDataset(_AliasDataset):
    options: dict
    implementation: OverridenTargetDataset

    def __init__(
        self, dataset: data.Dataset, *, size_corrupted: Union[float, int], num_classes: int, seed: int, device=None
    ):
        options = dict(size_corrupted=size_corrupted, num_classes=num_classes, seed=seed)

        super().__init__(dataset, f"CorruptedLabelsDataset(dataset={dataset}, {options})")
        self.options = options

        generator = np.random.default_rng(seed)

        N = len(dataset)

        if size_corrupted > 1:
            num_corrupted = size_corrupted
        else:
            num_corrupted = int(N * size_corrupted)

        indices = generator.choice(N, size=num_corrupted, replace=False)
        new_targets = generator.choice(num_classes, size=num_corrupted, replace=True)

        self.implementation = OverridenTargetDataset(
            dataset, indices=indices, new_targets=torch.as_tensor(new_targets, device=device)
        )

    def __getitem__(self, idx):
        return self.implementation[idx]

    def __len__(self):
        return len(self.implementation)


class RandomLabelsDataset(_AliasDataset):
    options: dict
    new_labels: list

    def __init__(self, dataset: data.Dataset, *, num_classes: int, seed: int, device=None):
        options = dict(num_classes=num_classes, seed=seed)

        super().__init__(dataset, f"RandomLabelsDataset(dataset={dataset}, {options})")
        self.options = options

        generator = np.random.default_rng(seed)
        N = len(dataset)

        self.new_labels = torch.as_tensor(generator.choice(num_classes, size=N, replace=True), device=device)

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.new_labels[idx]

# Cell


def get_class_indices(dataset: data.Dataset, *, class_counts: list, generator: np.random.Generator):
    class_counts = list(class_counts)

    subset_indices = []

    remaining_samples = sum(class_counts)

    indices = generator.permutation(len(dataset))
    for index in indices:
        _, y = dataset[index]

        if class_counts[y] > 0:
            subset_indices.append(index)
            class_counts[y] -= 1
            remaining_samples -= 1

            if remaining_samples <= 0:
                break

    return subset_indices


class ImbalancedDataset(_AliasDataset):
    options: dict
    indices: list

    def __init__(self, dataset: data.Dataset, *, class_counts: list, seed: int):
        options = dict(class_counts=class_counts, seed=seed)
        super().__init__(dataset, f"ImbalancedDataset(dataset={dataset}, {options})")
        self.options = options

        generator = np.random.default_rng(seed)
        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)


class ImbalancedClassSplitDataset(_AliasDataset):
    dataset: data.Dataset
    options: dict
    indices: list

    def __init__(self, dataset: data.Dataset, *, num_classes: int, majority_percentage: int, minority_percentage: int, seed: int):
        assert (num_classes % 2) == 0

        super().__init__(dataset, None)

        num_samples_per_class = len(dataset) // num_classes
        num_samples_majority = num_samples_per_class * majority_percentage // 100
        num_samples_minority = num_samples_per_class * minority_percentage // 100

        generator = np.random.default_rng(seed)

        class_counts = [num_samples_majority] * (num_classes // 2) + [num_samples_minority] * (num_classes // 2)
        class_counts = generator.permuted(class_counts)

        self.options = dict(
            num_classes=num_classes, majority_percentage=majority_percentage, seed=seed, class_counts=class_counts
        )
        self.alias = f"ImbalancedDataset(dataset={self.dataset}, {self.options})"

        self.indices = get_class_indices(dataset, class_counts=class_counts, generator=generator)

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)

# Cell


# Convert label dataset to one hot
class OneHotDataset(_AliasDataset):
    options: dict
    targets: list

    def __init__(self, dataset: data.Dataset, *, num_classes: int, dtype=None, device=None):
        options = dict(num_classes=num_classes)

        super().__init__(dataset, f"OneHotDataset(dataset={dataset}, {options})")
        self.options = options

        N = len(dataset)
        targets = torch.zeros(len(dataset), num_classes, dtype=dtype, device=device)
        for i, (_, label) in enumerate(dataset):
            targets[i, label] = 1.0

        self.targets = targets

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.targets[idx]


class SubsetDataset(_AliasDataset):
    options: dict
    indices: list

    def __init__(self, dataset: data.Dataset, *, size: Optional[int] = None, factor: Optional[float] = None, seed: int):
        options = dict(size=size, factor=factor, seed=seed)
        super().__init__(dataset, f"SubsetDataset(dataset={dataset}, {options})")
        self.options = options

        generator = np.random.default_rng(seed)

        assert ((size is not None) or (factor is not None)) and not (size is None and factor is None)
        if size is not None:
            subset_size = size
            if seed == 0:
                self.alias = f"~{dataset}[:{size}]"
        elif factor is not None:
            subset_size = int(len(dataset) * factor)
            if seed == 0:
                self.alias = f"~{dataset} * {factor}"

        self.indices = generator.choice(len(dataset), size=subset_size, replace=subset_size > len(dataset))

    def __getitem__(self, idx):
        return self.dataset[self.indices[idx]]

    def __len__(self):
        return len(self.indices)


class ConstantTargetDataset(_AliasDataset):
    target: object

    def __init__(self, dataset: data.Dataset, target: object):
        super().__init__(dataset, f"ConstantTargetDataset(dataset={dataset}, {target})")
        self.target = target

    def __getitem__(self, idx):
        data, _ = self.dataset[idx]
        return data, self.target


def UniformTargetDataset(dataset: data.Dataset, *, num_classes: int, device: str = None):
    target = torch.ones(num_classes, device=device) / num_classes
    result = ConstantTargetDataset(dataset, target)
    result.options = dict(num_classes=num_classes)
    result.alias = f"UniformTargetDataset({dataset}, {result.options})"
    return result

# Cell


class AdditiveGaussianNoise(_AliasDataset):
    noise: torch.Tensor
    options: dict

    def __init__(self, dataset: data.Dataset, sigma: float):
        sample = dataset[0][0]
        self.noise = torch.randn(len(dataset), *sample.shape, device=sample.device)
        self.options = dict(sigma=sigma)

        super().__init__(dataset, f"AdditiveGaussianNoise(dataset={dataset}, {self.options})")

    def __getitem__(self, idx):
        sample, target = self.dataset[idx]
        return sample + self.noise[idx], target


# Cell


def dataset_to_tensors(dataset):
    samples = []
    targets = []

    for sample, target in dataset:
        samples.append(sample.to(device="cpu", non_blocking=True))
        targets.append(target.to(device="cpu", non_blocking=True))

    samples = torch.stack(samples)
    targets = torch.stack(targets)

    return samples, targets


def get_dataset_state_dict(dataset):
    dataset_alias = repr(dataset)

    samples, targets = dataset_to_tensors(dataset)

    state_dict = dict(alias=dataset_alias, samples=samples,targets=targets)

    return state_dict


class ImportedDataset(_AliasDataset):
    def __init__(self, state_dict, device=None):
        tensor_dataset = data.TensorDataset(state_dict["samples"], state_dict["targets"])
        super().__init__(tensor_dataset, state_dict["alias"])


def save_dataset(dataset: data.Dataset, f, **kwargs):
    torch.save(get_dataset_state_dict(dataset), f, **kwargs)


def load_dataset(f, map_location=None, **kwargs):
    state_dict = torch.load(f, map_location=map_location, **kwargs)
    dataset = ImportedDataset(state_dict)
    return dataset