store = {}
store['timestamp']=1622025617
store['cmdline']=['/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py', '--id=0']
store['commit']='1f68e8b83aac9beae07be70736a56c87efb93e48'
store['github_url']='1f68e8b83aac9beae07be70736a56c87efb93e48'
store['experiment']='/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py'
store['job_id']=0
store['worker_id']=0
store['num_workers']=15
store['config']={'seed': 6666, 'id_dataset_name': 'CIFAR-10', 'initial_training_set_size': 1000, 'validation_set_size': 5000, 'evaluation_set_size': 0, 'id_repetitions': 1, 'add_dataset_noise': False, 'validation_split_random_state': 0, 'acquisition_size': 5, 'max_training_set': 20000, 'num_pool_samples': 100, 'num_validation_samples': 20, 'num_training_samples': 1, 'max_training_epochs': 60, 'training_batch_size': 128, 'device': 'cuda', 'min_samples_per_epoch': 5056, 'patience_schedule': [3, 3, 3], 'factor_schedule': [0.1], 'acquisition_function': 'batchbald_redux.acquisition_functions.BatchBALD', 'train_eval_model': 'batchbald_redux.train_eval_model.TrainSelfDistillationEvalModel', 'model_optimizer_factory': 'batchbald_redux.resnet_models.Cifar10BayesianResnetFactory', 'acquisition_function_args': None, 'temperature': 0.0}
store['log']={}
store['exception']='Traceback (most recent call last):\n  File "/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py", line 400, in <module>\n    config.run(store=store)\n  File "/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py", line 253, in run\n    data = self.load_experiment_data()\n  File "/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py", line 237, in load_experiment_data\n    return edc.load()\n  File "/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py", line 106, in load\n    return load_experiment_data(\n  File "/home/kell4989/git/active_learning_redux/batchbald_redux/experiment_cifar10_xmi_labels_clean.py", line 138, in load_experiment_data\n    train_predictions = torch.load("./data/cifar10_train_predictions.pt", map_location=device)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 592, in load\n    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 851, in _load\n    result = unpickler.load()\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 843, in persistent_load\n    load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 832, in load_tensor\n    loaded_storages[key] = restore_location(storage, location)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 809, in restore_location\n    return default_restore_location(storage, map_location)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 175, in default_restore_location\n    result = fn(storage, location)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/serialization.py", line 157, in _cuda_deserialize\n    return obj.cuda(device)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/_utils.py", line 80, in _cuda\n    return new_type(self.size()).copy_(self, non_blocking)\n  File "/tmp/conda_envs/job-1603663/lib/python3.8/site-packages/torch/cuda/__init__.py", line 484, in _lazy_new\n    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\nRuntimeError: CUDA error: out of memory\n'
