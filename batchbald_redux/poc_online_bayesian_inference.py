# AUTOGENERATED! DO NOT EDIT! File to edit: D_poc_online_bayesian_inference.ipynb (unless otherwise specified).

__all__ = ['compute_entropy_from_probs', 'Experiment', 'training_set_90', 'training_set_random', 'configs']

# Cell

import dataclasses
import traceback
from dataclasses import dataclass
from typing import Type, Union

import torch
import torch.utils.data
from blackhc.project import is_run_from_ipython
from blackhc.project.experiment import embedded_experiments
from torch.utils.data import Dataset

import batchbald_redux.acquisition_functions as acquisition_functions
import wandb
from .acquisition_functions import (
    CandidateBatchComputer,
    EvalModelBatchComputer,
)
from .active_learning import ActiveLearningData, RandomFixedLengthSampler
from .black_box_model_training import evaluate_old, train
from .dataset_challenges import (
    AdditiveGaussianNoise,
    NamedDataset,
    create_repeated_MNIST_dataset,
    get_balanced_sample_indices,
    get_base_dataset_index,
    get_target,
)
from .di import DependencyInjection
from .experiment_logging import asclassdict, init_wandb, log2wandb
from .fast_mnist import FastMNIST
from .model_optimizer_factory import ModelOptimizerFactory
from .models import MnistOptimizerFactory
from .online_bayesian_inference import evaluate_online_bayesian_inference

# Cell

# From the BatchBALD Repo
from .train_eval_model import (
    TrainEvalModel,
    TrainSelfDistillationEvalModel,
)
from .trained_model import TrainedBayesianModel

# Cell

from blackhc.progress_bar import create_progress_bar
from toma import toma


def compute_entropy_from_probs(probs_N_K_C: torch.Tensor) -> torch.Tensor:
    N, K, C = probs_N_K_C.shape

    entropies_N = torch.empty(N, dtype=torch.double)

    pbar = create_progress_bar(N, tqdm_args=dict(desc="Entropy", leave=False))
    pbar.start()

    @toma.execute.chunked(probs_N_K_C, 1024)
    def compute(probs_n_K_C, start: int, end: int):
        mean_probs_n_C = probs_n_K_C.mean(dim=1)
        nats_n_C = mean_probs_n_C * torch.log(mean_probs_n_C)
        nats_n_C[mean_probs_n_C == 0] = 0.0

        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1))
        pbar.update(end - start)

    pbar.finish()

    return entropies_N

# Cell

# Rerun from 2022/04/15

training_set_90 = [
    46377,
    44390,
    7485,
    25450,
    12220,
    43112,
    3612,
    54256,
    44467,
    17555,
    4218,
    45101,
    41296,
    32261,
    3281,
    6944,
    7701,
    30692,
    24891,
    24436,
    17776,
    36852,
    26891,
    17001,
    44414,
    23050,
    30884,
    41516,
    8897,
    34608,
    11721,
    13127,
    9870,
    39700,
    13985,
    9608,
    17808,
    23028,
    3494,
    10210,
    42384,
    31742,
    17100,
    46530,
    26733,
    4762,
    14113,
    20798,
    28388,
    42973,
    37347,
    55042,
    41295,
    28310,
    19280,
    18501,
    274,
    11208,
    13335,
    24440,
    49493,
    58129,
    56210,
    40766,
    13318,
    50040,
    18408,
    7255,
    15494,
    7168,
    9472,
    14845,
    14096,
    26072,
    35944,
    34836,
    12655,
    991,
    38577,
    47322,
    45212,
    16286,
    5308,
    38182,
    31624,
    12702,
    30861,
    46320,
    36408,
    49841,
    33426,
    57527,
    3273,
    24740,
    39627,
    22199,
    19362,
    42136,
    41965,
    43095,
]

training_set_90[:20] = [
    51348,
    49110,
    8222,
    28130,
    13484,
    47685,
    3938,
    59930,
    49196,
    19427,
    4601,
    49922,
    45635,
    35684,
    3579,
    7621,
    8465,
    33959,
    27521,
    27015,
]

training_set_random = [
    51348,
    49110,
    8222,
    28130,
    13484,
    47685,
    3938,
    59930,
    49196,
    19427,
    4601,
    49922,
    45635,
    35684,
    3579,
    7621,
    8465,
    33959,
    27521,
    27015,
    14546,
    56672,
    29175,
    34788,
    8131,
    26332,
    38192,
    21892,
    39029,
    38998,
    5055,
    59421,
    38657,
    56902,
    12450,
    24639,
    40834,
    49727,
    36103,
    9254,
    9245,
    58379,
    23120,
    33276,
    4809,
    1912,
    21893,
    40110,
    13327,
    12976,
    32620,
    33379,
    46791,
    54018,
    56012,
    14950,
    23198,
    23621,
    8800,
    13812,
    39580,
    51209,
    890,
    47684,
    52758,
    53629,
    1215,
    31729,
    24465,
    50917,
    48575,
    47113,
    20236,
    39311,
    25944,
    43811,
    37921,
    6902,
    321,
    49988,
    9436,
    33907,
    56920,
    13604,
    29309,
    14098,
    22260,
    40723,
    27635,
    36611,
    57194,
    44682,
    55813,
    29089,
    23673,
    55498,
    636,
    32250,
    33342,
    34457,
]


@dataclass
class Experiment:
    seed: int = 1337
    num_pool_samples: int = 20
    num_validation_samples: int = 20
    num_training_samples: int = 1
    num_patience_epochs: int = 5 * 4
    max_training_epochs: int = 30 * 4
    training_batch_size: int = 64
    device: str = "cuda"
    validation_set_size: int = 1024
    min_samples_per_epoch: int = 1024
    model_optimizer_factory: Type[ModelOptimizerFactory] = MnistOptimizerFactory
    max_obi_steps: int = 10
    num_samples_list: int = (100, 1000, 10000)
    up_factor: int = 2
    num_trials: int = 5
    acquisition_size: int = 5
    random_acquisition: bool = False

    def load_dataset(self, training_set_indices) -> (ActiveLearningData, Dataset, Dataset):
        train_dataset = NamedDataset(
            FastMNIST("data", train=True, download=True, device=self.device), "FastMNIST (train)"
        )

        active_learning_data = ActiveLearningData(train_dataset)

        active_learning_data.acquire_base_indices(training_set_indices[:20])

        active_learning_data_validation_set = ActiveLearningData(train_dataset)
        active_learning_data_validation_set.acquire_base_indices(training_set_indices)

        validation_dataset = active_learning_data_validation_set.extract_dataset_from_pool(self.validation_set_size)
        validation_dataset = NamedDataset(
            validation_dataset, f"FastMNIST (validation, {len(validation_dataset)} samples)"
        )

        test_dataset = FastMNIST("data", train=False, device=self.device)
        test_dataset = NamedDataset(test_dataset, f"FastMNIST (test, {len(test_dataset)} samples)")

        return active_learning_data, validation_dataset, test_dataset, training_set_indices[:20]

    # Simple Dependency Injection
    def create_train_eval_model(self, runtime_config) -> TrainEvalModel:
        config = {**vars(self), **runtime_config}
        di = DependencyInjection(config, [])
        return di.create_dataclass_type(self.train_eval_model)

    def run(self, store):
        init_wandb(self)

        training_set_indices = training_set_random if self.random_acquisition else training_set_90

        torch.manual_seed(self.seed)

        # Active Learning setup
        active_learning_data, validation_dataset, test_dataset, initial_training_set_indices = self.load_dataset(training_set_indices)
        store["initial_training_set_indices"] = initial_training_set_indices
        store["dataset_info"] = dict(training=repr(active_learning_data.base_dataset), test=repr(test_dataset))

        print(wandb.config)

        wandb.config.initial_training_set_indices = initial_training_set_indices
        wandb.config["dataset_info"] = store["dataset_info"]

        # initial_training_set_indices = active_learning_data.get_random_pool_indices(self.initial_set_size)
        # initial_training_set_indices = get_balanced_sample_indices(
        #     active_learning_data.pool_dataset, 10, self.initial_set_size // 10
        # )

        train_loader = torch.utils.data.DataLoader(
            active_learning_data.training_dataset,
            batch_size=self.training_batch_size,
            sampler=RandomFixedLengthSampler(active_learning_data.training_dataset, self.min_samples_per_epoch),
            drop_last=True,
        )

        validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=128, drop_last=False)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, drop_last=False)

        store["obi_performances"] = []
        obi_performances = store["obi_performances"]

        store["obi_topk_ensemble_performances"] = []
        obi_topk_ensemble_performances = store["obi_topk_ensemble_performances"]

        store["active_learning_steps"] = []
        active_learning_steps = store["active_learning_steps"]

        # Active Training Loop
        while True:
            training_set_size = len(active_learning_data.training_dataset)
            print(f"Training set size {training_set_size}:")

            # iteration_log = dict(training={}, pool_training={}, evaluation_metrics=None, acquisition=None)
            active_learning_steps.append({})
            iteration_log = active_learning_steps[-1]

            iteration_log["training"] = {}

            model_optimizer = self.model_optimizer_factory().create_model_optimizer()

            if training_set_size > 0:
                train(
                    model=model_optimizer.model,
                    optimizer=model_optimizer.optimizer,
                    training_samples=self.num_training_samples,
                    validation_samples=self.num_validation_samples,
                    train_loader=train_loader,
                    validation_loader=validation_loader,
                    patience=self.num_patience_epochs,
                    max_epochs=self.max_training_epochs,
                    device=self.device,
                    training_log=iteration_log["training"],
                    wandb_key_path="model_training",
                )

            evaluation_metrics = evaluate_old(
                model=model_optimizer.model,
                num_samples=self.num_validation_samples,
                loader=test_loader,
                device=self.device,
            )
            iteration_log["evaluation_metrics"] = evaluation_metrics
            log2wandb(evaluation_metrics, commit=False)
            print(f"Perf after training {evaluation_metrics}")

            obi_results, obi_topk_ensemble_results = evaluate_online_bayesian_inference(
                model=model_optimizer.model,
                real_training_set_size=training_set_size,
                train_dataset=active_learning_data.base_dataset,
                test_dataset=test_dataset,
                training_indices=training_set_indices[: training_set_size + self.max_obi_steps],
                start_index=training_set_size,
                num_samples_list=self.num_samples_list,
                num_trials=self.num_trials,
                up_factor=self.up_factor,
                eval_batchsize=512,
                device=self.device,
            )

            obi_performances.append(obi_results)
            obi_topk_ensemble_performances.append(obi_topk_ensemble_results)
            log2wandb(
                dict(
                    obi_performances=[asclassdict(result) for result in obi_results],
                    obi_topk_ensemble_results=[asclassdict(result) for result in obi_topk_ensemble_results],
                ),
                commit=False,
            )

            if training_set_size >= len(training_set_indices):
                log2wandb({}, commit=True)
                print("Done.")
                break

            next_indices = training_set_indices[training_set_size : training_set_size + self.acquisition_size]
            iteration_log["acquisition"] = dict(indices=next_indices)

            active_learning_data.acquire_base_indices(next_indices)
            log2wandb({}, commit=True)

# Cell

configs = [
    Experiment(
        seed=seed,
        acquisition_size=acquisition_size,
        num_pool_samples=num_pool_samples,
        random_acquisition=True
    )
    for seed in range(1)
    for acquisition_size in [5]
    for num_pool_samples in [100]
]

if not is_run_from_ipython() and __name__ == "__main__":
    for job_id, store in embedded_experiments(__file__, len(configs)):
        config = configs[job_id]
        config.seed += job_id
        print(config)
        store["config"] = dataclasses.asdict(config)
        store["log"] = {}

        try:
            config.run(store=store)
        except Exception:
            store["exception"] = traceback.format_exc()
            raise