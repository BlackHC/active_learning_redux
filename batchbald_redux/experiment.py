# AUTOGENERATED! DO NOT EDIT! File to edit: 09b_experiment.ipynb (unless otherwise specified).

__all__ = ['PredictionDataset', 'mnist_initial_samples', 'Experiment', 'configs']

# Cell

import dataclasses
import traceback
from dataclasses import dataclass
from enum import Enum
from typing import Union

import blackhc.project.script
import numpy as np
import torch
import torch.utils.data
from blackhc.project import is_run_from_ipython
from blackhc.project.experiment import embedded_experiments
from torch.utils.data import Dataset

from .acquisition_functions import CandidateBatchComputer, \
    EvalCandidateBatchComputer, CoreSetPoolPredictions, PoolPredictions
from .active_learning import ActiveLearningData, RandomFixedLengthSampler
from .black_box_model_training import (
    evaluate,
    get_log_mean_probs,
    get_predictions,
    get_predictions_labels,
    train,
)
from .consistent_mc_dropout import SamplerModel
from .dataset_challenges import (
    create_repeated_MNIST_dataset,
    get_balanced_sample_indices,
    get_base_index,
)
from .example_models import BayesianMNISTCNN

from .di import DependencyInjection

# Cell

class PredictionDataset(torch.utils.data.Dataset):
    dataset: torch.utils.data.Dataset
    predictions: torch.Tensor

    def __init__(self, dataset, predictions):
        assert len(dataset) == predictions.shape[0], f"{len(dataset)} == {predictions.shape[0]}"

        self.dataset = dataset
        self.predictions = predictions

    def __getitem__(self, index):
        x, y = self.dataset[index]
        p = self.predictions[index]
        return x, p

    def __len__(self):
        return len(self.dataset)

# Cell

# From the BatchBALD Repo

mnist_initial_samples = [
    38043,
    40091,
    17418,
    2094,
    39879,
    3133,
    5011,
    40683,
    54379,
    24287,
    9849,
    59305,
    39508,
    39356,
    8758,
    52579,
    13655,
    7636,
    21562,
    41329,
]

# Cell


@dataclass
class Experiment:
    seed: int = 1337
    acquisition_size: int = 5
    max_training_set: int = 300
    num_pool_samples: int = 20
    num_eval_samples: int = 20
    num_training_samples: int = 1
    num_patience_epochs: int = 3
    max_training_epochs: int = 30
    device = "cuda"
    validation_set_size: int = 1024
    initial_set_size: int = 20
    samples_per_epoch: int = 5056
    repeated_mnist_repetitions: int = 1
    add_dataset_noise: bool = False
    acquisition_function: AcquisitionFunctionType = AcquisitionFunctionType.bald
    acquisition_function_args: dict = None
    save_bald_scores: bool = False
    temperature: float = 0.0


    def load_dataset(self, initial_training_set_indices) -> (ActiveLearningData, Dataset, Dataset):
        train_dataset, test_dataset = create_repeated_MNIST_dataset(
            num_repetitions=self.repeated_mnist_repetitions, add_noise=self.add_dataset_noise
        )
        active_learning_data = ActiveLearningData(train_dataset)

        active_learning_data.acquire(initial_training_set_indices)

        validation_dataset = active_learning_data.extract_dataset_from_pool(self.validation_set_size)

        return active_learning_data, validation_dataset, test_dataset

    def new_model(self):
        return BayesianMNISTCNN()

    def new_optimizer(self, model):
        return torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    # Simple Dependency Injection
    def create_acquisition_function(self):
        acquisition_function_type = self.acquisition_function.factory()

        di = DependencyInjection(vars(self), [PoolPredictions, CoreSetPoolPredictions])
        return di.create_dataclass_type(acquisition_function_type)

    def train_eval_model(
        self, *, eval_dataset, eval_log_probs_N_C, validation_loader, num_epochs, training_log
    ):
        train_pool_prediction_dataset = PredictionDataset(eval_dataset, eval_log_probs_N_C)
        train_pool_prediction_loader = torch.utils.data.DataLoader(
            train_pool_prediction_dataset, batch_size=64, drop_last=True, shuffle=True
        )

        eval_model = self.new_model()
        eval_optimizer = self.new_optimizer(eval_model)

        loss = torch.nn.KLDivLoss(log_target=True, reduction="batchmean")

        train(
            model=eval_model,
            optimizer=eval_optimizer,
            loss=loss,
            validation_loss=torch.nn.NLLLoss(),
            training_samples=self.num_training_samples,
            validation_samples=self.num_eval_samples,
            train_loader=train_pool_prediction_loader,
            validation_loader=validation_loader,
            patience=self.num_patience_epochs,
            max_epochs=num_epochs,
            device=self.device,
            training_log=training_log,
        )

        return eval_model

    def run(self, store):
        torch.manual_seed(self.seed)

        initial_training_set_indices = mnist_initial_samples
        store["initial_training_set_indices"] = initial_training_set_indices

        # Active Learning setup
        active_learning_data, validation_dataset, test_dataset = self.load_dataset(initial_training_set_indices)
        store["dataset_info"] = dict(training=repr(active_learning_data.dataset), test=repr(test_dataset))

        # initial_training_set_indices = active_learning_data.get_random_pool_indices(self.initial_set_size)
        # initial_training_set_indices = get_balanced_sample_indices(
        #     active_learning_data.pool_dataset, 10, self.initial_set_size // 10
        # )

        train_loader = torch.utils.data.DataLoader(
            active_learning_data.training_dataset,
            batch_size=64,
            sampler=RandomFixedLengthSampler(active_learning_data.training_dataset, self.samples_per_epoch),
            drop_last=True,
        )
        pool_loader = torch.utils.data.DataLoader(
            active_learning_data.pool_dataset, batch_size=128, drop_last=False, shuffle=False
        )

        validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=128, drop_last=False)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, drop_last=False)

        store["active_learning_steps"] = []
        active_learning_steps = store["active_learning_steps"]

        acquisition_function = self.create_acquisition_function()

        # Active Training Loop
        while True:
            training_set_size = len(active_learning_data.training_dataset)
            print(f"Training set size {training_set_size}:")

            # iteration_log = dict(training={}, pool_training={}, evalution_metrics=None, acquisition=None)
            active_learning_steps.append({})
            iteration_log = active_learning_steps[-1]

            iteration_log["training"] = {}

            model = self.new_model()
            optimizer = self.new_optimizer(model)
            train(
                model=model,
                optimizer=optimizer,
                training_samples=self.num_training_samples,
                validation_samples=self.num_eval_samples,
                train_loader=train_loader,
                validation_loader=validation_loader,
                patience=self.num_patience_epochs,
                max_epochs=self.max_training_epochs,
                device=self.device,
                training_log=iteration_log["training"],
            )

            evaluation_metrics = evaluate(
                model=model, num_samples=self.num_eval_samples, loader=test_loader, device=self.device
            )
            iteration_log["evalution_metrics"] = evaluation_metrics
            print(f"Perf after training {evaluation_metrics}")

            if training_set_size >= self.max_training_set:
                print("Done.")
                break

            if isinstance(acquisition_function, CandidateBatchComputer):
                candidate_batch = acquisition_function.compute_candidate_batch(model, pool_loader, self.device)
            elif isinstance(acquisition_function, EvalCandidateBatchComputer):
                eval_dataset = torch.utils.data.ConcatDataset(
                    [active_learning_data.training_dataset, active_learning_data.pool_dataset]
                )
                eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=64, drop_last=False)

                eval_log_probs_N_C = get_log_mean_probs(
                    model=model,
                    num_samples=self.num_eval_samples,
                    num_classes=10,
                    loader=eval_loader,
                    device=self.device,
                )

                num_epochs = iteration_log["training"]["best_epoch"]

                iteration_log["pool_training"] = {}
                eval_model = self.train_eval_model(
                    eval_dataset=eval_dataset,
                    eval_log_probs_N_C=eval_log_probs_N_C,
                    validation_loader=validation_loader,
                    num_epochs=num_epochs,
                    training_log=iteration_log["pool_training"],
                )

                candidate_batch = acquisition_function.compute_candidate_batch(model, eval_model, pool_loader, device=self.device)
            else:
                raise ValueError(f"Unknown acquisition function {acquisition_function}!")

            candidate_global_indices = [
                get_base_index(active_learning_data.pool_dataset, index) for index in candidate_batch.indices
            ]
            candidate_labels = [active_learning_data.dataset[index][1].item() for index in candidate_global_indices]

            iteration_log["acquisition"] = dict(
                indices=candidate_global_indices, labels=candidate_labels, scores=candidate_batch.scores
            )

            active_learning_data.acquire(candidate_batch.indices)

            ls = ", ".join(f"{label} ({score:.4})" for label, score in zip(candidate_labels, candidate_batch.scores))
            print(f"Acquiring (label, score)s: {ls}")

# Cell

configs = [
    Experiment(
        seed=seed,
        acquisition_function=AcquisitionFunctionType.bald,
        acquisition_size=acquisition_size,
        num_pool_samples=num_pool_samples,
    )
    for seed in range(5)
    for acquisition_size in [5, 10, 20, 50]
    for num_pool_samples in [10, 20, 50, 100]
] + [
    Experiment(
        seed=seed,
        acquisition_function=AcquisitionFunctionType.random,
        acquisition_size=5,
        num_pool_samples=20,
    )
    for seed in range(20)
]

if not is_run_from_ipython() and __name__ == "__main__":
    for job_id, store in embedded_experiments(__file__, len(configs)):
        config = configs[job_id]
        config.seed += job_id
        print(config)
        store["config"] = dataclasses.asdict(config)
        store["log"] = {}

        try:
            config.run(store=store)
        except Exception:
            store["exception"] = traceback.format_exc()
            raise