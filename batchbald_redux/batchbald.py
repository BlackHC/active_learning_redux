# AUTOGENERATED! DO NOT EDIT! File to edit: 01_batchbald.ipynb (unless otherwise specified).

__all__ = ['compute_conditional_entropy', 'compute_entropy', 'CandidateBatch', 'BatchBALDScorer',
           'get_batch_bald_batch', 'get_bald_scores', 'get_top_k_scorers', 'get_bald_batch',
           'get_batch_eval_bald_batch', 'compute_each_conditional_entropy', 'get_thompson_bald_batch',
           'get_top_random_scorers', 'get_random_bald_batch', 'get_eval_bald_scores', 'get_eval_bald_batch',
           'get_top_random_eval_bald_batch', 'get_sampled_tempered_scorers', 'get_eig_scores', 'get_batch_eig_batch',
           'get_coreset_bald_scores_from_predictions', 'get_coreset_bald_scores', 'get_batch_coreset_bald_batch']

# Cell
import math
from dataclasses import dataclass
from typing import List

import numpy as np
import torch
from toma import toma
from blackhc.progress_bar import with_progress_bar, create_progress_bar

from batchbald_redux import joint_entropy

# Cell


def compute_conditional_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    entropies_N = torch.empty(N, dtype=torch.double)

    pbar = create_progress_bar(N, tqdm_args=dict(desc="Conditional Entropy", leave=False))
    pbar.start()

    @toma.execute.chunked(log_probs_N_K_C, 1024)
    def compute(log_probs_n_K_C, start: int, end: int):
        nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)

        entropies_N[start:end].copy_(-torch.sum(nats_n_K_C, dim=(1, 2)) / K)
        pbar.update(end - start)

    pbar.finish()

    return entropies_N


def compute_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    entropies_N = torch.empty(N, dtype=torch.double)

    pbar = create_progress_bar(N, tqdm_args=dict(desc="Entropy", leave=False))
    pbar.start()

    @toma.execute.chunked(log_probs_N_K_C, 1024)
    def compute(log_probs_n_K_C, start: int, end: int):
        mean_log_probs_n_C = torch.logsumexp(log_probs_n_K_C, dim=1) - math.log(K)
        nats_n_C = mean_log_probs_n_C * torch.exp(mean_log_probs_n_C)

        entropies_N[start:end].copy_(-torch.sum(nats_n_C, dim=1))
        pbar.update(end - start)

    pbar.finish()

    return entropies_N

# Cell


@dataclass
class CandidateBatch:
    scores: List[float]
    indices: List[int]

# Cell


class BatchBALDScorer:
    log_probs_N_K_C: torch.tensor
    conditional_entropies_N: torch.Tensor
    batch_joint_entropy: joint_entropy.JointEntropy
    batch_conditional_entropies: torch.Tensor

    def __init__(self, log_probs_N_K_C, *, max_size, num_samples: int, dtype=None, device=None):
        N, K, C = log_probs_N_K_C.shape
        self.log_probs_N_K_C = log_probs_N_K_C

        self.conditional_entropies_N = compute_conditional_entropy(self.log_probs_N_K_C)
        self.batch_conditional_entropies = 0

        self.batch_joint_entropy = joint_entropy.DynamicJointEntropy(
            num_samples, max_size, K, C, dtype=dtype, device=device
        )

    def append_to_batch(self, index: int):
        self.batch_joint_entropy.add_variables(self.log_probs_N_K_C[index : index + 1])
        self.batch_conditional_entropies += self.conditional_entropies_N[index].clone()

    def alloc_scores(self) -> torch.Tensor:
        # We always keep these on the CPU.
        scores_N = torch.empty(
            self.log_probs_N_K_C.shape[0],
            dtype=torch.double,
            pin_memory=torch.cuda.is_available(),
        )
        return scores_N

    def compute_scores(self, out_scores_N: torch.Tensor):
        self.batch_joint_entropy.compute_batch(self.log_probs_N_K_C, output_entropies_B=out_scores_N)

        out_scores_N -= self.conditional_entropies_N + self.batch_conditional_entropies

        return out_scores_N


def get_batch_bald_batch(
    log_probs_N_K_C: torch.Tensor, *, batch_size: int, num_samples: int, dtype=None, device=None
) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    if batch_size == 0:
        return CandidateBatch(candidate_scores, candidate_indices)

    batchbald_scorer = BatchBALDScorer(
        log_probs_N_K_C,
        max_size=batch_size - 1,
        num_samples=num_samples,
        dtype=dtype,
        device=device,
    )

    # We always keep these on the CPU.
    scores_N = batchbald_scorer.alloc_scores()

    for i in with_progress_bar(range(batch_size), tqdm_args=dict(desc="BatchBALD", leave=False)):
        if i > 0:
            latest_index = candidate_indices[-1]
            batchbald_scorer.append_to_batch(latest_index)

        batchbald_scorer.compute_scores(scores_N)
        scores_N[candidate_indices] = -float("inf")

        candidate_score, candidate_index = scores_N.max(dim=0)

        candidate_indices.append(candidate_index.item())
        candidate_scores.append(candidate_score.item())

    return CandidateBatch(candidate_scores, candidate_indices)

# Cell


def get_bald_scores(log_probs_N_K_C: torch.Tensor, *, dtype=None, device=None) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    scores_N += compute_entropy(log_probs_N_K_C)

    return scores_N

# Cell


def get_top_k_scorers(scores_N: torch.Tensor, *, batch_size: int) -> CandidateBatch:
    N = len(scores_N)
    batch_size = min(batch_size, N)

    candidate_scores, candidate_indices = torch.topk(scores_N, batch_size)

    return CandidateBatch(candidate_scores.tolist(), candidate_indices.tolist())


def get_bald_batch(log_probs_N_K_C: torch.Tensor, *, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    scores_N = get_bald_scores(log_probs_N_K_C, dtype=dtype, device=device)

    return get_top_k_scorers(scores_N, batch_size=batch_size)

# Cell


def get_batch_eval_bald_batch(
    training_log_probs_N_K_C: torch.Tensor,
    eval_log_probs_N_K_C: torch.Tensor,
    *,
    batch_size: int,
    num_samples: int,
    dtype=None,
    device=None,
) -> CandidateBatch:
    assert training_log_probs_N_K_C.shape == eval_log_probs_N_K_C.shape
    N, K, C = training_log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    if batch_size == 0:
        return CandidateBatch(candidate_scores, candidate_indices)

    training_batchbald_scorer = BatchBALDScorer(
        training_log_probs_N_K_C,
        max_size=batch_size - 1,
        num_samples=num_samples,
        dtype=dtype,
        device=device,
    )

    pool_batchbald_scorer = BatchBALDScorer(
        eval_log_probs_N_K_C,
        max_size=batch_size - 1,
        num_samples=num_samples,
        dtype=dtype,
        device=device,
    )

    # We always keep these on the CPU.
    training_scores_N = training_batchbald_scorer.alloc_scores()
    pool_scores_N = pool_batchbald_scorer.alloc_scores()

    for i in with_progress_bar(range(batch_size), tqdm_args=dict(desc="BatchBALD", leave=False)):
        if i > 0:
            latest_index = candidate_indices[-1]
            training_batchbald_scorer.append_to_batch(latest_index)
            pool_batchbald_scorer.append_to_batch(latest_index)

        training_batchbald_scorer.compute_scores(training_scores_N)
        pool_batchbald_scorer.compute_scores(pool_scores_N)

        scores_N = training_scores_N - pool_scores_N
        scores_N[candidate_indices] = -float("inf")

        candidate_score, candidate_index = scores_N.max(dim=0)

        candidate_indices.append(candidate_index.item())
        candidate_scores.append(candidate_score.item())

    return CandidateBatch(candidate_scores, candidate_indices)

# Cell


def compute_each_conditional_entropy(log_probs_N_K_C: torch.Tensor) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    entropies_N_K = torch.empty((N, K), dtype=torch.double)

    pbar = create_progress_bar(N, tqdm_args=dict(desc="Entropy", leave=False))
    pbar.start()

    @toma.execute.chunked(log_probs_N_K_C, 1024)
    def compute(log_probs_n_K_C, start: int, end: int):
        nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)

        entropies_N_K[start:end].copy_(-torch.sum(nats_n_K_C, dim=2))
        pbar.update(end - start)

    pbar.finish()

    return entropies_N_K


def get_thompson_bald_batch(
    log_probs_N_K_C: torch.Tensor, *, batch_size: int, dtype=None, device=None
) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape
    assert K >= batch_size

    batch_size = min(batch_size, N)

    entropy_N = compute_entropy(log_probs_N_K_C)
    all_conditional_entropies_N_K = compute_each_conditional_entropy(log_probs_N_K_C)

    thompson_bald_scores_N_K = entropy_N[:, None] - all_conditional_entropies_N_K

    candidate_scores, candidate_indices = [], []
    for b in range(batch_size):
        candidate_score, candidate_index = thompson_bald_scores_N_K[:, b].max(dim=0)

        candidate_scores.append(candidate_score.item())
        candidate_indices.append(candidate_index.item())

        thompson_bald_scores_N_K[candidate_index] = -float("inf")

    return CandidateBatch(candidate_scores, candidate_indices)

# Cell


def get_top_random_scorers(scores_N: torch.Tensor, *, num_classes: int, batch_size: int) -> CandidateBatch:
    N = len(scores_N)
    batch_size = min(batch_size, N)

    L = min(batch_size * num_classes, N)

    candidate_scores, candidate_indices = torch.topk(scores_N, L)

    sub_indices = torch.randperm(L)[:batch_size]

    return CandidateBatch(candidate_scores[sub_indices].tolist(), candidate_indices[sub_indices].tolist())


def get_random_bald_batch(log_probs_N_K_C: torch.Tensor, *, batch_size: int, dtype=None, device=None) -> CandidateBatch:
    N, K, C = log_probs_N_K_C.shape

    scores_N = get_bald_scores(log_probs_N_K_C, dtype=dtype, device=device)

    return get_top_random_scorers(scores_N, num_classes=C, batch_size=batch_size)

# Cell


def get_eval_bald_scores(
    training_log_probs_N_K_C: torch.Tensor,
    eval_log_probs_N_K_C: torch.Tensor,
    *,
    dtype=None,
    device=None,
) -> torch.Tensor:
    assert training_log_probs_N_K_C.shape == eval_log_probs_N_K_C.shape

    training_scores_N = get_bald_scores(training_log_probs_N_K_C, dtype=dtype, device=device)
    pool_scores_N = get_bald_scores(eval_log_probs_N_K_C, dtype=dtype, device=device)

    scores_N = training_scores_N - pool_scores_N

    return scores_N


def get_eval_bald_batch(
    training_log_probs_N_K_C: torch.Tensor,
    pool_log_probs_N_K_C: torch.Tensor,
    *,
    batch_size: int,
    dtype=None,
    device=None,
) -> CandidateBatch:
    return get_top_k_scorers(
        get_eval_bald_scores(training_log_probs_N_K_C, pool_log_probs_N_K_C, dtype=dtype, device=device),
        batch_size=batch_size,
    )


def get_top_random_eval_bald_batch(
    training_log_probs_N_K_C: torch.Tensor,
    pool_log_probs_N_K_C: torch.Tensor,
    *,
    batch_size: int,
    num_classes: int,
    dtype=None,
    device=None,
) -> CandidateBatch:
    return get_top_random_scorers(
        get_eval_bald_scores(training_log_probs_N_K_C, pool_log_probs_N_K_C, dtype=dtype, device=device),
        batch_size=batch_size,
        num_classes=num_classes,
    )

# Cell


def get_sampled_tempered_scorers(scores_N: torch.Tensor, *, temperature: float, batch_size: int) -> CandidateBatch:
    N = len(scores_N)
    batch_size = min(batch_size, N)

    tempered_scores_N = scores_N ** temperature
    tempered_scores_N[tempered_scores_N < 0] = 0.0
    partition_constant = tempered_scores_N.sum()
    p = tempered_scores_N / partition_constant

    # TODO: change this to use PyTorch instead of numpy?
    candidate_indices = np.random.choice(N, size=batch_size, replace=False, p=p.cpu().numpy())
    candidate_scores = scores_N[candidate_indices]

    return CandidateBatch(candidate_scores.tolist(), candidate_indices.tolist())

# Cell


def get_eig_scores(
    training_log_probs_N_K_C: torch.Tensor,
    pool_log_probs_N_K_C: torch.Tensor,
    *,
    dtype=None,
    device=None,
) -> torch.Tensor:
    assert training_log_probs_N_K_C.shape == pool_log_probs_N_K_C.shape

    N, K, C = training_log_probs_N_K_C.shape

    scores_N = compute_entropy(training_log_probs_N_K_C) - compute_entropy(pool_log_probs_N_K_C)

    return scores_N

# Cell


# TODO: refactor the BatchBALDScorer to deduplicate some of this?
def get_batch_eig_batch(
    training_log_probs_N_K_C: torch.Tensor,
    pool_log_probs_N_K_C: torch.Tensor,
    *,
    batch_size: int,
    num_samples: int,
    dtype=None,
    device=None,
) -> CandidateBatch:
    assert training_log_probs_N_K_C.shape == pool_log_probs_N_K_C.shape
    N, K, C = training_log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    if batch_size == 0:
        return CandidateBatch(candidate_scores, candidate_indices)

    training_joint_entropy = joint_entropy.DynamicJointEntropy(
        num_samples, batch_size, K, C, dtype=dtype, device=device
    )

    pool_joint_entropy = joint_entropy.DynamicJointEntropy(num_samples, batch_size, K, C, dtype=dtype, device=device)

    # We always keep these on the CPU.
    training_scores_N = torch.empty(
        N,
        dtype=torch.double,
        pin_memory=torch.cuda.is_available(),
    )

    pool_scores_N = torch.empty(
        N,
        dtype=torch.double,
        pin_memory=torch.cuda.is_available(),
    )

    for i in with_progress_bar(range(batch_size), tqdm_args=dict(desc="BatchBALD", leave=False)):
        if i > 0:
            latest_index = candidate_indices[-1]
            training_joint_entropy.add_variables(training_log_probs_N_K_C[latest_index : latest_index + 1])
            pool_joint_entropy.add_variables(pool_log_probs_N_K_C[latest_index : latest_index + 1])

        training_joint_entropy.compute_batch(training_log_probs_N_K_C, output_entropies_B=training_scores_N)
        pool_joint_entropy.compute_batch(pool_log_probs_N_K_C, output_entropies_B=pool_scores_N)

        scores_N = training_scores_N - pool_scores_N
        scores_N[candidate_indices] = -float("inf")

        candidate_score, candidate_index = scores_N.max(dim=0)

        candidate_indices.append(candidate_index.item())
        candidate_scores.append(candidate_score.item())

    return CandidateBatch(candidate_scores, candidate_indices)

# Cell

def get_coreset_bald_scores_from_predictions(
    log_probs_N_K_C: torch.Tensor, target_probs_N_C: torch.Tensor, *, dtype=None, device=None
) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape
    assert target_probs_N_C.shape == (N, C)

    log_probs_N_K_C = log_probs_N_K_C.to(dtype=dtype, device=device)
    target_probs_N_C = target_probs_N_C.to(dtype=dtype, device=device)

    log_prob_mean_N_C = torch.logsumexp(log_probs_N_K_C, dim=1) - np.log(K)

    entropy_N_C = -log_prob_mean_N_C
    conditional_entropy = -torch.mean(log_probs_N_K_C * log_probs_N_K_C.exp(), dim=1) / log_prob_mean_N_C.exp()
    mutual_bits_N_C = entropy_N_C - conditional_entropy

    cross_mutual_information = torch.sum(target_probs_N_C * mutual_bits_N_C, dim=1)

    return cross_mutual_information


def get_coreset_bald_scores(
    log_probs_N_K_C: torch.Tensor, labels_N: torch.Tensor, *, dtype=None, device=None
) -> torch.Tensor:
    N, K, C = log_probs_N_K_C.shape

    labels_N_1_1 = labels_N[:, None, None]
    log_probs_N_K = (
        joint_entropy.gather_expand(log_probs_N_K_C, dim=2, index=labels_N_1_1)
        .squeeze(2)
        .to(dtype=dtype, device=device)
    )

    log_prob_mean_N = torch.logsumexp(log_probs_N_K, dim=1) - np.log(K)

    lhs = -log_prob_mean_N
    rhs = -torch.mean(log_probs_N_K * log_probs_N_K.exp(), dim=1) / log_prob_mean_N.exp()

    coreset_infogain = lhs - rhs

    return coreset_infogain

# Cell


def get_batch_coreset_bald_batch(
    log_probs_N_K_C: torch.Tensor, labels_N: torch.Tensor, *, batch_size: int, dtype=None, device=None
) -> CandidateBatch:
    # We want to compute (note this does not follow the notation from below):
    # CoreSetBALD = H[y_1, ..., y_n ] - E_p(w) p(y_1, ..., y_n | w) / p(y_1, ..., y_n) H[y_1, ..., y_n | w]
    # H[y_1, ..., y_n | w] = H[y_1, ..., y_{n-1} | w] + H[y_n | w] because y_i _||_ y_j | w
    N, K, C = log_probs_N_K_C.shape

    batch_size = min(batch_size, N)

    candidate_indices = []
    candidate_scores = []

    if batch_size == 0:
        return CandidateBatch(candidate_scores, candidate_indices)

    labels_N_1_1 = labels_N[:, None, None]
    log_probs_N_K = (
        joint_entropy.gather_expand(log_probs_N_K_C, dim=2, index=labels_N_1_1)
        .squeeze(2)
        .to(dtype=dtype, device=device)
    )

    # p((y)_{B-1}|(x)_{B-1}, \omega)
    log_probs_conditional_joint_batch_K = torch.zeros_like(log_probs_N_K[0], dtype=dtype, device=device)

    for i in with_progress_bar(range(batch_size), tqdm_args=dict(desc="BatchCoreSetBALD", leave=False)):
        # p((y)_B|(x)_B, \omega) = p(y_B|x_B, \omega) * p((y)_{B-1}|(x)_{B-1}, \omega)
        log_prob_conditional_joint_N_K = log_probs_N_K + log_probs_conditional_joint_batch_K[None, :]

        # Marginalize over w (but using sum not mean):
        # p((y)_B|(x)_B) = E_p(\omega) p((y)_B|(x)_B, \omega)
        # log_prob_joint_N_1 = log_prob_conditional_joint_N_K.logsumexp(dim=1, keepdim=True) - np.log(K)
        log_prob_joint_pK_N_1 = log_prob_conditional_joint_N_K.logsumexp(dim=1, keepdim=True)
        # \frac{ p((y)_B| (x)_B, \omega) }{ p((y)_B| (x)_B) }
        # log_ratio_N_K = log_prob_conditional_joint_N_K - log_prob_joint_N_1
        # log_ratio_N_K = log_prob_conditional_joint_N_K - log_prob_joint_pK_N_1 + np.log(K)
        log_ratio_mK_N_K = log_prob_conditional_joint_N_K - log_prob_joint_pK_N_1
        # conditional_entropy_joint_N = -torch.mean(log_ratio_N_K.exp() * log_prob_conditional_joint_N_K, dim=1)
        # conditional_entropy_joint_N =
        #       -torch.mean((log_ratio_mK_N_K + np.log(K)).exp() * log_prob_conditional_joint_N_K, dim=1)
        conditional_entropy_joint_N = -torch.sum(log_ratio_mK_N_K.exp() * log_prob_conditional_joint_N_K, dim=1)
        # entropy_joint_N = -log_prob_joint_N_1.squeeze(1)
        # entropy_joint_N = -(log_prob_joint_pK_N_1 - np.log(K)).squeeze(1)
        entropy_joint_N = -log_prob_joint_pK_N_1.squeeze(1) + np.log(K)
        scores_N = entropy_joint_N - conditional_entropy_joint_N

        # Select candidate
        scores_N[candidate_indices] = -float("inf")

        candidate_score, candidate_index = scores_N.max(dim=0)

        candidate_indices.append(candidate_index.item())
        candidate_scores.append(candidate_score.item())

        # Update log_probs_conditional_joint_batch_K
        log_probs_conditional_joint_batch_K = log_prob_conditional_joint_N_K[candidate_index]

    return CandidateBatch(candidate_scores, candidate_indices)