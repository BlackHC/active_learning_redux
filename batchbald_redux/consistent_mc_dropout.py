# AUTOGENERATED! DO NOT EDIT! File to edit: 03_consistent_mc_dropout.ipynb (unless otherwise specified).

__all__ = ['BayesianModule', 'ConsistentMCDropout', 'ConsistentMCDropout2d', 'SamplerModel', 'multi_sample_loss',
           'geometric_mean_loss', 'GeometricMeanPrediction', 'LogProbMeanPrediction', 'get_predictions_labels',
           'get_predictions', 'get_log_mean_probs', 'get_module_predictions_labels', 'get_ensemble_predictions_labels']

# Cell

from functools import wraps

import numpy as np
import torch
from blackhc.progress_bar import create_progress_bar, with_progress_bar
from toma import toma
from torch.nn import Module

# Cell
from torch.utils import data

from .dataset_challenges import get_num_classes


class BayesianModule(Module):
    """A module that we can sample multiple times from given a single input batch.

    To be efficient, the module allows for a part of the forward pass to be deterministic.
    """

    k = None

    def __init__(self):
        super().__init__()

    # Returns B x K x ...
    def forward(self, input_B: torch.Tensor, k: int):
        BayesianModule.k = k

        mc_input_BK = BayesianModule.mc_tensor(input_B, k)
        mc_output_BK = self.mc_forward_impl(mc_input_BK)
        mc_output_B_K = BayesianModule.unflatten_tensor(mc_output_BK, k)
        return mc_output_B_K

    def mc_forward_impl(self, mc_input_BK: torch.Tensor):
        return mc_input_BK

    @staticmethod
    def unflatten_tensor(input: torch.Tensor, k: int):
        input = input.view([-1, k] + list(input.shape[1:]))
        return input

    @staticmethod
    def flatten_tensor(mc_input: torch.Tensor):
        return mc_input.flatten(0, 1)

    @staticmethod
    def mc_tensor(input: torch.tensor, k: int):
        mc_shape = [input.shape[0], k] + list(input.shape[1:])
        return input.unsqueeze(1).expand(mc_shape).flatten(0, 1)

    @torch.no_grad()
    def get_predictions_labels(self, *, num_samples: int, loader: data.DataLoader, device):
        self.to(device=device)

        N = len(loader.dataset)
        num_classes = get_num_classes(loader.dataset)
        predictions = torch.empty((N, num_samples, num_classes), dtype=torch.float, device="cpu")
        labels = torch.empty(N, dtype=torch.long, device="cpu")

        pbar = create_progress_bar(N * num_samples, tqdm_args=dict(desc="get_predictions_labels", leave=False))
        pbar.start()

        @toma.execute.range(0, num_samples, 128)
        def get_prediction_batch(start, end):
            if start == 0:
                pbar.reset()

            self.eval()

            num_sub_samples = end - start

            data_start = 0
            for batch_x, batch_labels in loader:
                batch_x = batch_x.to(device=device)

                batch_predictions = self(batch_x, num_sub_samples)

                batch_size = len(batch_predictions)
                data_end = data_start + batch_size

                predictions[data_start:data_end, start:end].copy_(batch_predictions.float(), non_blocking=True)
                if start == 0:
                    labels[data_start:data_end].copy_(batch_labels.long(), non_blocking=True)
                else:
                    assert labels[data_start:data_end] == batch_labels.long()

                data_start = data_end

                pbar.update(batch_size * num_sub_samples)

        pbar.finish()

        return predictions, labels

# Cell


class _ConsistentMCDropout(Module):
    def __init__(self, p=0.5):
        super().__init__()

        if p < 0 or p > 1:
            raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))

        self.p = p
        self.mask = None

    def extra_repr(self):
        return "p={}".format(self.p)

    def reset_mask(self):
        self.mask = None

    def train(self, mode=True):
        super().train(mode)
        if not mode:
            self.reset_mask()

    def _get_sample_mask_shape(self, sample_shape):
        return sample_shape

    def _create_mask(self, input, k):
        mask_shape = [1, k] + list(self._get_sample_mask_shape(input.shape[1:]))
        mask = torch.empty(mask_shape, dtype=torch.bool, device=input.device).bernoulli_(self.p)
        return mask

    def forward(self, input: torch.Tensor):
        if self.p == 0.0:
            return input

        k = BayesianModule.k
        if self.training:
            # Create a new mask on each call and for each batch element.
            k = input.shape[0]
            mask = self._create_mask(input, k)
        else:
            if self.mask is None:
                # print('recreating mask', self)
                # Recreate mask.
                self.mask = self._create_mask(input, k)

            mask = self.mask

        mc_input = BayesianModule.unflatten_tensor(input, k)
        mc_output = mc_input.masked_fill(mask, 0) / (1 - self.p)

        # Flatten MCDI, batch into one dimension again.
        return BayesianModule.flatten_tensor(mc_output)

# Cell


class ConsistentMCDropout(_ConsistentMCDropout):
    r"""Randomly zeroes some of the elements of the input
    tensor with probability :attr:`p` using samples from a Bernoulli
    distribution. The elements to zero are randomized on every forward call during training time.

    During eval time, a fixed mask is picked and kept until `reset_mask()` is called.

    This has proven to be an effective technique for regularization and
    preventing the co-adaptation of neurons as described in the paper
    `Improving neural networks by preventing co-adaptation of feature
    detectors`_ .

    Furthermore, the outputs are scaled by a factor of :math:`\frac{1}{1-p}` during
    training. This means that during evaluation the module simply computes an
    identity function.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: `Any`. Input can be of any shape
        - Output: `Same`. Output is of the same shape as input

    Examples::

        >>> m = nn.Dropout(p=0.2)
        >>> input = torch.randn(20, 16)
        >>> output = m(input)

    .. _Improving neural networks by preventing co-adaptation of feature
        detectors: https://arxiv.org/abs/1207.0580
    """
    pass


class ConsistentMCDropout2d(_ConsistentMCDropout):
    r"""Randomly zeroes whole channels of the input tensor.
    The channels to zero-out are randomized on every forward call.

    During eval time, a fixed mask is picked and kept until `reset_mask()` is called.

    Usually the input comes from :class:`nn.Conv2d` modules.

    As described in the paper
    `Efficient Object Localization Using Convolutional Networks`_ ,
    if adjacent pixels within feature maps are strongly correlated
    (as is normally the case in early convolution layers) then i.i.d. dropout
    will not regularize the activations and will otherwise just result
    in an effective learning rate decrease.

    In this case, :func:`nn.Dropout2d` will help promote independence between
    feature maps and should be used instead.

    Args:
        p (float, optional): probability of an element to be zero-ed.
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Examples::

        >>> m = nn.Dropout2d(p=0.2)
        >>> input = torch.randn(20, 16, 32, 32)
        >>> output = m(input)

    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """

    def _get_sample_mask_shape(self, sample_shape):
        return [sample_shape[0]] + [1] * (len(sample_shape) - 1)

# Cell


class SamplerModel(Module):
    """Wrap a `BayesianModule` to sample k MC dropout samples consistently."""
    def __init__(self, bayesian_module: BayesianModule, k: int):
        super().__init__()
        self.bayesian_module = bayesian_module
        self.k = k

    def forward(self, input: torch.Tensor):
        log_probs_B_K_C = self.bayesian_module(input, self.k)
        return log_probs_B_K_C

# Cell


def multi_sample_loss(loss):
    @wraps(loss)
    def wrapped_loss(input_B_K_C, target_B_, *args, **kwargs):
        assert input_B_K_C.shape[0] == target_B_.shape[0]
        input_BK_C = input_B_K_C.flatten(0, 1)
        target_B_K_ = target_B_[:, None].expand(input_B_K_C.shape[:2] + target_B_.shape[1:])
        target_BK_ = target_B_K_.flatten(0, 1)
        return loss(input_BK_C, target_BK_, *args, **kwargs)

    return wrapped_loss

# Cell


def geometric_mean_loss(loss):
    @wraps(loss)
    def wrapped_loss(log_probs_B_K_C, target_N, *args, **kwargs):
        return loss(log_probs_B_K_C.mean(dim=1, keepdim=False), target_N, *args, **kwargs)

    return wrapped_loss


class GeometricMeanPrediction(Module):
    """
    Use a eg. SamplerModel and compute the geometric mean as "ensemble" prediction.

    We assume we receive log probs (so after Softmax).
    """
    def __init__(self, module: Module):
        super().__init__()
        self.module = module

    def forward(self, input: torch.Tensor):
        log_probs_B_K_C = self.module(input)
        return log_probs_B_K_C.mean(dim=1, keepdim=False)


class LogProbMeanPrediction(Module):
    """
    Use a eg. SamplerModel and compute the geometric mean as "ensemble" prediction.
    """
    def __init__(self, module: Module):
        super().__init__()
        self.module = module

    def forward(self, input: torch.Tensor):
        log_probs_B_K_C = self.module(input)
        log_mean_probs = log_probs_B_K_C.logsumexp(dim=1, keepdim=False) - np.log(log_probs_B_K_C.shape[1])
        return log_mean_probs


@torch.no_grad()
def get_predictions_labels(*, model: BayesianModule, num_samples, num_classes, loader, device: str):
    # TODO: remove
    model.to(device=device)

    N = len(loader.dataset)
    predictions = torch.empty((N, num_samples, num_classes), dtype=torch.float, device="cpu")
    labels = torch.empty(N, dtype=torch.long, device="cpu")

    pbar = create_progress_bar(N * num_samples, tqdm_args=dict(desc="get_predictions_labels", leave=False))
    pbar.start()

    @toma.execute.range(0, num_samples, 128)
    def get_prediction_batch(start, end):
        if start == 0:
            pbar.reset()

        model.eval()

        prediction_model = SamplerModel(model, end - start)

        data_start = 0
        for batch_x, batch_labels in loader:
            batch_x = batch_x.to(device=device)

            batch_predictions = prediction_model(batch_x)

            batch_size = len(batch_predictions)
            data_end = data_start + batch_size

            predictions[data_start:data_end, start:end].copy_(batch_predictions.float(), non_blocking=True)
            if start == 0:
                labels[data_start:data_end].copy_(batch_labels.long(), non_blocking=True)
            else:
                # TODO: check whether the dataloader is shuffling or not!
                assert all(labels[data_start:data_end] == batch_labels.long())

            data_start = data_end

            pbar.update(batch_size * (end - start))

    pbar.finish()

    return predictions, labels


def get_predictions(*, model, num_samples, num_classes, loader, device: str):
    # TODO: remove
    predictions, _ = get_predictions_labels(
        model=model, num_samples=num_samples, num_classes=num_classes, loader=loader, device=device
    )

    return predictions


def get_log_mean_probs(log_probs_N_K_C):
    log_mean_probs_N_C = log_probs_N_K_C.logsumexp(dim=1, keepdim=False) - np.log(log_probs_N_K_C.shape[1])
    return log_mean_probs_N_C


@torch.no_grad()
def get_module_predictions_labels(*, module: torch.nn.Module, loader: data.DataLoader, device):
    module.to(device=device)

    N = len(loader.dataset)
    num_classes = get_num_classes(loader.dataset)
    predictions = torch.empty((N, num_classes), dtype=torch.float, device="cpu")
    labels = torch.empty(N, dtype=torch.long, device="cpu")

    pbar = create_progress_bar(N, tqdm_args=dict(desc="get_predictions_labels", leave=False))
    pbar.start()

    module.eval()

    data_start = 0
    for batch_x, batch_labels in loader:
        batch_x = batch_x.to(device=device)
        batch_predictions = module(batch_x)

        batch_size = len(batch_predictions)
        data_end = data_start + batch_size

        predictions[data_start:data_end].copy_(batch_predictions.float(), non_blocking=True)
        labels[data_start:data_end].copy_(batch_labels.long(), non_blocking=True)

        data_start = data_end

        pbar.update(batch_size)

    pbar.finish()

    return predictions, labels


def get_ensemble_predictions_labels(*, modules: [torch.nn.Module], loader: data.DataLoader, device):
    ensemble_predictions = []
    ensemble_labels = None

    for module in modules:
        predictions, labels = get_module_predictions_labels(module=module, loader=loader, device=device)

        ensemble_predictions += [predictions]
        if ensemble_labels is not None:
            assert all(ensemble_labels == labels)
        else:
            ensemble_labels = labels

        module.to("cpu")

    ensemble_predictions = torch.stack(ensemble_predictions, dim=1)
    return ensemble_predictions, ensemble_labels