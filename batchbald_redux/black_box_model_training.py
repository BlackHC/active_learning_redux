# AUTOGENERATED! DO NOT EDIT! File to edit: 07_black_box_training.ipynb (unless otherwise specified).

__all__ = ['create_supervised_trainer', 'train', 'LOG_INTERVAL', 'train_with_schedule', 'ModelOptimizerStateDicts',
           'DoubleSnapshots', 'train_double_snapshots', 'evaluate', 'create_metrics']

# Internal Cell

from dataclasses import dataclass
from typing import Optional, Union, Callable, Sequence, Tuple, Any

import torch
from blackhc.project import is_run_from_ipython
from blackhc.project.utils.ignite_progress_bar import ignite_progress_bar
from ignite.contrib.engines.common import setup_common_training_handlers
from ignite.contrib.handlers import ProgressBar
import ignite.distributed as idist
from ignite.engine import Events, create_supervised_evaluator, Engine, DeterministicEngine
from ignite.metrics import Accuracy, Loss, RunningAverage
from ignite.utils import convert_tensor
from torch import nn

if idist.has_xla_support:
    import torch_xla.core.xla_model as xm

from .consistent_mc_dropout import (
    GeometricMeanPrediction,
    SamplerModel,
    multi_sample_loss,
)
from .restoring_early_stopping import (
    PatienceWithSnapshot,
    ReduceLROnPlateauWithScheduleWrapper,
    RestoringEarlyStopping,
)

# Cell


LOG_INTERVAL = 10


def _prepare_batch(
    batch: Sequence[torch.Tensor], device: Optional[Union[str, torch.device]] = None, non_blocking: bool = False
):
    """Prepare batch for training: pass to a device with options.

    """
    x, y = batch
    return (
        convert_tensor(x, device=device, non_blocking=non_blocking),
        convert_tensor(y, device=device, non_blocking=non_blocking),
    )


# copied from ignite 0.4.2: ignite/engine/__init__.py
def create_supervised_trainer(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    *,
    augmentations: torch.nn.Module,
    loss_fn: Union[Callable, torch.nn.Module],
    device: Optional[Union[str, torch.device]] = None,
    non_blocking: bool = False,
    prepare_batch: Callable = _prepare_batch,
    output_transform: Callable = lambda x, y, y_pred, loss: loss.item(),
    deterministic: bool = False,
) -> Engine:
    """Factory function for creating a trainer for supervised models.

    Args:
        model (`torch.nn.Module`): the model to train.
        optimizer (`torch.optim.Optimizer`): the optimizer to use.
        loss_fn (torch.nn loss function): the loss function to use.
        device (str, optional): device type specification (default: None).
            Applies to batches after starting the engine. Model *will not* be moved.
            Device can be CPU, GPU or TPU.
        non_blocking (bool, optional): if True and this copy is between CPU and GPU, the copy may occur asynchronously
            with respect to the host. For other cases, this argument has no effect.
        prepare_batch (callable, optional): function that receives `batch`, `device`, `non_blocking` and outputs
            tuple of tensors `(batch_x, batch_y)`.
        output_transform (callable, optional): function that receives 'x', 'y', 'y_pred', 'loss' and returns value
            to be assigned to engine's state.output after each iteration. Default is returning `loss.item()`.
        deterministic (bool, optional): if True, returns deterministic engine of type
            :class:`~ignite.engine.deterministic.DeterministicEngine`, otherwise :class:`~ignite.engine.engine.Engine`
            (default: False).

    Note:
        `engine.state.output` for this engine is defined by `output_transform` parameter and is the loss
        of the processed batch by default.

    .. warning::

        The internal use of `device` has changed.
        `device` will now *only* be used to move the input data to the correct device.
        The `model` should be moved by the user before creating an optimizer.
        For more information see:

        - `PyTorch Documentation <https://pytorch.org/docs/stable/optim.html#constructing-it>`_

        - `PyTorch's Explanation <https://github.com/pytorch/pytorch/issues/7844#issuecomment-503713840>`_

    Returns:
        Engine: a trainer engine with supervised update function.
    """

    device_type = device.type if isinstance(device, torch.device) else device
    on_tpu = "xla" in device_type if device_type is not None else False

    if on_tpu and not idist.has_xla_support:
        raise RuntimeError("In order to run on TPU, please install PyTorch XLA")

    def _update(engine: Engine, batch: Sequence[torch.Tensor]) -> Union[Any, Tuple[torch.Tensor]]:
        model.train()
        optimizer.zero_grad()
        x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)
        # TODO: move this into the module itself? (pre-transform x)
        x = augmentations(x)
        y_pred = model(x)
        loss = loss_fn(y_pred, y)
        loss.backward()

        if on_tpu:
            xm.optimizer_step(optimizer, barrier=True)
        else:
            optimizer.step()

        return output_transform(x, y, y_pred, loss)

    trainer = Engine(_update) if not deterministic else DeterministicEngine(_update)

    return trainer


def train(
    *,
    model,
    training_samples,
    validation_samples,
    train_loader,
    validation_loader,
    patience: Optional[int],
    max_epochs: int,
    device: str,
    training_log: dict,
    loss=None,
    validation_loss=None,
    optimizer=None,
    prefer_accuracy=True,
    train_augmentations=None,
):
    """
    :param model:
    :param train_loader:
    :param val_loader:
    :param metric_loader: We compute metrics for debugging and introspection purposes with this data.
    :param patience: How many epochs to wait for early-stopping.
    :param max_epochs:
    :param tb_log_dir:
    :param device:
    :return: Optimizer that was used for training.
    """
    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss
    if train_augmentations is None:
        train_augmentations = torch.nn.Sequential()

    train_model = SamplerModel(model, training_samples)
    validation_model = GeometricMeanPrediction(SamplerModel(model, validation_samples))

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, augmentations=train_augmentations, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    @trainer.on(Events.EPOCH_COMPLETED)
    def compute_metrics(engine):
        validation_evaluator.run(validation_loader)

    # Only to look nicer.
    RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        ProgressBar(persist=False).attach(
            validation_evaluator,
            metric_names="all",
            event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
        )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    # Logging
    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def log_training_results(engine):
        metrics = dict(engine.state.metrics)
        epochs_log.append(metrics)

        if is_run_from_ipython():
            print(f"Epoch metrics: {metrics}")

    # Add early stopping
    if patience is not None:
        if prefer_accuracy:

            def score_function():
                return float(validation_evaluator.state.metrics["accuracy"])

        else:

            def score_function():
                return float(-validation_evaluator.state.metrics["crossentropy"])

        early_stopping = RestoringEarlyStopping(
            patience=patience,
            score_function=score_function,
            module=model,
            optimizer=optimizer,
            training_engine=trainer,
            validation_engine=validation_evaluator,
        )
    else:
        early_stopping = None

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    if early_stopping:
        training_log["best_epoch"] = early_stopping.best_epoch

    # Return the optimizer in case we want to continue training.
    return optimizer

# Cell


def train_with_schedule(
    *,
    model,
    training_samples,
    validation_samples,
    train_loader,
    validation_loader,
    patience_schedule: [int],
    factor_schedule: [int],
    max_epochs: int,
    device: str,
    training_log: dict,
    loss=None,
    validation_loss=None,
    optimizer=None,
    prefer_accuracy=True,
    train_augmentations=None,
):
    """
    :param model:
    :param train_loader:
    :param val_loader:
    :param metric_loader: We compute metrics for debugging and introspection purposes with this data.
    :param patience: How many epochs to wait for early-stopping.
    :param max_epochs:
    :param tb_log_dir:
    :param device:
    :return: Optimizer that was used for training.
    """
    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss
    if train_augmentations is None:
        train_augmentations = torch.nn.Sequential()

    train_model = SamplerModel(model, training_samples)
    validation_model = GeometricMeanPrediction(SamplerModel(model, validation_samples))

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, augmentations=train_augmentations, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    @trainer.on(Events.EPOCH_COMPLETED)
    def compute_metrics(engine):
        validation_evaluator.run(validation_loader)

    # Only to look nicer.
    RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        ProgressBar(persist=False).attach(
            validation_evaluator,
            metric_names="all",
            event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
        )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    # Logging
    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def log_training_results(engine):
        metrics = dict(engine.state.metrics)
        epochs_log.append(metrics)

        if is_run_from_ipython():
            print(f"Epoch metrics: {metrics}")

    if prefer_accuracy:
        def score_function(metrics):
            return float(metrics["accuracy"])

    else:

        def score_function(metrics):
            return float(metrics["crossentropy"])

    training_log["era_epochs"] = []

    def next_era_callback():
        training_log["era_epochs"].append(trainer.state.epoch)

    scheduler = ReduceLROnPlateauWithScheduleWrapper(
        optimizer,
        metrics_transform=score_function,
        factor_schedule=factor_schedule,
        patience_schedule=patience_schedule,
        end_callback=trainer.terminate,
        next_era_callback=next_era_callback,
        mode="max" if prefer_accuracy else "min",
        verbose=True,
    )

    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def step_scheduler(engine):
        scheduler.step(engine)

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    # Return the optimizer in case we want to continue training.
    return optimizer

# Cell


@dataclass
class ModelOptimizerStateDicts:
    model_state_dict: object
    optimizer_state_dict: object


@dataclass
class DoubleSnapshots:
    high_accuracy: ModelOptimizerStateDicts
    low_cross_entropy: ModelOptimizerStateDicts


def train_double_snapshots(
    *,
    model: torch.nn.Module,
    training_samples,
    validation_samples,
    train_loader,
    validation_loader,
    patience: int,
    max_epochs: int,
    device: str,
    training_log: dict,
    loss=None,
    validation_loss=None,
    optimizer=None,
    train_augmentations: torch.nn.Module,
) -> DoubleSnapshots:
    """
    :param model:
    :param train_loader:
    :param val_loader:
    :param metric_loader: We compute metrics for debugging and introspection purposes with this data.
    :param patience: How many epochs to wait for early-stopping.
    :param max_epochs:
    :param tb_log_dir:
    :param device:
    :return: Optimizer that was used for training.
    """
    if loss is None:
        loss = nn.NLLLoss()
    if validation_loss is None:
        validation_loss = loss
    if train_augmentations is None:
        train_augmentations = torch.nn.Sequential()

    train_model = SamplerModel(model, training_samples)
    validation_model = GeometricMeanPrediction(SamplerModel(model, validation_samples))

    # Move model to device before creating the optimizer
    train_model.to(device)

    if optimizer is None:
        optimizer = torch.optim.Adam(model.parameters(), weight_decay=5e-4)

    trainer = create_supervised_trainer(train_model, optimizer, train_augmentations=train_augmentations, loss_fn=multi_sample_loss(loss), device=device)

    metrics = create_metrics(validation_loss)

    validation_evaluator = create_supervised_evaluator(validation_model, metrics=metrics, device=device)

    @trainer.on(Events.EPOCH_COMPLETED)
    def compute_metrics(engine):
        validation_evaluator.run(validation_loader)

    # Only to look nicer.
    RunningAverage(output_transform=lambda x: x).attach(trainer, "crossentropy")

    enable_tqdm_pbars = is_run_from_ipython()

    setup_common_training_handlers(
        trainer, with_pbars=enable_tqdm_pbars, with_gpu_stats=torch.cuda.is_available(), log_every_iters=LOG_INTERVAL
    )

    if enable_tqdm_pbars:
        ProgressBar(persist=False).attach(
            validation_evaluator,
            metric_names="all",
            event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
        )
    else:
        ignite_progress_bar(trainer, desc=lambda engine: "Training", log_interval=LOG_INTERVAL)

    training_log["epochs"] = []
    epochs_log = training_log["epochs"]

    # Logging
    @validation_evaluator.on(Events.EPOCH_COMPLETED)
    def log_training_results(engine):
        metrics = dict(engine.state.metrics)
        epochs_log.append(metrics)

        if is_run_from_ipython():
            print(f"Epoch metrics: {metrics}")

    cross_entropy_state_dicts = None
    accuracy_state_dicts = None

    # Add early stopping
    if patience is not None:

        def cross_entropy_out_of_patience_callback(module_state_dict, optimizer_state_dict):
            nonlocal cross_entropy_state_dicts
            cross_entropy_state_dicts = ModelOptimizerStateDicts(
                model_state_dict=module_state_dict, optimizer_state_dict=optimizer_state_dict
            )
            if cross_entropy_pws.is_out_of_patience() and accuracy_entropy_pws.is_out_of_patience():
                trainer.terminate()

        cross_entropy_pws = PatienceWithSnapshot(
            name="LowCrossEntropy-",
            patience=patience,
            score_function=lambda: float(-validation_evaluator.state.metrics["crossentropy"]),
            module=model,
            optimizer=optimizer,
            training_engine=trainer,
            validation_engine=validation_evaluator,
            out_of_patience_callback=cross_entropy_out_of_patience_callback,
        )

        def accuracy_out_of_patience_callback(module_state_dict, optimizer_state_dict):
            nonlocal accuracy_state_dicts
            accuracy_state_dicts = ModelOptimizerStateDicts(
                model_state_dict=module_state_dict, optimizer_state_dict=optimizer_state_dict
            )
            if cross_entropy_pws.is_out_of_patience() and accuracy_entropy_pws.is_out_of_patience():
                trainer.terminate()

        accuracy_entropy_pws = PatienceWithSnapshot(
            name="Accuracy-",
            patience=patience,
            score_function=lambda: float(validation_evaluator.state.metrics["accuracy"]),
            module=model,
            optimizer=optimizer,
            training_engine=trainer,
            validation_engine=validation_evaluator,
            out_of_patience_callback=accuracy_out_of_patience_callback,
        )

    # Kick everything off
    trainer.run(train_loader, max_epochs=max_epochs)

    if cross_entropy_state_dicts:
        training_log["cross_entropy_best_epoch"] = cross_entropy_pws.best_epoch
    else:
        cross_entropy_state_dicts = ModelOptimizerStateDicts(
            model_state_dict=model.state_dict(), optimizer_state_dict=optimizer.state_dict()
        )

    if accuracy_state_dicts:
        training_log["cross_entropy_best_epoch"] = cross_entropy_pws.best_epoch
    else:
        accuracy_state_dicts = ModelOptimizerStateDicts(
            model_state_dict=model.state_dict(), optimizer_state_dict=optimizer.state_dict()
        )

    return DoubleSnapshots(accuracy_state_dicts, cross_entropy_state_dicts)

# Cell


def evaluate(*, model, num_samples, loader, device, loss=None):
        # TODO: rewrite this on top of TrainedModel?
        # Add "get_log_prob_predictions" which returns the mean?
        # Compute accuracy etc based on that?

    # Move model to device
    model.to(device)

    evaluation_model = GeometricMeanPrediction(SamplerModel(model, num_samples))

    if loss is None:
        loss = nn.NLLLoss()

    metrics = create_metrics(loss)

    evaluator = create_supervised_evaluator(evaluation_model, metrics=metrics, device=device)

    ProgressBar(persist=False).attach(
        evaluator,
        metric_names="all",
        event_name=Events.ITERATION_COMPLETED(every=LOG_INTERVAL),
    )

    # Kick everything off
    evaluator.run(loader, max_epochs=1)

    return evaluator.state.metrics


def create_metrics(loss):
    return {"accuracy": Accuracy(), "crossentropy": Loss(loss)}